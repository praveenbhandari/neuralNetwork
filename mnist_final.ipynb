{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron():\n",
    "    def __init__(self,bias):\n",
    "        self.bias=bias\n",
    "        self.weights=[]\n",
    "\n",
    "    # def sigmoid(self,x):\n",
    "    #     return 1/(1+np.exp(-x))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -100, 100)))  # Clipping inputs to avoid large values\n",
    "\n",
    "    \n",
    "    def sum_input_to_hiddedn(self):\n",
    "        sum=0\n",
    "        for i in range(len(self.input)):\n",
    "            sum += self.input[i]*self.weights[i]\n",
    "            # print(sum)\n",
    "        return sum+self.bias\n",
    "    \n",
    "    \n",
    "\n",
    "    def cost(self,input):\n",
    "        self.input=input\n",
    "        self.output=self.sigmoid(self.sum_input_to_hiddedn())\n",
    "        # print(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def cal_error(self,target_op):\n",
    "        return 0.5*np.square(target_op-self.output)\n",
    "    \n",
    "    def error_wrt_output(self, targer_op):\n",
    "        return -(targer_op-self.output)\n",
    "\n",
    "\n",
    "    def error_wrt_input(self):\n",
    "        return self.output*(1-self.output)\n",
    "\n",
    "    def total_error(self,targer_op):\n",
    "        return self.error_wrt_input()*self.error_wrt_output(targer_op)\n",
    "    \n",
    "\n",
    "    # def error_wrt_weight(self,index):\n",
    "    #     return self.input[index]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network():\n",
    "    def __init__(self,hidden_layer,bias):\n",
    "        self.network=[]\n",
    "        self.bias = bias if bias is not None else random.random()\n",
    "        for _ in range(hidden_layer):\n",
    "            self.network.append(neuron(self.bias))\n",
    "\n",
    "    def forward(self,input):\n",
    "        outputs=[]\n",
    "        for i in self.network:\n",
    "            outputs.append(i.cost(input))\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    # def backPropogation(self,):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_network():\n",
    "    def __init__(self,n_input_layer,n_hidden_layer,n_output_layer,hidden_weights,output_weights,hidden_bias,output_bias):\n",
    "        self.hidden_l_network=network(n_hidden_layer,hidden_bias)\n",
    "        self.output_l_network=network(n_output_layer,output_bias)\n",
    "        self.n_input_layer=n_input_layer\n",
    "        # self.hidden_weight_layer=self.add_weights_to_hidden(hidden_weights)\n",
    "        # self.output_weight_layer=self.add_weights_to_output(output_weights)\n",
    "        self.add_weights(self.hidden_l_network,hidden_weights,n_input_layer)\n",
    "        self.add_weights(self.output_l_network,output_weights,n_hidden_layer)\n",
    "\n",
    "\n",
    "    def add_weights(self, layer, weights, num_inputs):\n",
    "        count = 0\n",
    "        for neuron in layer.network:\n",
    "            neuron.weights = weights[count:count + num_inputs]\n",
    "            count += num_inputs\n",
    "    # def add_weights_to_output(self,output_weights):\n",
    "    #     count=0\n",
    "    #     for i in range(len(self.output_l_network.network)):\n",
    "    #         # print(hidden_l_network.network[i].weight)\n",
    "    #         for j in range((self.n_input_layer)):\n",
    "    #             self.output_l_network.network[i].weight.append(output_weights[count])\n",
    "    #             count+=1\n",
    "\n",
    "    def forward_hidden_op(self,input_data):\n",
    "        # hidden_op=self.hidden_l_network.forward(input_data)\n",
    "        # self.hidden_l_cost=self.hidden_l_network.forward(input_data)\n",
    "        # print(\"Hidden layer outputs:\", self.hidden_l_cost)\n",
    "        # self.output_l_cost=self.output_l_network.forward(self.hidden_l_cost)\n",
    "        # print(\"Output layer outputs:\", self.output_l_cost)  # Print output layer outputs\n",
    "        hidden_output = self.hidden_l_network.forward(input_data)\n",
    "        # print(f\"Hidden Layer Output: {hidden_output}\")\n",
    "        output = self.output_l_network.forward(hidden_output)\n",
    "        # print(f\"Output Layer Output: {output}\")\n",
    "\n",
    "        # return np.mean(output)\n",
    "        return output\n",
    "    \n",
    "    def backPropogation(self, input_data, target_op,learning_rate=0.5):\n",
    "        # Output layer deltas\n",
    "        # self.forward_hidden_op(target_inp)\n",
    "        hidden_output = self.hidden_l_network.forward(input_data)\n",
    "        final_output = self.output_l_network.forward(hidden_output)\n",
    "\n",
    "        # output_deltas = [0] * len(self.output_l_network.network)\n",
    "        # for i in range(len(self.output_l_network.network)):\n",
    "        #     output_deltas[i] = self.output_l_network.network[i].cal_error(target_op[i])\n",
    "\n",
    "        # output_deltas = []\n",
    "        output_deltas = [\n",
    "            (target - output) * neuron.error_wrt_input()\n",
    "            for target, output, neuron in zip(target_op, final_output, self.output_l_network.network)\n",
    "        ]\n",
    "        # hidden_deltas=[]\n",
    "        \n",
    "        # Hidden layer deltas\n",
    "        hidden_deltas = []\n",
    "        for i, hidden_neuron in enumerate(self.hidden_l_network.network):\n",
    "            weighted_sum = sum(\n",
    "                delta * output_neuron.weights[i]\n",
    "                for delta, output_neuron in zip(output_deltas, self.output_l_network.network)\n",
    "            )\n",
    "            hidden_deltas.append(weighted_sum * hidden_neuron.error_wrt_input())\n",
    "\n",
    "        # Update output layer weights and biases\n",
    "        for neuron, delta in zip(self.output_l_network.network, output_deltas):\n",
    "            neuron.weights = [\n",
    "                weight + learning_rate * delta * hidden_output[i]\n",
    "                for i, weight in enumerate(neuron.weights)\n",
    "            ]\n",
    "            neuron.bias += learning_rate * delta\n",
    "\n",
    "        # Update hidden layer weights and biases\n",
    "        for neuron, delta in zip(self.hidden_l_network.network, hidden_deltas):\n",
    "            neuron.weights = [\n",
    "                weight + learning_rate * delta * input_data[i]\n",
    "                for i, weight in enumerate(neuron.weights)\n",
    "            ]\n",
    "            neuron.bias += learning_rate * delta\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_total_error(self, dataset):\n",
    "        total_error = 0\n",
    "        for input_data, target_op in dataset:\n",
    "            outputs = self.forward_hidden_op(input_data)\n",
    "            total_error += sum(0.5 * (target_op[i] - outputs[i]) ** 2 for i in range(len(target_op)))\n",
    "        return total_error\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class probabilities: [0.49460936 0.50539064]\n"
     ]
    }
   ],
   "source": [
    "nn = all_network(2, 2, 2, hidden_weights=[0.15, 0.2, 0.25, 0.3], hidden_bias=0.35, output_weights=[0.4, 0.45, 0.5, 0.55], output_bias=0.6)\n",
    "a=nn.forward_hidden_op([0.05, 0.1])\n",
    "exp_logits = np.exp(a - np.max(a))  # Normalize for numerical stability\n",
    "probabilities = exp_logits / np.sum(exp_logits)\n",
    "print(\"Class probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 148\n",
      "Before training: 150\n",
      "Before training: 149\n",
      "Before training: 147\n",
      "Epoch 1/5, Total Error: 4038.587603\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m y_train_onehot[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_data, target_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_X, batch_y):\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackPropogation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m         total_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mcalculate_total_error([(input_data\u001b[38;5;241m.\u001b[39mtolist(), target_output\u001b[38;5;241m.\u001b[39mtolist())])\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_error\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mall_network.backPropogation\u001b[0;34m(self, input_data, target_op, learning_rate)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackPropogation\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_data, target_op,learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Output layer deltas\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# self.forward_hidden_op(target_inp)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     hidden_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_l_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_l_network\u001b[38;5;241m.\u001b[39mforward(hidden_output)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# output_deltas = [0] * len(self.output_l_network.network)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# for i in range(len(self.output_l_network.network)):\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m#     output_deltas[i] = self.output_l_network.network[i].cal_error(target_op[i])\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# output_deltas = []\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mnetwork.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      9\u001b[0m outputs\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork:\n\u001b[0;32m---> 11\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mneuron.cost\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost\u001b[39m(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum_input_to_hiddedn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# print(self.output)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mneuron.sum_input_to_hiddedn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28msum\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput)):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[i]\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(sum)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train_1 = X_train\n",
    "y_train_1 = y_train\n",
    "X_test_1 = X_test\n",
    "y_test_1 = y_test\n",
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "X_test = X_test[:10000]\n",
    "y_test = y_test[:10000]\n",
    "\n",
    "# print(X_train_1.shape)\n",
    "# print(y_train_1.shape)\n",
    "# print(X_test_1.shape)\n",
    "# print(y_test_1.shape)\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0  \n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "input_size = 784\n",
    "hidden_layer_size = 64\n",
    "output_layer_size = 10\n",
    "\n",
    "hidden_weights = np.random.rand(hidden_layer_size * input_size) * 0.1\n",
    "output_weights = np.random.rand(output_layer_size * hidden_layer_size) * 0.1\n",
    "hidden_bias = 0.1\n",
    "output_bias = 0.1\n",
    "\n",
    "nn = all_network( n_input_layer=input_size, n_hidden_layer=hidden_layer_size, n_output_layer=output_layer_size, hidden_weights=hidden_weights.tolist(), hidden_bias=hidden_bias, output_weights=output_weights.tolist(), output_bias=output_bias )\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "output = nn.forward_hidden_op(X_test_1[1111])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"Before training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test_1[2222])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"Before training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test_1[3333])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"Before training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test_1[5555])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"Before training:\", predicted_class)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train_onehot[i:i+batch_size]\n",
    "        for input_data, target_output in zip(batch_X, batch_y):\n",
    "            nn.backPropogation(input_data.tolist(), target_output.tolist(), learning_rate)\n",
    "            total_error += nn.calculate_total_error([(input_data.tolist(), target_output.tolist())])\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Total Error: {total_error:.6f}\")\n",
    "\n",
    "\n",
    "correct_predictions = 0\n",
    "for input_data, target_label in zip(X_test, y_test):\n",
    "    predicted_output = nn.forward_hidden_op(input_data.tolist())\n",
    "    predicted_class = predicted_output.index(max(predicted_output))\n",
    "    if predicted_class == target_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(X_test)\n",
    "print(f\"\\nAccuracy on test data: {accuracy * 100}%\")\n",
    "output = nn.forward_hidden_op(X_test_1[1111])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test_1[2222])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test_1[3333])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test_1[5555])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "\n",
    "\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[0])\n",
    "print(\"Actual class:\", actual_class)\n",
    "print(\"after train\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "images = [X_test_1[1111], X_test_1[2222], X_test_1[3333], X_test_1[5555]]\n",
    "for ax, img in zip(axes, images):\n",
    "    ax.imshow(img, cmap=\"gray\")  \n",
    "    ax.axis(\"off\")  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_network_model_on_10000.pkl\", \"rb\") as file:\n",
    "    loaded_nn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: 3\n",
      "\n",
      "Training the network on MNIST data...\n",
      "Epoch 1/5, Total Error: 9139.620941\n",
      "Epoch 2/5, Total Error: 2195.848908\n",
      "Epoch 3/5, Total Error: 1924.854084\n",
      "Epoch 4/5, Total Error: 1782.671257\n",
      "Epoch 5/5, Total Error: 1668.967155\n",
      "\n",
      "Accuracy on MNIST test data: 93.80%\n",
      "Prediction after training: 7\n",
      "Actual class: 7\n",
      "after train [np.float64(0.9681478106756721), np.float64(0.9654559298175209), np.float64(0.9559403131723999), np.float64(0.9712328651655572), np.float64(0.9690818226041124), np.float64(0.9564146843861848), np.float64(0.963861185789479), np.float64(0.9665142734788169), np.float64(0.9640462778014949), np.float64(0.9555501801353329)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train\n",
    "y_train = y_train\n",
    "X_test = X_test\n",
    "y_test = y_test\n",
    "X_train_1 = X_train[:1000]\n",
    "y_train_1 = y_train[:1000]\n",
    "X_test_1 = X_test[:1000]\n",
    "y_test_1 = y_test[:1000]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0  \n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "input_size = 784\n",
    "hidden_layer_size = 64\n",
    "output_layer_size = 10\n",
    "\n",
    "hidden_weights = np.random.rand(hidden_layer_size * input_size) * 0.1\n",
    "output_weights = np.random.rand(output_layer_size * hidden_layer_size) * 0.1\n",
    "hidden_bias = 0.1\n",
    "output_bias = 0.1\n",
    "\n",
    "nn = all_network( n_input_layer=input_size, n_hidden_layer=hidden_layer_size, n_output_layer=output_layer_size, hidden_weights=hidden_weights.tolist(), hidden_bias=hidden_bias, output_weights=output_weights.tolist(), output_bias=output_bias )\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "output = nn.forward_hidden_op(X_test[0])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"Before training:\", predicted_class)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_error = 0\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train_onehot[i:i+batch_size]\n",
    "        for input_data, target_output in zip(batch_X, batch_y):\n",
    "            nn.backPropogation(input_data.tolist(), target_output.tolist(), learning_rate)\n",
    "            total_error += nn.calculate_total_error([(input_data.tolist(), target_output.tolist())])\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Total Error: {total_error:.6f}\")\n",
    "\n",
    "\n",
    "correct_predictions = 0\n",
    "for input_data, target_label in zip(X_test, y_test):\n",
    "    predicted_output = nn.forward_hidden_op(input_data.tolist())\n",
    "    predicted_class = predicted_output.index(max(predicted_output))\n",
    "    if predicted_class == target_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(X_test)\n",
    "print(f\"\\nAccuracy on test data: {accuracy * 100}%\")\n",
    "output_after_training = nn.forward_hidden_op(X_test[0])\n",
    "predicted_class_after_training = np.argmax(output_after_training)\n",
    "\n",
    "\n",
    "print(\"After training:\", predicted_class_after_training)\n",
    "\n",
    "# Print the actual target class\n",
    "actual_class = np.argmax(y_test_onehot[0])\n",
    "print(\"Actual class:\", actual_class)\n",
    "print(\"after train\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_network_model.pkl\", \"rb\") as file:\n",
    "    loaded_nn = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x141d00a70>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test_1[1111])\n",
    "plt.imshow(X_test_1[2222])\n",
    "plt.imshow(X_test_1[3333])\n",
    "plt.imshow(X_test_1[5555])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
