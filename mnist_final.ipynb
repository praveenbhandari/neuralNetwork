{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuron():\n",
    "    def __init__(self,bias):\n",
    "        self.bias=bias\n",
    "        self.weights=[]\n",
    "\n",
    "    # def sigmoid(self,x):\n",
    "    #     return 1/(1+np.exp(-x))\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -100, 100)))  # Clipping inputs to avoid large values\n",
    "\n",
    "    \n",
    "    def sum_input_to_hiddedn(self):\n",
    "        sum=0\n",
    "        for i in range(len(self.input)):\n",
    "            sum += self.input[i]*self.weights[i]\n",
    "            # print(sum)\n",
    "        return sum+self.bias\n",
    "    \n",
    "    \n",
    "\n",
    "    def cost(self,input):\n",
    "        self.input=input\n",
    "        self.output=self.sigmoid(self.sum_input_to_hiddedn())\n",
    "        # print(self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def cal_error(self,target_op):\n",
    "        return 0.5*np.square(target_op-self.output)\n",
    "    \n",
    "    def error_wrt_output(self, targer_op):\n",
    "        return -(targer_op-self.output)\n",
    "\n",
    "\n",
    "    def error_wrt_input(self):\n",
    "        return self.output*(1-self.output)\n",
    "\n",
    "    def total_error(self,targer_op):\n",
    "        return self.error_wrt_input()*self.error_wrt_output(targer_op)\n",
    "    \n",
    "\n",
    "    # def error_wrt_weight(self,index):\n",
    "    #     return self.input[index]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network():\n",
    "    def __init__(self,hidden_layer,bias):\n",
    "        self.network=[]\n",
    "        self.bias = bias if bias is not None else random.random()\n",
    "        for _ in range(hidden_layer):\n",
    "            self.network.append(neuron(self.bias))\n",
    "\n",
    "    def forward(self,input):\n",
    "        outputs=[]\n",
    "        for i in self.network:\n",
    "            outputs.append(i.cost(input))\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "    # def backPropogation(self,):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class all_network():\n",
    "    def __init__(self,n_input_layer,n_hidden_layer,n_output_layer,hidden_weights,output_weights,hidden_bias,output_bias):\n",
    "        self.hidden_l_network=network(n_hidden_layer,hidden_bias)\n",
    "        self.output_l_network=network(n_output_layer,output_bias)\n",
    "        self.n_input_layer=n_input_layer\n",
    "        # self.hidden_weight_layer=self.add_weights_to_hidden(hidden_weights)\n",
    "        # self.output_weight_layer=self.add_weights_to_output(output_weights)\n",
    "        self.add_weights(self.hidden_l_network,hidden_weights,n_input_layer)\n",
    "        self.add_weights(self.output_l_network,output_weights,n_hidden_layer)\n",
    "\n",
    "\n",
    "    def add_weights(self, layer, weights, num_inputs):\n",
    "        count = 0\n",
    "        for neuron in layer.network:\n",
    "            neuron.weights = weights[count:count + num_inputs]\n",
    "            count += num_inputs\n",
    "    # def add_weights_to_output(self,output_weights):\n",
    "    #     count=0\n",
    "    #     for i in range(len(self.output_l_network.network)):\n",
    "    #         # print(hidden_l_network.network[i].weight)\n",
    "    #         for j in range((self.n_input_layer)):\n",
    "    #             self.output_l_network.network[i].weight.append(output_weights[count])\n",
    "    #             count+=1\n",
    "\n",
    "    def forward_hidden_op(self,input_data):\n",
    "        # hidden_op=self.hidden_l_network.forward(input_data)\n",
    "        # self.hidden_l_cost=self.hidden_l_network.forward(input_data)\n",
    "        # print(\"Hidden layer outputs:\", self.hidden_l_cost)\n",
    "        # self.output_l_cost=self.output_l_network.forward(self.hidden_l_cost)\n",
    "        # print(\"Output layer outputs:\", self.output_l_cost)  # Print output layer outputs\n",
    "        hidden_output = self.hidden_l_network.forward(input_data)\n",
    "        # print(f\"Hidden Layer Output: {hidden_output}\")\n",
    "        output = self.output_l_network.forward(hidden_output)\n",
    "        # print(f\"Output Layer Output: {output}\")\n",
    "\n",
    "        # return np.mean(output)\n",
    "        return output\n",
    "    \n",
    "    def backPropogation(self, input_data, target_op,learning_rate=0.5):\n",
    "        # Output layer deltas\n",
    "        # self.forward_hidden_op(target_inp)\n",
    "        hidden_output = self.hidden_l_network.forward(input_data)\n",
    "        final_output = self.output_l_network.forward(hidden_output)\n",
    "\n",
    "        # output_deltas = [0] * len(self.output_l_network.network)\n",
    "        # for i in range(len(self.output_l_network.network)):\n",
    "        #     output_deltas[i] = self.output_l_network.network[i].cal_error(target_op[i])\n",
    "\n",
    "        # output_deltas = []\n",
    "        output_deltas = [\n",
    "            (target - output) * neuron.error_wrt_input()\n",
    "            for target, output, neuron in zip(target_op, final_output, self.output_l_network.network)\n",
    "        ]\n",
    "        # hidden_deltas=[]\n",
    "        \n",
    "        # Hidden layer deltas\n",
    "        hidden_deltas = []\n",
    "        for i, hidden_neuron in enumerate(self.hidden_l_network.network):\n",
    "            weighted_sum = sum(\n",
    "                delta * output_neuron.weights[i]\n",
    "                for delta, output_neuron in zip(output_deltas, self.output_l_network.network)\n",
    "            )\n",
    "            hidden_deltas.append(weighted_sum * hidden_neuron.error_wrt_input())\n",
    "\n",
    "        # Update output layer weights and biases\n",
    "        for neuron, delta in zip(self.output_l_network.network, output_deltas):\n",
    "            neuron.weights = [\n",
    "                weight + learning_rate * delta * hidden_output[i]\n",
    "                for i, weight in enumerate(neuron.weights)\n",
    "            ]\n",
    "            neuron.bias += learning_rate * delta\n",
    "\n",
    "        # Update hidden layer weights and biases\n",
    "        for neuron, delta in zip(self.hidden_l_network.network, hidden_deltas):\n",
    "            neuron.weights = [\n",
    "                weight + learning_rate * delta * input_data[i]\n",
    "                for i, weight in enumerate(neuron.weights)\n",
    "            ]\n",
    "            neuron.bias += learning_rate * delta\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_total_error(self, dataset):\n",
    "        total_error = 0\n",
    "        for input_data, target_op in dataset:\n",
    "            outputs = self.forward_hidden_op(input_data)\n",
    "            total_error += sum(0.5 * (target_op[i] - outputs[i]) ** 2 for i in range(len(target_op)))\n",
    "        return total_error\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class probabilities: [0.49460936 0.50539064]\n"
     ]
    }
   ],
   "source": [
    "nn = all_network(2, 2, 2, hidden_weights=[0.15, 0.2, 0.25, 0.3], hidden_bias=0.35, output_weights=[0.4, 0.45, 0.5, 0.55], output_bias=0.6)\n",
    "a=nn.forward_hidden_op([0.05, 0.1])\n",
    "exp_logits = np.exp(a - np.max(a))  # Normalize for numerical stability\n",
    "probabilities = exp_logits / np.sum(exp_logits)\n",
    "print(\"Class probabilities:\", probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train_1 = X_train\n",
    "y_train_1 = y_train\n",
    "X_test_1 = X_test\n",
    "y_test_1 = y_test\n",
    "X_train = X_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "X_test = X_test[:10000]\n",
    "y_test = y_test[:10000]\n",
    "\n",
    "# print(X_train_1.shape)\n",
    "# print(y_train_1.shape)\n",
    "# print(X_test_1.shape)\n",
    "# print(y_test_1.shape)\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0  \n",
    "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "input_size = 784\n",
    "hidden_layer_size = 64\n",
    "output_layer_size = 10\n",
    "\n",
    "hidden_weights = np.random.rand(hidden_layer_size * input_size) * 0.1\n",
    "output_weights = np.random.rand(output_layer_size * hidden_layer_size) * 0.1\n",
    "hidden_bias = 0.1\n",
    "output_bias = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "images = [X_test_1[1111], X_test_1[2222], X_test_1[3333], X_test_1[5555]]\n",
    "for ax, img in zip(axes, images):\n",
    "    ax.imshow(img, cmap=\"gray\")  \n",
    "    ax.axis(\"off\")  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment below to train on 10000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 64\n",
      "Before training: 66\n",
      "Before training: 65\n",
      "Before training: 63\n",
      "Epoch 1/5, Total Error: 4036.792085\n",
      "Epoch 2/5, Total Error: 2885.090342\n",
      "Epoch 3/5, Total Error: 1073.855171\n",
      "Epoch 4/5, Total Error: 550.843854\n",
      "Epoch 5/5, Total Error: 441.958124\n",
      "\n",
      "Accuracy on test data: 89.19%\n",
      "After training: 7\n",
      "After training: 23\n",
      "After training: 20\n",
      "After training: 6\n",
      "Actual class: 4\n",
      "Actual class: 3\n",
      "Actual class: 7\n",
      "Actual class: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# nn = all_network( n_input_layer=input_size, n_hidden_layer=hidden_layer_size, n_output_layer=output_layer_size, hidden_weights=hidden_weights.tolist(), hidden_bias=hidden_bias, output_weights=output_weights.tolist(), output_bias=output_bias )\n",
    "\n",
    "\n",
    "# epochs = 5\n",
    "# learning_rate = 0.1\n",
    "# batch_size = 32\n",
    "# output = nn.forward_hidden_op(X_test_1[1111])\n",
    "# predicted_class = np.argmax(output)\n",
    "# print(\"Before training:\", predicted_class)\n",
    "# output = nn.forward_hidden_op(X_test_1[2222])\n",
    "# predicted_class = np.argmax(output)\n",
    "# print(\"Before training:\", predicted_class)\n",
    "# output = nn.forward_hidden_op(X_test_1[3333])\n",
    "# predicted_class = np.argmax(output)\n",
    "# print(\"Before training:\", predicted_class)\n",
    "# output = nn.forward_hidden_op(X_test_1[5555])\n",
    "# predicted_class = np.argmax(output)\n",
    "# print(\"Before training:\", predicted_class)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_error = 0\n",
    "#     for i in range(0, len(X_train), batch_size):\n",
    "#         batch_X = X_train[i:i+batch_size]\n",
    "#         batch_y = y_train_onehot[i:i+batch_size]\n",
    "#         for input_data, target_output in zip(batch_X, batch_y):\n",
    "#             nn.backPropogation(input_data.tolist(), target_output.tolist(), learning_rate)\n",
    "#             total_error += nn.calculate_total_error([(input_data.tolist(), target_output.tolist())])\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}, Total Error: {total_error:.6f}\")\n",
    "\n",
    "\n",
    "# correct_predictions = 0\n",
    "# for input_data, target_label in zip(X_test, y_test):\n",
    "#     predicted_output = nn.forward_hidden_op(input_data.tolist())\n",
    "#     predicted_class = predicted_output.index(max(predicted_output))\n",
    "#     if predicted_class == target_label:\n",
    "#         correct_predictions += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training: 4\n",
      "After training: 3\n",
      "After training: 9\n",
      "After training: 3\n",
      "Actual class: 4\n",
      "Actual class: 3\n",
      "Actual class: 7\n",
      "Actual class: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# accuracy = correct_predictions / len(X_test)\n",
    "# print(f\"\\nAccuracy on test data: {accuracy * 100}%\")\n",
    "output = nn.forward_hidden_op(X_test[1111])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test[2222])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test[3333])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test[5555])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "\n",
    "\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[1111])\n",
    "print(\"Actual class:\", actual_class)\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[2222])\n",
    "print(\"Actual class:\", actual_class)\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[3333])\n",
    "print(\"Actual class:\", actual_class)\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[5555])\n",
    "print(\"Actual class:\", actual_class)\n",
    "# print(\"after train\",output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"mnist_network_model_on_10000.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(nn, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment below to train on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training: 2\n",
      "Epoch 1/5, Total Error: 4036.792085\n",
      "Epoch 2/5, Total Error: 2885.090342\n",
      "Epoch 3/5, Total Error: 1073.855171\n",
      "Epoch 4/5, Total Error: 550.843854\n",
      "Epoch 5/5, Total Error: 441.958124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# # X_train = X_train\n",
    "# # y_train = y_train\n",
    "# # X_test = X_test\n",
    "# # y_test = y_test\n",
    "# # X_train_1 = X_train[:1000]\n",
    "# # y_train_1 = y_train[:1000]\n",
    "# # X_test_1 = X_test[:1000]\n",
    "# # y_test_1 = y_test[:1000]\n",
    "\n",
    "# # X_train = X_train.reshape(X_train.shape[0], -1) / 255.0  \n",
    "# # X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "\n",
    "# # encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "# # y_train_onehot = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "# # y_test_onehot = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# # input_size = 784\n",
    "# # hidden_layer_size = 64\n",
    "# # output_layer_size = 10\n",
    "\n",
    "# # hidden_weights = np.random.rand(hidden_layer_size * input_size) * 0.1\n",
    "# # output_weights = np.random.rand(output_layer_size * hidden_layer_size) * 0.1\n",
    "# # hidden_bias = 0.1\n",
    "# # output_bias = 0.1\n",
    "\n",
    "# nn = all_network( n_input_layer=input_size, n_hidden_layer=hidden_layer_size, n_output_layer=output_layer_size, hidden_weights=hidden_weights.tolist(), hidden_bias=hidden_bias, output_weights=output_weights.tolist(), output_bias=output_bias )\n",
    "\n",
    "\n",
    "# epochs = 5\n",
    "# learning_rate = 0.1\n",
    "# batch_size = 32\n",
    "# output = nn.forward_hidden_op(X_test[0])\n",
    "# predicted_class = np.argmax(output)\n",
    "# print(\"Before training:\", predicted_class)\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_error = 0\n",
    "#     for i in range(0, len(X_train), batch_size):\n",
    "#         batch_X = X_train[i:i+batch_size]\n",
    "#         batch_y = y_train_onehot[i:i+batch_size]\n",
    "#         for input_data, target_output in zip(batch_X, batch_y):\n",
    "#             nn.backPropogation(input_data.tolist(), target_output.tolist(), learning_rate)\n",
    "#             total_error += nn.calculate_total_error([(input_data.tolist(), target_output.tolist())])\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}, Total Error: {total_error:.6f}\")\n",
    "\n",
    "\n",
    "# correct_predictions = 0\n",
    "# for input_data, target_label in zip(X_test, y_test):\n",
    "#     predicted_output = nn.forward_hidden_op(input_data.tolist())\n",
    "#     predicted_class = predicted_output.index(max(predicted_output))\n",
    "#     if predicted_class == target_label:\n",
    "#         correct_predictions += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist_network_model_all.pkl\", \"rb\") as file:\n",
    "    nn = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training: 2\n",
      "After training: 1\n",
      "After training: 0\n",
      "After training: 4\n",
      "Actual class: 2\n",
      "Actual class: 1\n",
      "Actual class: 0\n",
      "Actual class: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# accuracy = correct_predictions / len(X_test)\n",
    "# print(f\"\\nAccuracy on test data: {accuracy * 100}%\")\n",
    "output = nn.forward_hidden_op(X_test[1])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test[2])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test[3])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "output = nn.forward_hidden_op(X_test[4])\n",
    "predicted_class = np.argmax(output)\n",
    "print(\"After training:\", predicted_class)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[1])\n",
    "print(\"Actual class:\", actual_class)\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[2])\n",
    "print(\"Actual class:\", actual_class)\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[3])\n",
    "print(\"Actual class:\", actual_class)\n",
    "\n",
    "actual_class = np.argmax(y_test_onehot[4])\n",
    "print(\"Actual class:\", actual_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
