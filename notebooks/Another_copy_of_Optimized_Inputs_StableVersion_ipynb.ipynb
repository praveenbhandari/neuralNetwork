{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lovHNIh5BuDv"
      },
      "source": [
        "# Modified Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcggmytf_eLl"
      },
      "source": [
        "## Objective\n",
        "\n",
        "The objective of this notebook is to modify the Neural Network that runs on custom tau and bias values and dataset. This customization aims to aid in understanding the model training process and the data flow within the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY7ekxTvBz2D"
      },
      "source": [
        "## Step 1: Import libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1vcnRkRk6XS",
        "outputId": "70b7a1d3-f2b3-4fc2-ff6f-b8b95c7d3c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.2.5-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from reportlab) (11.1.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from reportlab) (5.2.0)\n",
            "Downloading reportlab-4.2.5-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.2.5\n"
          ]
        }
      ],
      "source": [
        "#Import the dataset and other python libraries\n",
        "\n",
        "!pip install reportlab\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import struct\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import sys\n",
        "import sympy as sp\n",
        "\n",
        "from tabulate import tabulate\n",
        "from array import array\n",
        "from os.path import join\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "from io import StringIO\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.utils import ImageReader\n",
        "\n",
        "def clear_seed():\n",
        "    np.random.seed(None)\n",
        "    random.seed(None)\n",
        "    os.environ.pop('PYTHONHASHSEED', None)\n",
        "\n",
        "# Call clear_seed at the beginning of the script\n",
        "clear_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFA2zc3YkkQe"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def reset_seed():\n",
        "    np.random.seed(None)\n",
        "    random.seed(None)\n",
        "    os.environ.pop('PYTHONHASHSEED', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jrg10v8KSvyT"
      },
      "outputs": [],
      "source": [
        "#This code handles the folders creation and saving results to review after the run\n",
        "\n",
        "def create_run_folder(base_folder, run_number): # Create a folder for each run\n",
        "    run_folder = os.path.join(base_folder, f\"run_{run_number}\")\n",
        "    os.makedirs(run_folder, exist_ok=True)\n",
        "    return run_folder\n",
        "\n",
        "\n",
        "def save_results(folder_path, run_number, x_train,x_train_1, y_train, bias_values, tau_values, errors, final_bias_values, final_tau_values, output_text, subfolder_name=None):\n",
        "    \"\"\"\n",
        "    Save the results under the specified folder path.\n",
        "    If subfolder_name is None, files will be saved directly under the folder path.\n",
        "    \"\"\"\n",
        "    # If subfolder_name is provided, create it under the folder path; otherwise, use the main folder path\n",
        "    if subfolder_name:\n",
        "        subfolder = os.path.join(folder_path, subfolder_name)\n",
        "    else:\n",
        "        subfolder = folder_path\n",
        "\n",
        "    os.makedirs(subfolder, exist_ok=True)\n",
        "\n",
        "    # Save x_train and y_train as .npy\n",
        "    np.save(os.path.join(subfolder, \"x_train.npy\"), x_train)\n",
        "    np.save(os.path.join(subfolder, \"y_train.npy\"), y_train)\n",
        "\n",
        "     # Save x_train and y_train as CSV files\n",
        "    pd.DataFrame(x_train).to_csv(os.path.join(subfolder, \"x_train.csv\"), index=False)\n",
        "    pd.DataFrame(x_train_1).to_csv(os.path.join(subfolder, \"x_train_1.csv\"), index=False)\n",
        "    pd.DataFrame(y_train).to_csv(os.path.join(subfolder, \"y_train.csv\"), index=False)\n",
        "\n",
        "    # Save x_train and y_train as .json\n",
        "    pd.DataFrame(x_train).to_json(os.path.join(subfolder, \"x_train.json\"))\n",
        "    pd.DataFrame(y_train).to_json(os.path.join(subfolder, \"y_train.json\"))\n",
        "\n",
        "    # Save bias_values and tau_values as .json\n",
        "    with open(os.path.join(subfolder, \"bias_values.json\"), \"w\") as f:\n",
        "        json.dump([b.tolist() for b in bias_values], f, indent=4)\n",
        "    with open(os.path.join(subfolder, \"tau_values.json\"), \"w\") as f:\n",
        "        json.dump([t.tolist() for t in tau_values], f, indent=4)\n",
        "\n",
        "    # Save final bias_values and tau_values as .json\n",
        "    with open(os.path.join(subfolder, \"final_bias_values.json\"), \"w\") as f:\n",
        "        json.dump([b.tolist() for b in final_bias_values], f, indent=4)\n",
        "    with open(os.path.join(subfolder, \"final_tau_values.json\"), \"w\") as f:\n",
        "        json.dump([t.tolist() for t in final_tau_values], f, indent=4)\n",
        "\n",
        "    # Save the errors array\n",
        "    errors_file_path = os.path.join(subfolder, \"errors.npy\")\n",
        "    np.save(errors_file_path, errors)\n",
        "    with open(os.path.join(subfolder, \"errors.json\"), \"w\") as f:\n",
        "        json.dump([b.tolist() for b in errors], f, indent=4)\n",
        "    print(f\"Errors saved to: {errors_file_path}\")\n",
        "\n",
        "    # Save the error plot\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, len(errors) + 1), errors, marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Average Error')\n",
        "    plt.title('Error Rate Across Epochs')\n",
        "    plt.savefig(os.path.join(subfolder, \"error_plot.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Save console output as .json\n",
        "    output_file_json = os.path.join(subfolder, \"output.json\")\n",
        "    with open(output_file_json, \"w\") as f:\n",
        "        json.dump({\"output\": output_text}, f, indent=4)\n",
        "\n",
        "    # Save console output as .pdf\n",
        "    output_file_pdf = os.path.join(subfolder, \"output.pdf\")\n",
        "    save_output_as_pdf(output_text, os.path.join(subfolder, \"error_plot.png\"), output_file_pdf, run_number)\n",
        "\n",
        "    # Copy parameters.json file to the run's subfolder\n",
        "    original_parameters_path = \"/content/DATA/parameters.json\"\n",
        "    parameters_destination_path = os.path.join(folder_path, \"parameters.json\")\n",
        "    shutil.copyfile(original_parameters_path, parameters_destination_path)\n",
        "\n",
        "def save_output_as_pdf(output_text, plot_file, file_path, run_number):\n",
        "    c = canvas.Canvas(file_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "\n",
        "    # Add a big title with the run number\n",
        "    c.setFont(\"Helvetica-Bold\", 20)\n",
        "    c.drawString(50, height - 50, f\"Run Number: {run_number}\")\n",
        "\n",
        "    # Add the console output text\n",
        "    c.setFont(\"Helvetica\", 10)\n",
        "    x_margin = 50\n",
        "    y_margin = height - 100\n",
        "    line_height = 12\n",
        "\n",
        "    lines = output_text.splitlines()\n",
        "    y_position = y_margin\n",
        "    for line in lines:\n",
        "        if y_position < 50:  # start a new page if there's no more space\n",
        "            c.showPage()\n",
        "            y_position = height - 50\n",
        "        c.drawString(x_margin, y_position, line)\n",
        "        y_position -= line_height\n",
        "\n",
        "    # Add the error plot to the PDF\n",
        "    if os.path.exists(plot_file):\n",
        "        c.showPage()  # Start a new page for the plot\n",
        "        c.drawImage(ImageReader(plot_file), 100, 400, width=400, height=300)\n",
        "\n",
        "    c.save()\n",
        "\n",
        "def save_console_output(run_number, output_text, final_error, run_folder, is_bias_update=False):\n",
        "    # Determine the subfolder based on whether it's a bias update or not\n",
        "    subfolder_name = \"bias_update_result\" if is_bias_update else \"original_result\"\n",
        "    run_folder_path = os.path.join(run_folder, subfolder_name)\n",
        "\n",
        "    # Ensure the run_folder exists\n",
        "    os.makedirs(run_folder_path, exist_ok=True)\n",
        "\n",
        "    # Save console output as JSON\n",
        "    output_file_json = os.path.join(run_folder_path, \"output.json\")\n",
        "    with open(output_file_json, \"w\") as f:\n",
        "        json.dump({\"output\": output_text}, f, indent=4)\n",
        "\n",
        "    # Define the path to the existing error plot (if any)\n",
        "    plot_file = os.path.join(run_folder_path, \"error_plot.png\")\n",
        "\n",
        "    # Save console output as PDF\n",
        "    output_file_pdf = os.path.join(run_folder_path, \"output.pdf\")\n",
        "    save_output_as_pdf(output_text, plot_file, output_file_pdf, run_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvo9ienknvhQ"
      },
      "outputs": [],
      "source": [
        "#Loads the particular data for specific re-run\n",
        "\n",
        "def load_run_data(run_folder, base_folder):\n",
        "    parameters_file_path = os.path.join(base_folder, \"parameters.json\")\n",
        "    with open(parameters_file_path, \"r\") as file:\n",
        "        parameters = json.load(file)\n",
        "\n",
        "    num_training_inputs = parameters['num_training_inputs']\n",
        "    lower_bound = parameters['lower_bound']\n",
        "    max_lower_bound = parameters['max_lower_bound']\n",
        "    upper_bound = parameters['upper_bound']\n",
        "    min_upper_bound = parameters['min_upper_bound']\n",
        "    func_expressions = parameters['func_expressions']\n",
        "    num_layers = parameters['num_layers']\n",
        "    num_neurons = parameters['num_neurons']\n",
        "    epochs = parameters['epochs']\n",
        "    bias_learning_rate = parameters['bias_learning_rate']\n",
        "    tau_learning_rate = parameters['tau_learning_rate']\n",
        "    # tau_bouncing_mode = parameters['tau_bouncing_mode']\n",
        "    output_num_neurons =  parameters['output_num_neurons']\n",
        "    num_input_neurons =  parameters['num_input_neurons']\n",
        "\n",
        "    # Load x_train and y_train\n",
        "    x_train = np.load(os.path.join(run_folder, \"x_train.npy\"))\n",
        "    y_train = np.load(os.path.join(run_folder, \"y_train.npy\"))\n",
        "\n",
        "    # Load bias_values and tau_values\n",
        "    with open(os.path.join(run_folder, \"bias_values.json\"), \"r\") as f:\n",
        "        bias_values = [np.array(b) for b in json.load(f)]\n",
        "    with open(os.path.join(run_folder, \"tau_values.json\"), \"r\") as f:\n",
        "        tau_values = [np.array(t) for t in json.load(f)]\n",
        "\n",
        "    return num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, num_layers, num_neurons, num_input_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, x_train, y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLdYDRTURvZ0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rerun_specific_run(run_number, base_folder):\n",
        "    set_seed(42)  # Set the seed only for reruns\n",
        "\n",
        "    # Define the folder where the run data is stored\n",
        "    run_folder = os.path.join(base_folder, f\"run_{run_number}\", 'original_result')\n",
        "\n",
        "    if not os.path.exists(run_folder):\n",
        "        print(f\"Run folder for run number {run_number} does not exist.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Rerunning run number: {run_number} from folder: {run_folder}\")\n",
        "\n",
        "    # Load data from the run folder\n",
        "    num_training_inputs, lower_bound, max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, num_input_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, x_train, y_train = load_run_data(run_folder, base_folder)\n",
        "\n",
        "    net = Network()\n",
        "    output_num_neurons = num_neurons[-1] #added by KW - present in the original code, not sure the imapct of this change\n",
        "    for i in range(num_layers):\n",
        "        if i == num_layers - 1:  # last layer\n",
        "            net.add(FCLayer(num_neurons[i], output_num_neurons, f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
        "        else:\n",
        "            net.add(FCLayer(num_neurons[i], num_neurons[i + 1], f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
        "        net.add(ActivationLayer(ReLU))\n",
        "\n",
        "    net.use(mse)\n",
        "\n",
        "    # No capturing of console output, just printing everything to the console - not saving the results\n",
        "    errors, final_bias_values, final_tau_values = net.fit(x_train, y_train, epochs, bias_learning_rate, tau_learning_rate)\n",
        "\n",
        "    final_error = errors[-1]\n",
        "    print(f\"Re-Run of run number {run_number} completed with final error: {final_error}\")\n",
        "\n",
        "    reset_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_C6hC97xEay"
      },
      "outputs": [],
      "source": [
        "# function to check if the last two epochs' error difference is constant\n",
        "def is_error_stable(errors):\n",
        "    # Set a tolerance level for floating-point comparisons\n",
        "    tolerance = 1e-10\n",
        "\n",
        "    # Ensure we have at least three error values to compare\n",
        "    if len(errors) < 3:\n",
        "        return False  # Not enough data to determine consistency\n",
        "\n",
        "    # Calculate differences between consecutive error values\n",
        "    print('error 1 :',errors[-1])\n",
        "    print('error 2 :',errors[-2])\n",
        "    print('error 3 :',errors[-3])\n",
        "    diffs = np.diff(errors[-3:])\n",
        "\n",
        "    # Case of stagnation: All differences are zero\n",
        "    if np.all(np.abs(diffs) < tolerance):\n",
        "      print('Error Status: Stagnant')\n",
        "      return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nlFVBVTknA_"
      },
      "outputs": [],
      "source": [
        "# not used anywhere\n",
        "\n",
        "def get_new_parameter_value(param_name):\n",
        "    # Prompt the user for the new value of the specified parameter\n",
        "    return float(input(f\"Enter new value for {param_name}: \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFtxOzoq3yUy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Update parameters for a specific run and rerun the process with the new values.\n",
        "\n",
        "    This function fetches the original data for the specified run number, updates parameters on the provided\n",
        "    `new_value` flags, and then reruns the neural network training process with these updated parameters.\n",
        "\n",
        "    Args:\n",
        "        run_number (int): The number of the run to update.\n",
        "        base_folder (str): The base directory where run data is stored.\n",
        "        new_value (list of tuples): A list containing tuples that represent whether a parameter should be updated\n",
        "                                     (1) or not (0), along with the new value for the parameter if it is to be updated.\n",
        "                                     Format: [(bias_update_flag, new_bias_learning_rate),\n",
        "                                              (tau_update_flag, new_tau_learning_rate),\n",
        "                                              (taubounce_update_flag),\n",
        "                                              (epochs_update_flag, new_epochs),\n",
        "                                              (dataset_update_flag, new_dataset)].\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the run number and the rounded final error after rerunning the training.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the original run data folder does not exist.\n",
        "\n",
        "    Example:\n",
        "        run_number = 2\n",
        "        base_folder = \"/path/to/data\"\n",
        "        new_value = [(1, 0.01), (0, None), (1, 'yes'), (1, 100), (0, None)]\n",
        "        result = update_rerun_parameter(run_number, base_folder, new_value)\n",
        "    \"\"\"\n",
        "# def update_rerun_parameter(run_number, base_folder, param_to_update, new_value):\n",
        "def update_rerun_parameter(run_number, base_folder, new_value):\n",
        "\n",
        "    run_folder = os.path.join(base_folder, f\"run_{run_number}\", 'original_result')\n",
        "    if os.path.exists(run_folder):\n",
        "       # Load the original data and parameters\n",
        "      num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, num_layers, num_neurons, num_input_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, x_train, y_train = load_run_data(run_folder, base_folder)\n",
        "      # Initialize debug_mode as 'no' by default\n",
        "      debug_mode = 'no'\n",
        "\n",
        "       # Create the folder structure for parameter updates\n",
        "      # parameter_update_folder = os.path.join(base_folder, f\"run_{run_number}\", \"parameter_update_result\", f\"{param_to_update}_result\")\n",
        "      parameter_update_folder = os.path.join(base_folder, f\"run_{run_number}\", \"parameter_update_result\")\n",
        "      os.makedirs(parameter_update_folder, exist_ok=True)\n",
        "\n",
        "      # Capture the console output\n",
        "      original_stdout = sys.stdout\n",
        "      sys.stdout = StringIO()\n",
        "\n",
        "       # Update the specific parameter\n",
        "      # if param_to_update == 'tau':\n",
        "      #   tau_learning_rate = new_value\n",
        "      # elif param_to_update == 'bias':\n",
        "      #   bias_learning_rate = new_value\n",
        "      # elif param_to_update == 'epochs':\n",
        "      #   epochs = int(new_value)\n",
        "      # elif param_to_update == 'taubounce':\n",
        "      #   tau_bouncing_mode = new_value\n",
        "\n",
        "      # Update the specific parameters based on the flags in new_value\n",
        "      if new_value[0][0] == 1:  # Update bias learning rate\n",
        "            bias_learning_rate = new_value[0][1]\n",
        "      if new_value[1][0] == 1:  # Update tau learning rate\n",
        "            tau_learning_rate = new_value[1][1]\n",
        "      # if new_value[2][0] == 1:  # Update tau bouncing mode\n",
        "      #       tau_bouncing_mode = new_value[2][1]\n",
        "      if new_value[2][0] == 1:  # Update epochs\n",
        "            epochs = int(new_value[2][1])\n",
        "      if new_value[3][0] == 1:  # Update dataset\n",
        "            # x_train, y_train = generate_data(num_neurons[0], \"/content/DATA\", True)\n",
        "            x_train, y_train = generate_data(num_neurons[0], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, \"/content/DATA\", True)\n",
        "      if new_value[4][0] == 1:  # Enter the debug mode\n",
        "            debug_mode = new_value[4][1]\n",
        "\n",
        "      try:\n",
        "        # Rerun the process for this specific run with the new parameters' value\n",
        "        net = Network()\n",
        "        for i in range(num_layers):\n",
        "          if i == num_layers - 1:  # last layer\n",
        "            net.add(FCLayer(num_neurons[i], output_num_neurons, f'Layer {i}', bias=bias_values[i], tau=tau_values[i]))\n",
        "          else:\n",
        "            net.add(FCLayer(num_neurons[i], num_neurons[i + 1], f'Layer {i}', bias=bias_values[i], tau=tau_values[i]))\n",
        "          net.add(ActivationLayer(ReLU))\n",
        "\n",
        "        net.use(mse)\n",
        "        errors, final_bias_values, final_tau_values = net.fit(x_train, y_train, epochs, bias_learning_rate, tau_learning_rate, debug_mode)\n",
        "\n",
        "        final_error = errors[-1]\n",
        "        rounded_final_error = round(final_error, 2)\n",
        "\n",
        "        # Save results directly in the update folder without the original_result subfolder\n",
        "        save_results(parameter_update_folder, run_number, x_train,[], y_train, bias_values, tau_values, errors, final_bias_values, final_tau_values, sys.stdout.getvalue())\n",
        "\n",
        "      finally:\n",
        "                # Restore the original stdout\n",
        "                sys.stdout = original_stdout\n",
        "\n",
        "    else:\n",
        "            print(f\"Run folder for run number {run_number} does not exist.\")\n",
        "\n",
        "    # return new_run_summary\n",
        "    return (run_number, rounded_final_error)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzmNUP0MGUxp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Manage the rerunning or updating of neural network training parameters based on user input.\n",
        "\n",
        "    This function prompts the user to decide whether to rerun an existing training run or update the parameters\n",
        "    for all runs. It collects new parameter values if necessary, calls the `update_rerun_parameter` function to\n",
        "    perform the rerun or update, and displays a comparison of errors before and after the update.\n",
        "\n",
        "    Args:\n",
        "        rerun_or_update (str): A string indicating the operation to perform; either 'rerun' or 'update'.\n",
        "        base_folder (str): The base directory where the run data is stored.\n",
        "\n",
        "    Returns:\n",
        "        None: This function does not return a value but prints the results and comparison tables to the console.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the user inputs an invalid run number or if binary input for updates is not valid.\n",
        "\n",
        "    Example:\n",
        "        manage_rerun_update(\"update\", \"/path/to/data\")\n",
        "    \"\"\"\n",
        "\n",
        "def manage_rerun_update(rerun_or_update, base_folder):\n",
        "\n",
        "    new_run_summary = []\n",
        "    # Initialize new_value to have 6 elements as (0, None)\n",
        "    new_value = [(0, None) for _ in range(6)]\n",
        "\n",
        "    if rerun_or_update == \"rerun\":\n",
        "      rerun_number = int(get_valid_input( f\"Enter the run number to update the parameters for (between 1 to {num_runs}): \", lambda x: validate_run_number(x, num_runs),\n",
        "        f\"Please enter a run number.\" ))\n",
        "    update_type = get_valid_input(\n",
        "    \"Do you want to update the bias learning rate, tau learning rate, epochs, change the dataset, or enter debug mode? Enter binary digits (e.g., 01001) 0 = no, 1 = yes: \",\n",
        "    validate_binary_string, \"Invalid input. Please enter a 5-digit binary string like '01001'.\" )\n",
        "\n",
        "    # Parse the binary flags (e.g., '0110' -> bias: 0, tau: 1, taubounce: 1, epochs: 0)\n",
        "    for i, flag in enumerate(update_type):\n",
        "      if flag == '1':\n",
        "        if i == 0:\n",
        "            new_bias_learning_rate = get_valid_input(\"Enter new value for bias learning rate: \", is_valid_number, \"Invalid input.\")\n",
        "            new_value[i] = (1, float(new_bias_learning_rate))  # Assign to the correct index\n",
        "        elif i == 1:\n",
        "            new_tau_learning_rate = get_valid_input(\"Enter new value for tau learning rate: \", is_valid_number, \"Invalid input.\")\n",
        "            new_value[i] = (1, float(new_tau_learning_rate))  # Assign to the correct index\n",
        "        # elif i == 2:\n",
        "        #     new_tau_bouncing_mode = get_valid_input(\"Tau Bouncing Mode (yes/no): \", validate_yes_no, \"Enter either 'yes' or 'no'\")\n",
        "        #     new_value[i] = (1, new_tau_bouncing_mode)  # Assign to the correct index\n",
        "        elif i == 2:\n",
        "            new_epochs = get_valid_input(\"Enter new value for epochs: \", is_valid_number, \"Invalid input.\")\n",
        "            new_value[i] = (1, int(new_epochs))  # Assign to the correct index\n",
        "        elif i == 3:\n",
        "            new_dataset = get_valid_input(\"Do you want to generate a new dataset? (yes/no): \", validate_yes_no, \"Enter either 'yes' or 'no'\")\n",
        "            new_value[i] = (1, new_dataset)  # Assign to the correct index\n",
        "        elif i == 4:\n",
        "            new_debug = get_valid_input(\"Do you want to enter debug mode (yes/no): \", validate_yes_no, \"Enter either 'yes' or 'no'\")\n",
        "            new_value[i] = (1, new_debug)  # Assign to the correct index\n",
        "\n",
        "    if rerun_or_update == \"update\":\n",
        "      for run_number in range(1, num_runs + 1):\n",
        "        # run_summary_tuple = update_rerun_parameter(run_number, base_folder, param_to_update, new_value)\n",
        "        run_summary_tuple = update_rerun_parameter(run_number, base_folder, new_value)\n",
        "        new_run_summary.append(run_summary_tuple)\n",
        "\n",
        "      print(\"Update completed with updated parameters.\")\n",
        "\n",
        "      # Generate comparison table\n",
        "      headers = [\"Former Run Number\", \"Former Error\", \"New Error\"]\n",
        "      comparison_table = [[i+1, former_run_summary[i][1], new_run_summary[i][1]] for i in range(len(new_run_summary))]\n",
        "      print(tabulate(comparison_table, headers, tablefmt=\"grid\"))\n",
        "\n",
        "    elif rerun_or_update == \"rerun\":\n",
        "      # run_summary_tuple = update_rerun_parameter(rerun_number, base_folder, param_to_update, new_value)\n",
        "      run_summary_tuple = update_rerun_parameter(rerun_number, base_folder, new_value)\n",
        "      new_run_summary.append(run_summary_tuple)\n",
        "\n",
        "      print(f\"Rerun for run number {rerun_number} completed with updated {update_type}.\")\n",
        "\n",
        "      # Generate comparison table using former_run_summary\n",
        "      headers = [\"Run Number\", \"Former Error\", \"New Error\"]\n",
        "      comparison_table = [[rerun_number, former_run_summary[rerun_number - 1][1], new_run_summary[0][1]]]\n",
        "      print(tabulate(comparison_table, headers, tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7fPdnpLYeRo"
      },
      "outputs": [],
      "source": [
        "#The clear_folders function is used to clear out a folder completely, removing all of its contents, and then recreating an empty folder with the same name\n",
        "\n",
        "def clear_folders(base_folder):\n",
        "    if os.path.exists(base_folder):\n",
        "        shutil.rmtree(base_folder)\n",
        "    os.makedirs(base_folder, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wylo1P2OgLWu"
      },
      "outputs": [],
      "source": [
        "#The zip_folder function compresses the contents of a folder into a ZIP file\n",
        "\n",
        "def zip_folder(folder_path, zip_name):\n",
        "    with ZipFile(zip_name, 'w') as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                zipf.write(file_path, os.path.relpath(file_path, folder_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yToH5TnxSQpg"
      },
      "outputs": [],
      "source": [
        "#Function to get user inputs - This function has two modes:\n",
        "#it either loads existing parameters from a file or prompts the user to input new parameters\n",
        "def get_input_with_default(prompt, default, data_type=str):\n",
        "    user_input = input(f\"{prompt} [{default}]: \")\n",
        "    return data_type(user_input) if user_input else default\n",
        "\n",
        "\n",
        "def get_user_inputs(create_new, run_number=1):\n",
        "    folder_path = \"/content/DATA\"\n",
        "    run_folder_path = os.path.join(folder_path, f\"run_{run_number}\")\n",
        "    parameters_file_path = os.path.join(folder_path, \"parameters.json\")\n",
        "\n",
        "    # Create the path to the parameters file within the specific run folder\n",
        "    # parameters_file_path = os.path.join(run_folder_path, \"parameters.json\")\n",
        "\n",
        "    if create_new and run_number == 1:\n",
        "        print(\"Clearing existing files and creating new parameters...\")\n",
        "        clear_folders(folder_path)\n",
        "\n",
        "    # if not create_new and os.path.exists(parameters_file_path):\n",
        "    if not create_new:\n",
        "      if os.path.exists(parameters_file_path):\n",
        "            print(f\"Loading parameters from {parameters_file_path} in common folder.\")\n",
        "      else:\n",
        "        # If it doesn't exist in the common folder, check the run folder\n",
        "        parameters_file_path = os.path.join(run_folder_path, \"parameters.json\")\n",
        "        if os.path.exists(parameters_file_path):\n",
        "                print(f\"Loading parameters from {parameters_file_path} in run folder.\")\n",
        "        else:\n",
        "                raise FileNotFoundError(\"Parameters file not found in either run or common folder.\")\n",
        "\n",
        "      with open(parameters_file_path, \"r\") as file:\n",
        "          parameters = json.load(file)\n",
        "      num_training_inputs = parameters['num_training_inputs']\n",
        "      lower_bound = parameters['lower_bound']\n",
        "      max_lower_bound = parameters['max_lower_bound']\n",
        "      upper_bound = parameters['upper_bound']\n",
        "      min_upper_bound = parameters['min_upper_bound']\n",
        "      func_expressions = parameters['func_expressions']\n",
        "      num_layers = parameters['num_layers']\n",
        "      num_neurons = parameters['num_neurons']\n",
        "      output_num_neurons =  parameters['output_num_neurons']\n",
        "      epochs = parameters['epochs']\n",
        "      bias_learning_rate = parameters['bias_learning_rate']\n",
        "      tau_learning_rate = parameters['tau_learning_rate']\n",
        "      bias_values = [np.array(bias) for bias in parameters['bias_values']]\n",
        "      tau_values = [np.array(tau) for tau in parameters['tau_values']]\n",
        "\n",
        "      folder_path = \"/content/DATA\"\n",
        "\n",
        "      # return num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path  # Ensure initial values are returned\n",
        "      return num_training_inputs, lower_bound, max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, folder_path  # Ensure initial values are returned\n",
        "\n",
        "    else:\n",
        "        print(\"No parameters file found or user chose to create new parameters. Asking user for inputs...\")\n",
        "\n",
        "        # Ask for training data details\n",
        "        num_training_inputs = get_input_with_default(\"Enter the number of training inputs (total number of samples) in the dataset\", 50, int)\n",
        "        num_input_neurons = get_input_with_default(\"Enter the number of neurons for the input layer (>=1)\", 2, int)\n",
        "        lower_bound = get_input_with_default(\"Enter the lower bound of input range\", 0.0, float)\n",
        "        max_lower_bound = get_input_with_default(\"Enter the max lower bound for input\", 0.45, float)\n",
        "        upper_bound = get_input_with_default(\"Enter the upper bound of input range\", 1.0, float)\n",
        "        min_upper_bound = get_input_with_default(\"Enter the min upper bound for input\", 0.55, float)\n",
        "        output_num_neurons = get_input_with_default(\"Enter the number of neurons for the output layer (>=1)\", 2, int)\n",
        "\n",
        "        # Collect function expressions before running\n",
        "        func_expressions = []\n",
        "        for i in range(output_num_neurons):\n",
        "            func_str = input(f\"Enter y_train({i+1}) function using symbols x0, x1, x2,...(use '&' for AND, '|' for OR): \")\n",
        "            func_expressions.append(func_str)\n",
        "\n",
        "        num_layers = get_input_with_default(\"Enter the total number of layers (>=2): \", 3, int)\n",
        "        num_layers -= 2   # Subtracting input and output layers\n",
        "        hidden_num_neurons = [int(input(f\"Enter the number of neurons for layer {i+1}: \")) for i in range(num_layers)]\n",
        "        num_neurons = [num_input_neurons] + hidden_num_neurons\n",
        "        epochs = int(input(\"Enter the number of epochs: \"))\n",
        "        bias_learning_rate = get_input_with_default(\"Enter the bias learning rate : \", 5.0, float)\n",
        "        tau_learning_rate = get_input_with_default(\"Enter the tau learning rate : \", 3.0, float)\n",
        "        bias_values = []\n",
        "        tau_values = []\n",
        "        num_layers= num_layers+1\n",
        "        # values_choice = input(\"Enter '0' to manually input values for bias and tau, or '1' to generate random values: \")\n",
        "        values_choice = 1\n",
        "\n",
        "        if values_choice == '0':\n",
        "          #Input for layers except the output layer\n",
        "            for i in range(1, num_layers):\n",
        "                current_layer_neurons = num_neurons[i]\n",
        "                previous_layer_neurons = num_neurons[i - 1]\n",
        "\n",
        "                # Input for each neuron in the current layer\n",
        "                layer_biases = []\n",
        "                layer_taus = []\n",
        "\n",
        "                for j in range(current_layer_neurons):\n",
        "                    for k in range(previous_layer_neurons):\n",
        "                        layer_biases.append(int(input(f\"Enter bias value for hidden layer {i}, neuron {k+1}: \")))\n",
        "                        layer_taus.append(float(input(f\"Enter tau value for hidden layer {i}, neuron {k+1}: \")))\n",
        "\n",
        "                #Reshape and append for layers except the last layer\n",
        "                layer_biases = np.array(layer_biases).reshape(num_neurons[i], num_neurons[i-1])\n",
        "                layer_taus = np.array(layer_taus).reshape(num_neurons[i], num_neurons[i-1])\n",
        "                bias_values.append(layer_biases)\n",
        "                tau_values.append(layer_taus)\n",
        "\n",
        "            #Input for the output layer separately\n",
        "            layer_biases = []\n",
        "            layer_taus = []\n",
        "            previous_layer_neurons = num_neurons[-2]\n",
        "\n",
        "            for j in range(num_neurons[-1]):\n",
        "                for k in range(previous_layer_neurons):\n",
        "                    layer_biases.append(int(input(f\"Enter bias value for output layer, neuron {k+1}: \")))\n",
        "                    layer_taus.append(float(input(f\"Enter tau value for output layer, neuron {k+1}: \")))\n",
        "\n",
        "            #Reshape and append for the output layer\n",
        "            layer_biases = np.array(layer_biases).reshape(output_num_neurons, num_neurons[-1])\n",
        "            layer_taus = np.array(layer_taus).reshape(output_num_neurons, num_neurons[-1])\n",
        "            bias_values.append(layer_biases)\n",
        "            tau_values.append(layer_taus)\n",
        "\n",
        "        else:\n",
        "           #randomly initialized bias values\n",
        "            bias_values = [np.random.randint(0, 2, (num_neurons[i + 1], num_neurons[i])) for i in range(num_layers - 1)]\n",
        "            #Reshape and append bias for the output layer\n",
        "            bias_values.append(np.random.randint(0, 2, (output_num_neurons, num_neurons[-1])))\n",
        "\n",
        "            #randomly initialized tau values\n",
        "            tau_values = [np.round(np.random.rand(num_neurons[i + 1], num_neurons[i]) * 0.8+0.1, decimals=1) for i in range(num_layers - 1)]\n",
        "            #Reshape and append tau for the output layer\n",
        "            tau_values.append(np.round(np.random.rand(output_num_neurons, num_neurons[-1]) * 0.8+0.1, decimals=1))\n",
        "\n",
        "\n",
        "        #prepare a dictionary of parameters, save these parameters to a JSON file, and return them\n",
        "        parameters = {\n",
        "            'num_training_inputs': num_training_inputs,\n",
        "            'lower_bound': lower_bound,\n",
        "            'max_lower_bound': max_lower_bound,\n",
        "            'upper_bound': upper_bound,\n",
        "            'min_upper_bound': min_upper_bound,\n",
        "            'func_expressions': func_expressions,\n",
        "            'num_layers': num_layers,\n",
        "            'num_neurons' : num_neurons,\n",
        "            'num_input_neurons' : num_input_neurons,\n",
        "            'output_num_neurons': output_num_neurons,\n",
        "            'epochs': epochs,\n",
        "            'bias_learning_rate': bias_learning_rate,\n",
        "            'tau_learning_rate': tau_learning_rate,\n",
        "            'bias_values': [b.tolist() for b in bias_values],  # Ensure 2D structure\n",
        "            'tau_values': [t.tolist() for t in tau_values]\n",
        "        }\n",
        "        with open(parameters_file_path, \"w\") as file:\n",
        "            json.dump(parameters, file, indent=4)\n",
        "\n",
        "        # return num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path\n",
        "        return num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, folder_path\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nra_P8Poanye"
      },
      "source": [
        "Afsaneh\n",
        "Explanation for Different Cases for XOR function: Case 1: All inputs are the same\n",
        "\n",
        "Example: [1, 1, 1] Sum: 3, which is odd XOR output: 3 % 2 = 1 This is because an odd number of 1s results in 1. Case 2: One input is different\n",
        "\n",
        "Example: [1, 0, 1] Sum: 2, which is even XOR output: 2 % 2 = 0 This is because an even number of 1s results in 0. Case 3: All inputs are different\n",
        "\n",
        "Example: [0, 1, 0] Sum: 1, which is odd XOR output: 1 % 2 = 1 This is because an odd number of 1s results in 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZn-Vtp_daxv"
      },
      "outputs": [],
      "source": [
        "# # Function to generate random inputs and compute outputs for AND, OR, XOR gates\n",
        "# # def generate_data(num_input_neurons, output_num_neurons, folder_path, create_new):\n",
        "# def generate_data(num_input_neurons, output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, create_new):\n",
        "#   x_train_file_path = os.path.join(folder_path, \"x_train.npy\")\n",
        "#   y_train_file_path = os.path.join(folder_path, \"y_train.npy\")\n",
        "#   print ('value for create_new from generate_data: ', create_new)\n",
        "\n",
        "#   # Function to check if the operation is logical (like AND/OR)\n",
        "#   def is_logical_operation(func_str):\n",
        "#     logical_ops = ['&', '|', 'and', 'or']  # Add any other logical operators you want to support\n",
        "#     return any(op in func_str for op in logical_ops)\n",
        "\n",
        "#   print(\"x_train_file_path: \", x_train_file_path)\n",
        "#   print(\"y_train_file_path: \", y_train_file_path)\n",
        "#   if not create_new and os.path.exists(x_train_file_path) and os.path.exists(y_train_file_path):\n",
        "#     #if os.path.exists(x_train_file_path) and os.path.exists(y_train_file_path):\n",
        "#         print(\"Loading data from files...\")\n",
        "#         x_train = np.load(x_train_file_path)\n",
        "#         y_train = np.load(y_train_file_path)\n",
        "#   else:\n",
        "\n",
        "#         # Create X_train dataset with random values uniformly between the bounds\n",
        "#         x_train = np.random.uniform(lower_bound, upper_bound, (num_training_inputs, num_input_neurons))\n",
        "#         # Apply limiting conditions\n",
        "#         for i in range(num_input_neurons):\n",
        "#             mask = np.random.choice([0, 1], size=num_training_inputs)\n",
        "\n",
        "#             # Set values in the range [lower_bound, max_lower_bound]\n",
        "#             x_train[:, i][mask == 0] = np.random.uniform(lower_bound, max_lower_bound, size=(mask == 0).sum())\n",
        "\n",
        "#             # Set values in the range [min_upper_bound, upper_bound]\n",
        "#             x_train[:, i][mask == 1] = np.random.uniform(min_upper_bound, upper_bound, size=(mask == 1).sum())\n",
        "#         # Normalize the X_train data between 0 and 1\n",
        "#         x_train = np.round(x_train, 2)\n",
        "\n",
        "#         x_train_1=x_train\n",
        "\n",
        "#         # x_train_min = x_train.min(axis=0)\n",
        "#         # x_train_max = x_train.max(axis=0)\n",
        "#         # x_train = (x_train - x_train_min) / (x_train_max - x_train_min)\n",
        "#         # normalization\n",
        "#         x_train = (x_train - lower_bound) / (upper_bound - lower_bound)\n",
        "\n",
        "#         x_train = np.round(x_train, 2)\n",
        "\n",
        "#         # Create y_train by asking user to provide functions for each output\n",
        "#         y_train = []\n",
        "#         x_symbols = sp.symbols(f'x0:{num_input_neurons}')  # Creates symbols x0, x1, x2, ..., up to num_inputs\n",
        "\n",
        "#         # for i in  range(output_num_neurons):\n",
        "#         for func_str in  func_expressions:\n",
        "#           func = sp.sympify(func_str)  # Convert string to sympy expression\n",
        "#           func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
        "\n",
        "#           if is_logical_operation(func_str):\n",
        "#             # Apply thresholding if the function is a logical operation\n",
        "#             binary_x_train = (x_train > 0.5).astype(int)  # Convert to binary values based on threshold\n",
        "\n",
        "#             # func = sp.sympify(func_str)  # Convert string to sympy expression\n",
        "#             # func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
        "\n",
        "#             # Evaluate the function for each row\n",
        "#             y_values = np.array([func_callable(*binary_x_train[row]) for row in range(num_training_inputs)])\n",
        "#             # y_values = np.array([func_callable(*x_train[row]) for row in range(num_training_inputs)])\n",
        "\n",
        "#             # Convert boolean results to binary (0 and 1)\n",
        "#             y_values = y_values.astype(int)\n",
        "\n",
        "#           else:\n",
        "#             # For mathematical functions, apply directly on continuous values\n",
        "#             # func = sp.sympify(func_str)  # Convert string to sympy expression\n",
        "#             # func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
        "#             y_values = np.array([func_callable(*x_train[row]) for row in range(num_training_inputs)])\n",
        "#             # def evaluate_row(row):\n",
        "#             #         input_map = {f'x{i}': row[i] for i in range(len(row))}\n",
        "#             #         return float(func.subs(input_map).evalf())\n",
        "\n",
        "#             #     # Evaluate the function for each row\n",
        "#             # y_values = np.array([evaluate_row(x_train[row]) for row in range(num_training_inputs)])\n",
        "\n",
        "\n",
        "#           # Append results to y_train\n",
        "#           y_train.append(y_values)\n",
        "\n",
        "#       # Convert y_train to numpy array and transpose to match the format (num_training_inputs, num_outputs)\n",
        "#         y_train = np.array(y_train).T\n",
        "\n",
        "#         # print(\"\\ny_train:\")\n",
        "#         # print(y_train)\n",
        "\n",
        "#         # Save x_train and y_train to the folder path\n",
        "#         # np.save(x_train_file_path, x_train)\n",
        "#         # np.save(y_train_file_path, y_train)\n",
        "\n",
        "#         # dataset_file_path = os.path.join(folder_path, \"dataset.csv\")\n",
        "#         #dataset = pd.DataFrame(np.hstack((x_train, y_train)), columns=[f'Input_{i+1}' for i in range(num_input_neurons)] + ['AND', 'OR', 'XOR'])\n",
        "#         # dataset = pd.DataFrame(np.hstack((x_train, y_train)), columns=[f'Input_{i+1}' for i in range(num_input_neurons)] + [f'Output_{i+1}' for i in range(output_num_neurons)])\n",
        "\n",
        "#         # dataset.to_csv(dataset_file_path, index=False)\n",
        "\n",
        "#   return x_train, y_train,x_train_1\n",
        "\n",
        "# Function to generate random inputs and compute outputs for AND, OR, XOR gates\n",
        "# def generate_data(num_input_neurons, output_num_neurons, folder_path, create_new):\n",
        "def generate_data(num_input_neurons, output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, create_new):\n",
        "  x_train_file_path = os.path.join(folder_path, \"x_train.npy\")\n",
        "  y_train_file_path = os.path.join(folder_path, \"y_train.npy\")\n",
        "  print ('value for create_new from generate_data: ', create_new)\n",
        "\n",
        "  # Function to check if the operation is logical (like AND/OR)\n",
        "  def is_logical_operation(func_str):\n",
        "    logical_ops = ['&', '|', 'and', 'or']  # Add any other logical operators you want to support\n",
        "    return any(op in func_str for op in logical_ops)\n",
        "\n",
        "  print(\"x_train_file_path: \", x_train_file_path)\n",
        "  print(\"y_train_file_path: \", y_train_file_path)\n",
        "  if not create_new and os.path.exists(x_train_file_path) and os.path.exists(y_train_file_path):\n",
        "    #if os.path.exists(x_train_file_path) and os.path.exists(y_train_file_path):\n",
        "        print(\"Loading data from files...\")\n",
        "        x_train = np.load(x_train_file_path)\n",
        "        y_train = np.load(y_train_file_path)\n",
        "  else:\n",
        "\n",
        "        # Create X_train dataset with random values uniformly between the bounds\n",
        "        x_train = np.random.uniform(lower_bound, upper_bound, (num_training_inputs, num_input_neurons))\n",
        "        # Apply limiting conditions\n",
        "        for i in range(num_input_neurons):\n",
        "            mask = np.random.choice([0, 1], size=num_training_inputs)\n",
        "\n",
        "            # Set values in the range [lower_bound, max_lower_bound]\n",
        "            x_train[:, i][mask == 0] = np.random.uniform(lower_bound, max_lower_bound, size=(mask == 0).sum())\n",
        "\n",
        "            # Set values in the range [min_upper_bound, upper_bound]\n",
        "            x_train[:, i][mask == 1] = np.random.uniform(min_upper_bound, upper_bound, size=(mask == 1).sum())\n",
        "        x_train_1=x_train\n",
        "        x_train = np.round(x_train, 2)\n",
        "        x_train = (x_train - lower_bound) / (upper_bound - lower_bound)\n",
        "        # Create y_train by asking user to provide functions for each output\n",
        "        y_train = []\n",
        "        x_symbols = sp.symbols(f'x0:{num_input_neurons}')  # Creates symbols x0, x1, x2, ..., up to num_inputs\n",
        "\n",
        "        # for i in  range(output_num_neurons):\n",
        "        for func_str in  func_expressions:\n",
        "          func = sp.sympify(func_str)  # Convert string to sympy expression\n",
        "          func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
        "\n",
        "          if is_logical_operation(func_str):\n",
        "            # Apply thresholding if the function is a logical operation\n",
        "            binary_x_train = (x_train > 0.5).astype(int)  # Convert to binary values based on threshold\n",
        "\n",
        "            # func = sp.sympify(func_str)  # Convert string to sympy expression\n",
        "            # func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
        "\n",
        "            # Evaluate the function for each row\n",
        "            y_values = np.array([func_callable(*binary_x_train[row]) for row in range(num_training_inputs)])\n",
        "            # y_values = np.array([func_callable(*x_train[row]) for row in range(num_training_inputs)])\n",
        "\n",
        "            # Convert boolean results to binary (0 and 1)\n",
        "            y_values = y_values.astype(int)\n",
        "\n",
        "          else:\n",
        "            # For mathematical functions, apply directly on continuous values\n",
        "            # func = sp.sympify(func_str)  # Convert string to sympy expression\n",
        "            # func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
        "            y_values = np.array([func_callable(*x_train[row]) for row in range(num_training_inputs)])\n",
        "\n",
        "          # Append results to y_train\n",
        "          y_train.append(y_values)\n",
        "\n",
        "      # Convert y_train to numpy array and transpose to match the format (num_training_inputs, num_outputs)\n",
        "        y_train = np.array(y_train).T\n",
        "\n",
        "        # print(\"\\ny_train:\")\n",
        "        # print(y_train)\n",
        "\n",
        "        # Save x_train and y_train to the folder path\n",
        "        # np.save(x_train_file_path, x_train)\n",
        "        # np.save(y_train_file_path, y_train)\n",
        "\n",
        "        # dataset_file_path = os.path.join(folder_path, \"dataset.csv\")\n",
        "        #dataset = pd.DataFrame(np.hstack((x_train, y_train)), columns=[f'Input_{i+1}' for i in range(num_input_neurons)] + ['AND', 'OR', 'XOR'])\n",
        "        # dataset = pd.DataFrame(np.hstack((x_train, y_train)), columns=[f'Input_{i+1}' for i in range(num_input_neurons)] + [f'Output_{i+1}' for i in range(output_num_neurons)])\n",
        "\n",
        "        # dataset.to_csv(dataset_file_path, index=False)\n",
        "\n",
        "  return x_train, y_train,x_train_1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry3N5Dl4BGBl"
      },
      "outputs": [],
      "source": [
        "# def zip_files(zip_filename, folder_path): - not used anywhere\n",
        "#     with ZipFile(zip_filename, 'w') as zipf:\n",
        "#         for root, dirs, files in os.walk(folder_path):\n",
        "#             for file in files:\n",
        "#                 zipf.write(os.path.join(root, file), file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7Tylug0qvcy"
      },
      "outputs": [],
      "source": [
        "def get_valid_input(prompt, validation_fn, error_message):\n",
        "    \"\"\"\n",
        "    General function to get validated user input.\n",
        "\n",
        "    Parameters:\n",
        "    - prompt: The message to display when asking for input.\n",
        "    - validation_fn: A function that takes the input and returns True if it's valid, False otherwise.\n",
        "    - error_message: The message to display when the input is invalid.\n",
        "\n",
        "    Returns:\n",
        "    - The validated input.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        user_input = input(prompt).strip().lower()\n",
        "        if validation_fn(user_input):\n",
        "            return user_input\n",
        "        else:\n",
        "            print(error_message)\n",
        "\n",
        "#  validation functions for different input types\n",
        "def validate_run_number(input_value, max_runs):\n",
        "    try:\n",
        "        run_number = int(input_value)\n",
        "        return 1 <= run_number <= max_runs\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# def validate_update_type(input_value):\n",
        "#     valid_types = {\"bias\", \"tau\", \"taubounce\", \"epochs\"}\n",
        "#     return input_value.lower() in valid_types\n",
        "\n",
        "def validate_yes_no(input_value):\n",
        "    return input_value.lower() in {\"yes\", \"no\"}\n",
        "\n",
        "def is_valid_number(value):\n",
        "    \"\"\"Check if the input value can be converted to a valid number (int or float).\"\"\"\n",
        "    try:\n",
        "        num_value = float(value.strip())  # Convert to float\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def validate_binary_string(input_str):    #This is a replacement for the validate_update_type function defined above.\n",
        "    \"\"\"Check if the input string is a 5-digit binary string (i.e., contains only '0' or '1').\"\"\"\n",
        "    return len(input_str) == 5 and all(char in '01' for char in input_str)\n",
        "\n",
        "def validate_rerun_or_update(input_value):\n",
        "    \"\"\"Check if the input value is one of the valid options.\"\"\"\n",
        "    return input_value in {\"rerun\", \"update\", \"exit\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpcakg7VB_QQ"
      },
      "source": [
        "## Step 2: Code the Base Class\n",
        "\n",
        "Abstract Base Class Layer: This class Layer, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward and backward methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtVG9EmrdenT"
      },
      "outputs": [],
      "source": [
        "# Base class\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        self.bias = None\n",
        "        self.tau = None\n",
        "        self.z = None\n",
        "        self.Z = None  # New attribute to store the value of Z\n",
        "        self.constant = None  # New attribute to store the value of constant\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, bias_learning_rate, tau_learning_rate, epochs,Change_bias,Change_tau):\n",
        "        raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RyvcxqwCLOo"
      },
      "source": [
        "## Step 3: Fully Connected Layer\n",
        "\n",
        "The below piece of code defines a custom Fully Connected Layer (FCLayer) for a neural network. It inherits from a base class Layer (which is defined above) and includes methods for forward propagation and backward propagation.\n",
        "\n",
        "**Forward propagation for each layer**\n",
        "\n",
        "    Z(i) = self.Z * [{j=1to J} δ ((200 b (j, i) - 100) * (y(j) -  ζ (j, i))) - J + self.constant]\n",
        "        \n",
        "    y(i) = ReLU (Zi)\n",
        "\n",
        "\n",
        "**Backward Propagation for each layer:**\n",
        "\n",
        "*Calculate beta and theta*\n",
        "\n",
        "    β(i,k) = self.Z *  δ' [(200 b(i, k) - 100) (yi - ζ (i , k))] * (200 b(i,k) - 100)\n",
        "\n",
        "    θ (i, k) = self.Z *  δ' [(200 b(i, k) - 100) (yi - ζ (i , k))] * 200 * (yi - ζ (i, k))\n",
        "\n",
        "*Calculate the gradients*\n",
        "\n",
        "    ∇ b(i, k) =  { (yk – tk) * θ (i, k)}, if z(k) > 0\n",
        "\n",
        "    ∇ b(i, k) =  { 0 }, if z(k) < 0\n",
        "\n",
        "*Update the bias and tau*\n",
        "\n",
        "    self.bias = self.bias + bias_gradient\n",
        "\n",
        "    self.tau = self.tau + tau_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OCsSBjae26U"
      },
      "outputs": [],
      "source": [
        "# inherit from base class Layer: Fully connected layer code\n",
        "class FCLayer(Layer):\n",
        "\n",
        "    def __init__(self, input_size, output_size, name, bias=None, tau=None):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.name = name  # Layer name\n",
        "        self.bias = bias\n",
        "        self.tau = tau\n",
        "\n",
        "    # Set Z and constant based on the input_size\n",
        "        if self.input_size >= 10:\n",
        "            self.Z = 0.1\n",
        "            self.constant = 10\n",
        "        else :\n",
        "            self.Z = round(1/self.input_size, 2)\n",
        "            self.constant = self.input_size\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        print(f\"Forward propagation for : {self.name} with input neurons: {self.input_size} and output neurons: {self.output_size}\")\n",
        "        self.input = input_data\n",
        "        print(f\"input for the layer : {self.input} with tau: {self.tau} and bias: {self.bias}\")\n",
        "\n",
        "        # Ensure input and tau are reshaped to 1D arrays for element-wise comparison\n",
        "        input_flat = self.input.flatten()\n",
        "        tau_flat = self.tau.flatten()\n",
        "\n",
        "        # Ensure that tau and input have the same size before comparison\n",
        "        min_size = min(input_flat.size, tau_flat.size)\n",
        "        input_flat = input_flat[:min_size]\n",
        "        tau_flat = tau_flat[:min_size]\n",
        "\n",
        "        # Use np.isclose to find indices where input and tau are approximately the same within a tolerance\n",
        "        identical_indices = np.where(np.isclose(input_flat, tau_flat, atol=0.01))\n",
        "\n",
        "        if identical_indices[0].size > 0:\n",
        "            print(f\"Identical input and tau detected at indices {identical_indices}. Updating input values.\")\n",
        "\n",
        "            # Loop through the indices where input and tau are close\n",
        "            for idx in identical_indices[0]:\n",
        "              if (0.6 <= tau_flat[idx] <= 1) or (0.4 <= tau_flat[idx] <= 0.5):  #between 0.6 to 1 OR between 0.4 and 0.5\n",
        "                # Subtract 0.1 if tau is greater than 0.8\n",
        "                input_flat[idx] -= 0.1\n",
        "                print(f\"Tau value at index {idx} is greater than 0.8. Subtracting 0.1 from input.\")\n",
        "              else:\n",
        "                # Add 0.1 if tau is 0.8 or less\n",
        "                input_flat[idx] += 0.1\n",
        "                print(f\"Tau value at index {idx} is less than or equal to 0.8. Adding 0.1 to input.\")\n",
        "\n",
        "            # Reshape input_flat back to original shape\n",
        "            self.input = input_flat.reshape(self.input.shape)\n",
        "            print(f\"Updated input for the layer: {self.input} with tau: {self.tau} and bias: {self.bias}\")\n",
        "\n",
        "        #calculate z(i)\n",
        "        self.z = self.Z * (np.sum(sigmoid((200 * self.bias - 100) * (self.input - self.tau)), axis = 1, keepdims=True) - self.input_size + self.constant)\n",
        "        self.output =  self.z.T\n",
        "        print(f\"output after Forward Propagation : {self.output}\")\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    # computes thresholds - tau, tau_gradient and bias_gradient\n",
        "    def backward_propagation(self, is_last_layer, layer_target_output, prev_tau_gradient, bias_learning_rate, tau_learning_rate, epochs,Change_bias,Change_tau):\n",
        "        print(f\"Backward propagation begins for : {self.name}\")\n",
        "        bias_learning_rate_modified = bias_learning_rate\n",
        "        tau_learning_rate_modified = tau_learning_rate\n",
        "        #calculate the beta and theta for the previous layer\n",
        "        beta = self.Z * sigmoid_prime((2 * self.bias - 1) * (self.input - self.tau)) * (2 * self.bias - 1)\n",
        "        theta = self.Z * sigmoid_prime((2 * self.bias - 1) * (self.input - self.tau)) * 2 * (self.input - self.tau)\n",
        "        print('beta from BP: ',  beta)\n",
        "        print('theta from BP: ',  theta)\n",
        "        if Change_tau == True:\n",
        "          tau_learning_rate_modified = tau_learning_rate - 1\n",
        "\n",
        "        if Change_bias == True:\n",
        "          bias_learning_rate_modified = bias_learning_rate - 2\n",
        "\n",
        "        print('Effective Bias Learning rate = ', bias_learning_rate_modified)\n",
        "        print('Effective Tau Learning rate = ', tau_learning_rate_modified)\n",
        "\n",
        "        #calculate gradients based on layer\n",
        "        print(' Calculate tau and bias gradients:')\n",
        "        if is_last_layer:\n",
        "\n",
        "            tau_gradient = np.round((layer_target_output - self.output).T * beta, decimals=2)\n",
        "            tau_gradient = np.where(tau_gradient < 0.1, tau_gradient * tau_learning_rate_modified, tau_gradient)\n",
        "\n",
        "            bias_gradient = (self.output - layer_target_output).T * theta\n",
        "            bias_gradient = np.where(np.absolute(bias_gradient) < 0.05, bias_gradient , bias_gradient * bias_learning_rate_modified)\n",
        "\n",
        "        else:\n",
        "            tau_gradient = np.round(np.dot(np.sum(prev_tau_gradient, axis=0), beta), decimals = 2)\n",
        "            tau_gradient = np.where(tau_gradient < 0.1, tau_gradient * tau_learning_rate_modified, tau_gradient)\n",
        "\n",
        "            bias_gradient = np.dot(np.sum(prev_tau_gradient, axis=0), -theta)\n",
        "            bias_gradient = np.where(np.absolute(bias_gradient) < 0.05, bias_gradient , bias_gradient * bias_learning_rate_modified)\n",
        "            bias_gradient = np.where(bias_gradient > 1, 1, bias_gradient)\n",
        "\n",
        "        #tau_gradient = np.clip(tau_gradient, -1, 1)\n",
        "        print('tau_gradient from BP: ',  tau_gradient)\n",
        "        print('bias_gradient from BP: ',  bias_gradient)\n",
        "\n",
        "        #store the value of tau_gradient to be passed on to the next layer in BP\n",
        "        prev_tau_gradient = tau_gradient\n",
        "\n",
        "        # update bias and tau for the current layer\n",
        "        print(' Update tau and bias values based on gradients:')\n",
        "\n",
        "        self.tau = np.round(self.tau - tau_gradient, decimals=2)\n",
        "        self.tau = np.where(\n",
        "            np.logical_or(self.tau > 0.9, self.tau < 0.1),\n",
        "            np.where(self.tau > 0.9, 0.9 - (self.tau - 0.9), 0.1 + (0.1 - self.tau)),\n",
        "            # Limit the change to 0.2 for values within the range\n",
        "            np.clip(self.tau, self.tau - 0.2, self.tau + 0.2)\n",
        "            )\n",
        "\n",
        "        self.bias = np.round(self.bias - bias_gradient, decimals=0)\n",
        "        self.bias = np.clip(self.bias, 0, 1)\n",
        "\n",
        "        print('self.tau after update from BP: ', self.tau)\n",
        "        print('self.bias after update from BP: ',  self.bias)\n",
        "        print(f\"Backward propagation ends for : {self.name}\")\n",
        "\n",
        "        return prev_tau_gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlGmlQSiE9WU"
      },
      "source": [
        "## Step 4: Activation Layer\n",
        "\n",
        "In a neural network, an activation layer introduces non-linearity to the model. The Relu is implemented in this layer which returns the activated input: y(i) = ReLU (Zi) which is the final output for the layer. For the backward propagation, the activation layer is not used in the modified NN.\n",
        "\n",
        "*This can be further edited by removing it and directly integrating the Relu function into the forward propagation within the FC layer class. The activation layer has been defined separately to enhance clarity in the network structure.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Iw0WeYue7d-"
      },
      "outputs": [],
      "source": [
        "# inherit from base class Layer: Activation Layer code\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "\n",
        "    # returns the activated input: y(i) = ReLU (Zi)\n",
        "\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        print('output for the layer after ReLu (y): ', self.output)\n",
        "        return self.output\n",
        "\n",
        "    # Not returning anything for the modified NN\n",
        "    def backward_propagation(self, is_last_layer, layer_target_output, prev_tau_gradient, bias_learning_rate, tau_learning_rate, epochs,Change_bias,Change_tau):\n",
        "        _ = self.activation(self.input)\n",
        "        return prev_tau_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1k6n7JcOe-R8"
      },
      "outputs": [],
      "source": [
        "# updated activation function and its derivative\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Modified Sigmoid activation function used in FP\n",
        "def sigmoid(x, alpha=1):\n",
        "    x = x.astype(float)\n",
        "    return 1 / (1 + np.exp(-alpha * x))\n",
        "\n",
        "# Modified Derivative of Sigmoid activation function used in BP\n",
        "def sigmoid_prime(x, alpha=1):\n",
        "    sig_prime_1 = (1 - sigmoid(x, alpha))\n",
        "    sig_prime_2 = sigmoid(x, alpha)\n",
        "    return alpha * sigmoid(x, alpha) * (1 - sigmoid(x, alpha))\n",
        "\n",
        "#Softmax Function for the last layer - used for classification tasks\n",
        "def softmax(x, alpha=1):\n",
        "    sig_prime_1 = (1 - sigmoid(x, alpha))\n",
        "    sig_prime_2 = sigmoid(x, alpha)\n",
        "    return alpha * sigmoid(x, alpha) * (1 - sigmoid(x, alpha))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7SjilzdFKbT"
      },
      "source": [
        "## Step 5: Loss Function\n",
        "\n",
        "Mean Squared Error (MSE) Loss Function: The mse function calculates the mean of the squared differences between the true values (y_true) and the predicted values (y_pred). It quantifies the average magnitude of the errors.\n",
        "\n",
        "*This is solely for visualizing the model training, and the values are not directly utilized in the backward propagation.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pFhZ_P9fBCv"
      },
      "outputs": [],
      "source": [
        "# loss function: Used to visualize the model learning - error after each epoch\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(np.squeeze(y_true) - np.squeeze(y_pred), 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6NVs6ZeFJvh"
      },
      "source": [
        "## Step 6: Network Layer\n",
        "The below code defines a basic neural network class named Network.\n",
        "\n",
        "init Method: Initializes the neural network with an empty list of layers and placeholders for the loss function and its derivative.\n",
        "\n",
        "add Method: Adds a layer to the neural network.\n",
        "\n",
        "use Method: Sets the loss function\n",
        "\n",
        "predict Method: Predicts the output for a given input by performing forward propagation through all the layers.\n",
        "\n",
        "fit Method: Trains the neural network on the provided training data (x_train, y_train) using a specified number of epochs and learning rate. It includes a training loop that iterates through each epoch and sample, performing forward and backward propagation for each sample. The loss is computed during training for display purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay80dAN7fJaZ"
      },
      "outputs": [],
      "source": [
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use(self, loss):\n",
        "        self.loss = loss\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        # sample dimension first\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        # run network over all samplesnp\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "              output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, bias_learning_rate, tau_learning_rate, debug_mode='no'):\n",
        "\n",
        "        # sample dimension first\n",
        "        prev_tau_gradient = None\n",
        "        samples = len(x_train)\n",
        "        errors = []  # List to store average errors for plotting\n",
        "\n",
        "        Skip_tau_change = False\n",
        "\n",
        "        # Track errors for the last 3 epochs to adjust bias learning rate\n",
        "        last_3_errors = []\n",
        "        lowest_error = float('inf')\n",
        "        best_tau_values = []  # To track tau values corresponding to the lowest error\n",
        "        best_bias_values = []  # To track tau values corresponding to the lowest error\n",
        "        adjusted_bias_learning_rate = False\n",
        "        initial_bias = bias_learning_rate  # Store the initial bias value\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            Change_tau = False\n",
        "            Change_bias = False\n",
        "            print(f\"For epoch {i+1}, bias learning rate = {bias_learning_rate}\")\n",
        "\n",
        "            pi_va=np.pi/2\n",
        "\n",
        "            print(f\"For epoch {i+1}, skipping tau change = {Skip_tau_change}\")\n",
        "            err = 0  # Reset error for the current epoch\n",
        "            print(f\"For epoch {i+1}, Change_tau Prior to check= {Change_tau}, and Change_bias = {Change_bias}\")\n",
        "            if ((i % 2 != 0) or (i > 4 and not Skip_tau_change)):\n",
        "                Change_tau = True\n",
        "                Skip_tau_change = False\n",
        "            if ((i % 2 == 0) or (i > 4)):\n",
        "                Change_bias = True  # Check if the epoch is even\n",
        "            print(f\"For epoch {i+1}, Change_tau = {Change_tau}, and Change_bias = {Change_bias}\")\n",
        "            for j in range(samples):\n",
        "\n",
        "                # forward propagation\n",
        "\n",
        "                # updated val based on calculations\n",
        "                output = x_train[j]*pi_va\n",
        "\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                sample_err = self.loss(y_train[j], output)\n",
        "                err += sample_err\n",
        "                print(f'Epoch {i+1}, Sample {j+1}, Error: {sample_err}')\n",
        "\n",
        "\n",
        "                #backward propagation\n",
        "                print(f\" *** Backward propagation Begins *** \")\n",
        "                for layer in reversed(self.layers):\n",
        "                    is_last_layer = (layer == self.layers[-2])  # Check if it's the last layerindex\n",
        "                    prev_tau_gradient = layer.backward_propagation(is_last_layer, y_train[j], prev_tau_gradient, bias_learning_rate, tau_learning_rate, epochs,Change_bias,Change_tau)\n",
        "                print(f\" *** End of Backward propagation *** \\n\")\n",
        "\n",
        "                # forward propagation - once again FP only for debug mode\n",
        "                if debug_mode in [\"yes\", \"y\"]:\n",
        "                  print(f\" *** Forward propagation 2 Begins for the same input *** \")\n",
        "                  for layer in self.layers:\n",
        "                    output = layer.forward_propagation(x_train[j])\n",
        "\n",
        "                  sample_err = self.loss(y_train[j], output)\n",
        "                  err += sample_err\n",
        "                  print(f'Epoch {i+1}, Sample {j+1}, Error: {sample_err}')\n",
        "                  print(f\" *** End of Forward propagation 2 *** \\n\")\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            print(f\" *** End of epoch: {i+1} ***\")\n",
        "            avg_err = err / samples\n",
        "            errors.append(avg_err)\n",
        "            print('Epoch %d/%d,   Average Error: %f \\n' % (i+1, epochs, avg_err))\n",
        "\n",
        "            # Track the lowest error and associated tau values\n",
        "            if avg_err < lowest_error:\n",
        "              lowest_error = avg_err\n",
        "              for layer in self.layers:\n",
        "                # if isinstance(layer, FCLayer):\n",
        "                    best_bias_values.append(np.copy(layer.bias))\n",
        "                    best_tau_values.append(np.copy(layer.tau))\n",
        "                    print(f\" Saving the tau value {best_tau_values} \")\n",
        "                    print(f\"Saving the bias value {best_bias_values}\")\n",
        "\n",
        "            # Check if the last 3 errors are constant or increasing\n",
        "            last_3_errors.append(avg_err)\n",
        "            if len(last_3_errors) > 3:\n",
        "                last_3_errors.pop(0)  # Keep only the last 3 errors\n",
        "\n",
        "              #  if len(last_3_errors) == 3 and last_3_errors[0] < last_3_errors[-1]:\n",
        "              #   print(f\"Error increasing/constant over last 3 epochs. Reverting to best tau and bias values observed so far.\")\n",
        "              #   for layer, best_tau, best_bias in zip(self.layers, best_tau_values, best_bias_values):\n",
        "              #     # if isinstance(layer, FCLayer):\n",
        "              #         layer.tau = np.copy(best_tau)\n",
        "              #         layer.bias = np.copy(best_bias)\n",
        "              #         print(f\" Adjusted tau value {layer.tau} \")\n",
        "              #         print(f\"Adjusted bias value {layer.bias}\")\n",
        "\n",
        "                  # Error is increasing or constant, adjust learning rate\n",
        "                Skip_tau_change = True\n",
        "                if not adjusted_bias_learning_rate:\n",
        "                    bias_learning_rate += 1\n",
        "                    adjusted_bias_learning_rate = True\n",
        "                    print(f\"Adjusting bias learning rate to {bias_learning_rate} for Epoch {i+1}\")\n",
        "\n",
        "        # Save final bias and tau values\n",
        "        final_bias_values = [layer.bias for layer in self.layers if isinstance(layer, FCLayer)]\n",
        "        final_tau_values = [layer.tau for layer in self.layers if isinstance(layer, FCLayer)]\n",
        "\n",
        "        # Plotting error rates across epochs\n",
        "        plt.plot(range(1, epochs + 1), errors, marker='o')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Average Error')\n",
        "        plt.title('Error Rate Across Epochs')\n",
        "        plt.show()\n",
        "\n",
        "        return errors, final_bias_values, final_tau_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Run \\_\\_main\\_\\_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0nu5n7drCJMv",
        "outputId": "66636a4f-ae4f-4167-9a84-45d2dc2bdba5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Main function\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_folder = \"DATA\"\n",
        "    clear_folders(base_folder)  # Clear previous data\n",
        "    create_new = True #by default always create new dataset when running the program for the 1st run\n",
        "\n",
        "    debug_mode = input(\"Do you want to enter debug mode? (yes/no): \").strip().lower()\n",
        "    upload_mode = input(\"Do you want to upload existing run folder? (yes/no): \").strip().lower()\n",
        "    if upload_mode == 'yes':\n",
        "      # Step 2: Upload the compressed folder (.zip file)\n",
        "      print(\"Please upload a .zip file containing the folder.\")\n",
        "      uploaded = files.upload()  # Prompts user to upload files\n",
        "\n",
        "      # Assuming only one file is uploaded, get its name\n",
        "      zip_filename = list(uploaded.keys())[0]\n",
        "\n",
        "      # Step 3: Extract the .zip to /content/DATA/\n",
        "      extract_path = '/content/DATA/'\n",
        "      os.makedirs(extract_path, exist_ok=True)  # Ensure the destination directory exists\n",
        "\n",
        "      with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        # Extract the entire folder (e.g., 'run_3') to '/content/DATA/run_3/'\n",
        "          zip_ref.extractall(extract_path)\n",
        "          print(f\"Folder '{zip_filename}' has been extracted to '{extract_path}'.\")\n",
        "\n",
        "          # Get the list of folders inside the extracted directory\n",
        "          extracted_folders = [f for f in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, f))]\n",
        "\n",
        "          # Assuming only one folder exists (e.g., 'run_3'), get the folder name\n",
        "          if len(extracted_folders) == 1:\n",
        "              extracted_folder = extracted_folders[0]\n",
        "              print(f\"Detected extracted folder: {extracted_folder}\")\n",
        "\n",
        "              # Locate parameters.json file inside the extracted folder (e.g., 'run_3')\n",
        "              parameters_file_path_in_run = os.path.join(extract_path, extracted_folder, 'parameters.json')\n",
        "              # Check for x_train.npy and y_train.npy in the extracted folder\n",
        "              x_train_file_path_in_run = os.path.join(extract_path, extracted_folder,'original_result', 'x_train.npy')\n",
        "              y_train_file_path_in_run = os.path.join(extract_path, extracted_folder,'original_result', 'y_train.npy')\n",
        "\n",
        "\n",
        "              # If parameters.json exists, copy it to /content/DATA/ directory\n",
        "              if os.path.exists(parameters_file_path_in_run):\n",
        "                  shutil.copy(parameters_file_path_in_run, os.path.join(extract_path, 'parameters.json'))\n",
        "                  print(f\"'parameters.json' has been copied to '{extract_path}'.\")\n",
        "              else:\n",
        "                  print(\"No 'parameters.json' file found in the extracted folder.\")\n",
        "\n",
        "              # If x_train.npy exists, copy it to the common folder\n",
        "              if os.path.exists(x_train_file_path_in_run):\n",
        "                  shutil.copy(x_train_file_path_in_run, os.path.join(extract_path, 'x_train.npy'))\n",
        "                  print(f\"'x_train.npy' has been copied to '{extract_path}'.\")\n",
        "              else:\n",
        "                  print(\"No 'x_train.npy' file found in the extracted folder.\")\n",
        "\n",
        "              # If y_train.npy exists, copy it to the common folder\n",
        "              if os.path.exists(y_train_file_path_in_run):\n",
        "                  shutil.copy(y_train_file_path_in_run, os.path.join(extract_path))\n",
        "                  print(f\"'y_train.npy' has been copied to '{extract_path}'.\")\n",
        "              else:\n",
        "                  print(\"No 'y_train.npy' file found in the extracted folder.\")\n",
        "\n",
        "          else:\n",
        "              print(\"Error: More than one folder was extracted. Unable to locate the run folder.\")\n",
        "\n",
        "    num_runs = int(input(\"Enter the number of runs: \").strip())\n",
        "    run_summary = []\n",
        "\n",
        "    for run_number in range(1, num_runs + 1):\n",
        "        print ('run_number: ', run_number)\n",
        "\n",
        "        # Create a folder for this run\n",
        "        run_folder = create_run_folder(base_folder, run_number)\n",
        "\n",
        "        # Capture the console output\n",
        "        original_stdout = sys.stdout\n",
        "        sys.stdout = StringIO()\n",
        "\n",
        "        try:\n",
        "            if upload_mode == \"yes\":\n",
        "              num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, folder_path = get_user_inputs(False, run_number)\n",
        "              x_train, y_train = generate_data(num_neurons[0], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, False)\n",
        "            else:\n",
        "              num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, folder_path = get_user_inputs(create_new if run_number == 1 else False, run_number)\n",
        "              x_train, y_train,x_train_1 = generate_data(num_neurons[0], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, create_new)\n",
        "\n",
        "            net = Network()\n",
        "            for i in range(num_layers):\n",
        "                if i == num_layers - 1:  # last layer\n",
        "                    net.add(FCLayer(num_neurons[i], output_num_neurons, f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
        "                else:\n",
        "                    net.add(FCLayer(num_neurons[i], num_neurons[i + 1], f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
        "                net.add(ActivationLayer(ReLU))\n",
        "\n",
        "            net.use(mse)\n",
        "\n",
        "            print(f\"Run number: {run_number}\")\n",
        "            # Train the network\n",
        "\n",
        "\n",
        "\n",
        "            errors, final_bias_values, final_tau_values = net.fit(x_train, y_train, epochs, bias_learning_rate, tau_learning_rate, debug_mode)\n",
        "\n",
        "            # Check final error rate and save results in the appropriate folder\n",
        "            final_error = errors[-1]\n",
        "            print(f'Final Error: {final_error}')  # Debug statement to check the final error\n",
        "\n",
        "            rounded_final_error = round(final_error, 2)\n",
        "            if rounded_final_error == 0.00:\n",
        "                rounded_final_error = 0\n",
        "            run_summary.append((run_number, rounded_final_error))\n",
        "\n",
        "            # Capture the console output to save\n",
        "            output_text = sys.stdout.getvalue()\n",
        "\n",
        "            # Save results in the run folder\n",
        "            save_results(run_folder, run_number, x_train,x_train_1, y_train, bias_values, tau_values, errors, final_bias_values, final_tau_values, output_text, subfolder_name=\"original_result\")\n",
        "\n",
        "        finally:\n",
        "            # Restore the original stdout\n",
        "            sys.stdout = original_stdout\n",
        "\n",
        "        # Save the output to JSON and PDF\n",
        "        save_console_output(run_number, output_text, rounded_final_error, run_folder, None)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nRun Summary:\")\n",
        "    headers = [\"Run\", \"Final Error\"]\n",
        "    table = [[run_number, error] for run_number, error in run_summary]\n",
        "    print(tabulate(table, headers, tablefmt=\"grid\"))\n",
        "\n",
        "    # Store former run summary before rerunning with updated bias\n",
        "    former_run_summary = run_summary.copy()\n",
        "\n",
        "    while True:\n",
        "        rerun_or_update = get_valid_input( \"\\nDo you want to rerun a run number or update the parameters? (rerun/update/exit): \",\n",
        "    validate_rerun_or_update, \"Invalid input. Please enter 'rerun', 'update', or 'exit'.\" )\n",
        "\n",
        "        if rerun_or_update == \"exit\":\n",
        "            print(\"Exiting the program.\")\n",
        "            break\n",
        "\n",
        "# int(sin(x0) + cos(x1))\n",
        "# int(cos (x0 + x1))\n",
        "# np.floor(sin(x0) + cos(x1))\n",
        "# np.floor(cos (x0 + x1))\n",
        "\n",
        "# np.floor(np.sin(x0) + np.cos(x1))\n",
        "# np.floor(np.cos(x0 + x1))\n",
        "\n",
        "# sp.floor(sp.sin(x0) + sp.cos(x1))\n",
        "# sp.floor(sp.cos(x0 + x1))\n",
        "\n",
        "# floor(sin(x0) + cos(x1))\n",
        "# floor(cos (x0 + x1))\n",
        "\n",
        "\n",
        "# floor(sin(x0)+cos(x1)+sin(x2)+cos(x3)+sin(x4)+cos(x5))\n",
        "# floor(cos(x0+x1+x2+x3+x4+x5))\n",
        "\n",
        "# floor( (sin(x0)+cos(x1)+sin(x2)+cos(x3)+sin(x4)+cos(x5) )/6 )\n",
        "# floor(cos(x0+x1+x2+x3+x4+x5)*(3.14/2)/6)\n",
        "\n",
        "# *( PI()/2))/6\n",
        "        else:\n",
        "\n",
        "          manage_rerun_update(rerun_or_update, base_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tHqfdnCMTXpa",
        "outputId": "490e4e74-3b62-40bd-f64e-a45db26f2b8d"
      },
      "outputs": [],
      "source": [
        "    num_runs = 5\n",
        "    # num_layers = 6\n",
        "    for run_number in range(1, num_runs + 1):\n",
        "        print ('run_number: ', run_number)\n",
        "\n",
        "        # Create a folder for this run\n",
        "        # run_folder = create_run_folder(base_folder, run_number)\n",
        "\n",
        "        # Capture the console output\n",
        "        # original_stdout = sys.stdout\n",
        "        # sys.stdout = StringIO()\n",
        "\n",
        "        try:\n",
        "            # if upload_mode == \"yes\":\n",
        "            #   num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, folder_path = get_user_inputs(False, run_number)\n",
        "            #   x_train, y_train = generate_data(num_neurons[0], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, False)\n",
        "            # else:\n",
        "            #   num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, folder_path = get_user_inputs(create_new if run_number == 1 else False, run_number)\n",
        "            #   x_train, y_train,x_train_1 = generate_data(num_neurons[0], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, create_new)\n",
        "\n",
        "            net = Network()\n",
        "            for i in range(num_layers):\n",
        "                if i == num_layers - 1:  # last layer\n",
        "                    net.add(FCLayer(num_neurons[i], output_num_neurons, f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
        "                else:\n",
        "                    net.add(FCLayer(num_neurons[i], num_neurons[i + 1], f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
        "                net.add(ActivationLayer(ReLU))\n",
        "\n",
        "            net.use(mse)\n",
        "\n",
        "            print(f\"Run number: {run_number}\")\n",
        "            # Train the network\n",
        "            errors, final_bias_values, final_tau_values = net.fit(x_train_1, y_train, epochs, bias_learning_rate, tau_learning_rate, debug_mode)\n",
        "\n",
        "            # Check final error rate and save results in the appropriate folder\n",
        "            final_error = errors[-1]\n",
        "            print(f'Final Error: {final_error}')  # Debug statement to check the final error\n",
        "\n",
        "            rounded_final_error = round(final_error, 2)\n",
        "            if rounded_final_error == 0.00:\n",
        "                rounded_final_error = 0\n",
        "            run_summary.append((run_number, rounded_final_error))\n",
        "\n",
        "            # Capture the console output to save\n",
        "            # output_text = sys.stdout.getvalue()\n",
        "\n",
        "            # Save results in the run folder\n",
        "            # save_results(run_folder, run_number, x_train,x_train_1, y_train, bias_values, tau_values, errors, final_bias_values, final_tau_values, output_text, subfolder_name=\"original_result\")\n",
        "\n",
        "        finally:\n",
        "            # Restore the original stdout\n",
        "            sys.stdout = original_stdout\n",
        "\n",
        "        # Save the output to JSON and PDF\n",
        "        # save_console_output(run_number, output_text, rounded_final_error, run_folder, None)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\nRun Summary:\")\n",
        "    headers = [\"Run\", \"Final Error\"]\n",
        "    table = [[run_number, error] for run_number, error in run_summary]\n",
        "    print(tabulate(table, headers, tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5GO4X-GV-2B"
      },
      "source": [
        "# **Traditional Neural Network vs Custom**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hWu8ttJJDZP"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import sympy as sp\n",
        "\n",
        "# # Function to check if the operation is logical (like AND/OR)\n",
        "# def is_logical_operation(func_str):\n",
        "#     logical_ops = ['&', '|', 'and', 'or']  # Add any other logical operators you want to support\n",
        "#     return any(op in func_str for op in logical_ops)\n",
        "\n",
        "# # Ask user for number of inputs\n",
        "# num_inputs = int(input(\"Enter the number of inputs (dimension): \"))\n",
        "\n",
        "# # Ask user for range for input variable\n",
        "# lower_bound = float(input(\"Enter the lower bound of input range: \"))\n",
        "# upper_bound = float(input(\"Enter the upper bound of input range: \"))\n",
        "\n",
        "# # Ask user for the number of training inputs in the dataset\n",
        "# num_training_inputs = int(input(\"Enter the number of training inputs in the dataset: \"))\n",
        "\n",
        "# # Create X_train dataset with random values uniformly between the bounds\n",
        "# X_train = np.random.uniform(lower_bound, upper_bound, (num_training_inputs, num_inputs))\n",
        "\n",
        "# # Ask user for maximum upper bound for normalization\n",
        "# max_upper_bound = float(input(\"Enter the maximum upper bound for normalization: \"))\n",
        "\n",
        "# # Normalize the X_train data between lower_bound and max_upper_bound\n",
        "# X_train_min = X_train.min(axis=0)  # Minimum value of each feature (column)\n",
        "# X_train_max = X_train.max(axis=0)  # Maximum value of each feature (column)\n",
        "\n",
        "# # Apply the normalization formula: (x - min) / (max - min) * (max_upper_bound - lower_bound) + lower_bound\n",
        "# X_train = ((X_train - X_train_min) / (X_train_max - X_train_min)) * (max_upper_bound - lower_bound) + lower_bound\n",
        "\n",
        "# # Round the values to 2 decimal places\n",
        "# X_train = np.round(X_train, 2)\n",
        "\n",
        "# print(\"\\nX_train:\")\n",
        "# print(X_train)\n",
        "# print(\"Shape of X_train:\", X_train.shape)\n",
        "\n",
        "# # Ask user for number of outputs\n",
        "# num_outputs = int(input(\"\\nEnter the number of outputs: \"))\n",
        "\n",
        "# # Ask the user to provide functions for each output\n",
        "# func_expressions = []\n",
        "# for i in range(num_outputs):\n",
        "#     func_expr = input(f\"Enter function for output {i+1} (e.g., 'sin(x0) + cos(x1)'): \")\n",
        "#     func_expressions.append(func_expr)\n",
        "\n",
        "# # Create y_train by evaluating functions for each output\n",
        "# y_train = []\n",
        "# x_symbols = sp.symbols(f'x0:{num_inputs}')  # Creates symbols x0, x1, x2, ..., up to num_inputs\n",
        "\n",
        "# # Loop over the function expressions to generate the outputs\n",
        "# for func_str in func_expressions:\n",
        "#     # Convert string to sympy expression\n",
        "#     func = sp.sympify(func_str)\n",
        "#     # Convert to callable function using lambdify\n",
        "#     func_callable = sp.lambdify(x_symbols, func, 'numpy')\n",
        "\n",
        "#     # Check if the function is logical (thresholding will be applied if it's logical)\n",
        "#     if is_logical_operation(func_str):\n",
        "#         # Apply thresholding if the function is a logical operation\n",
        "#         binary_X_train = (X_train > 0.5).astype(int)  # Convert to binary values based on threshold\n",
        "\n",
        "#         # Evaluate the function for each row\n",
        "#         y_values = np.array([func_callable(*binary_X_train[row]) for row in range(num_training_inputs)])\n",
        "\n",
        "#         # Convert boolean results to binary (0 and 1)\n",
        "#         y_values = y_values.astype(int)\n",
        "#     else:\n",
        "#         # For mathematical functions, apply directly on continuous values\n",
        "#         y_values = np.array([func_callable(*X_train[row]) for row in range(num_training_inputs)])\n",
        "\n",
        "#     # Append results to y_train\n",
        "#     y_train.append(y_values)\n",
        "\n",
        "# # Convert y_train to numpy array and transpose to match the format (num_training_inputs, num_outputs)\n",
        "# y_train = np.array(y_train).T\n",
        "\n",
        "# print(\"\\nFinal y_train:\")\n",
        "# print(y_train)\n",
        "# print(\"Shape of y_train:\", y_train.shape)\n",
        "\n",
        "# # Optionally, save X_train and y_train to files if needed\n",
        "# # np.save(\"X_train.npy\", X_train)\n",
        "# # np.save(\"y_train.npy\", y_train)\n",
        "\n",
        "# # floor(sin(x0) + cos(x1))\n",
        "# # floor(cos (x0 + x1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ4RyVpQEDSV",
        "outputId": "02b15751-b14c-4850-b1bb-0b2f8e57d737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function 1: floor(sin(x0) + cos(x1))\n",
            "Function 2: floor(cos(x0 + x1))\n",
            "Results for func1 (floor(sin(x0) + cos(x1))): [1. 1. 1.]\n",
            "Results for func2 (floor(cos(x0 + x1))): [0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "import sympy as sp\n",
        "import numpy as np\n",
        "\n",
        "# Define the symbols\n",
        "x0, x1 = sp.symbols('x0 x1')\n",
        "\n",
        "# Define the symbolic expressions using sp.floor\n",
        "func1 = sp.floor(sp.sin(x0) + sp.cos(x1))  # np.floor(np.sin(x0) + np.cos(x1))\n",
        "func2 = sp.floor(sp.cos(x0 + x1))  # np.floor(np.cos(x0 + x1))\n",
        "\n",
        "# Print the symbolic expressions\n",
        "print(f\"Function 1: {func1}\")\n",
        "print(f\"Function 2: {func2}\")\n",
        "\n",
        "# Convert symbolic expressions to callable functions\n",
        "func1_callable = sp.lambdify((x0, x1), func1, 'numpy')\n",
        "func2_callable = sp.lambdify((x0, x1), func2, 'numpy')\n",
        "\n",
        "# Example input values (e.g., x_train)\n",
        "x_train = np.array([[0.5, 0.6], [0.7, 0.8], [0.9, 0.1]])\n",
        "\n",
        "# Evaluate the functions on the input data\n",
        "y_values_1 = np.array([func1_callable(*x_row) for x_row in x_train])\n",
        "y_values_2 = np.array([func2_callable(*x_row) for x_row in x_train])\n",
        "\n",
        "# Print the results\n",
        "print(f\"Results for func1 (floor(sin(x0) + cos(x1))): {y_values_1}\")\n",
        "print(f\"Results for func2 (floor(cos(x0 + x1))): {y_values_2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O_Amty3WBp3"
      },
      "outputs": [],
      "source": [
        "# !pip install keras\n",
        "# !pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2g2sQQqWDC7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "base_folder = \"DATA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXmQPPYSWEtW"
      },
      "outputs": [],
      "source": [
        "class CustomLogger(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Access the weights and biases after each epoch\n",
        "        weights = self.model.get_weights()  # List of numpy arrays\n",
        "\n",
        "        # Access the current learning rate from the optimizer\n",
        "        learning_rate = float(self.model.optimizer.learning_rate.numpy())\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1} ended\")\n",
        "        print(f\"Loss: {logs['loss']}, Accuracy: {logs['accuracy']}\")\n",
        "        print(f\"Learning Rate: {learning_rate}\")\n",
        "\n",
        "        # Log each layer's weights and biases\n",
        "        for i, weight in enumerate(weights):\n",
        "            print(f\"Layer {i // 2 + 1} {'Weights' if i % 2 == 0 else 'Biases'}: {weight}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BdgO8f-s_EQ"
      },
      "outputs": [],
      "source": [
        "# Define the Traditional Neural Network\n",
        "\n",
        "# input_dim = number of input neurons\n",
        "# output_dim = number of output neurons\n",
        "\n",
        "def create_traditional_nn(input_dim, output_dim, hidden_layers, neurons_per_layer, learning_rate):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer with the first hidden layer\n",
        "    model.add(Dense(neurons_per_layer[0], input_dim=input_dim, activation='relu'))\n",
        "\n",
        "    # Add additional hidden layers as per the specified configuration\n",
        "    for i in range(1, hidden_layers):\n",
        "        model.add(Dense(neurons_per_layer[i], activation='relu'))\n",
        "\n",
        "     # Output layer\n",
        "    model.add(Dense(output_dim, activation='sigmoid')) # output layer\n",
        "\n",
        "    # Compile the model with specified learning rate\n",
        "    model.compile(optimizer=Adam(learning_rate=float(learning_rate)), loss='mse', metrics=['accuracy'])\n",
        "    # Display the model summary\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # model.add(Dense(input_dim[1], input_dim=input_dim[0], activation='relu')) # Hidden layer with 1 neuron\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZDoG0-sYKQD",
        "outputId": "7cf08c73-a7f7-46e0-91e4-e4b737455737"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[0.5, 0.6],\n",
              "        [0.7, 0.8],\n",
              "        [0.9, 0.1]]),\n",
              " array([[ 0, -1],\n",
              "        [ 0, -1],\n",
              "        [ 0, -1],\n",
              "        [ 0, -1],\n",
              "        [ 0, -1]]))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[:5], y_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zsEBqfMsWGgv",
        "outputId": "47056325-05a8-44e9-bda3-748991400a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to modify the traditional network setup? (yes/no): no\n",
            "run_folder: /content/DATA/run_1/original_result\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m70\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │              \u001b[38;5;34m55\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m60\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m22\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6s/step - accuracy: 1.0000 - loss: 0.9226\n",
            "Epoch 1 ended\n",
            "Loss: 0.5352222323417664, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 13.457939   -13.07546     12.979758    13.462879    13.122888\n",
            "   12.694789    -0.25082165  13.14887     13.49205     -0.58613753]\n",
            " [ 13.30902    -12.532971    13.339746    12.6311455   12.713373\n",
            "   13.607815    -0.45372224  13.260719   -12.707749    -0.37114614]\n",
            " [ 13.28042    -12.876825    13.540612    12.981893    13.116836\n",
            "   13.333133    -0.18728814  12.889519    12.597485    -0.56364506]\n",
            " [ 13.391145   -13.278877    13.418608    13.070738    13.426985\n",
            "   13.593631    -0.31802675  12.924628    13.222856    -0.5551134 ]\n",
            " [ 12.587182   -12.764906    13.002652    12.463968    13.113208\n",
            "   12.627407     0.41176218  13.578294    13.086189    -0.33738226]\n",
            " [ 13.052044   -12.766673    12.638202    13.143818    13.46193\n",
            "   13.061051    -0.2837247   12.573362    12.648043    -0.29698232]]\n",
            "Layer 1 Biases: [ 13.060725 -12.985943  13.06057   13.044331  13.061029  13.060737\n",
            "   0.        13.060155  -7.131689   0.      ]\n",
            "Layer 2 Weights: [[-13.281552    12.971429    13.47878     12.623094   -13.135566  ]\n",
            " [ 10.000988    12.943448    12.942478   -12.928367    -0.5413875 ]\n",
            " [-13.220164    13.129658    13.374474    13.201095   -13.247005  ]\n",
            " [-12.716634    13.193276    13.0295105   13.459919   -13.012501  ]\n",
            " [-12.784767    13.533494    13.5375185   13.60906    -12.845455  ]\n",
            " [-13.227438    13.080532    13.558276    13.183275   -12.985384  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-11.004503    13.012915    13.389318    13.191073   -12.76317   ]\n",
            " [-13.21769     13.160548    13.014567    12.729887   -13.326687  ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-13.055093  13.060823  13.061146  13.058177 -13.057861]\n",
            "Layer 3 Weights: [[ 13.217726    -0.24632552  12.598876    12.5968895  -12.034377\n",
            "   -0.33244935 -12.575965     9.308266    13.06005    -13.386438  ]\n",
            " [ 13.031765    -0.14825454  13.204718    13.566968    12.959503\n",
            "    0.44860345 -12.964235   -13.012243    13.574099   -12.4290905 ]\n",
            " [ 13.434204    -0.13441947  13.333794    12.647755    13.658595\n",
            "   -0.58227986 -13.445412   -12.852213    13.679844   -13.627426  ]\n",
            " [ 13.66238      0.09806406  12.723292    12.853024    12.787906\n",
            "   -0.4941727  -12.454109   -12.321693    12.820989   -13.080027  ]\n",
            " [ 12.450954     0.3267088   13.537373    -0.25607768 -10.030088\n",
            "   -0.6121563  -12.749073     4.484659    12.904682    -0.09522545]]\n",
            "Layer 3 Biases: [ 13.061119   0.        13.060965  13.054788  13.052723   0.\n",
            " -13.058224 -13.047278  13.061063 -13.056836]\n",
            "Layer 4 Weights: [[-13.358438   -13.497206  ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-12.768148   -13.546547  ]\n",
            " [-12.038583   -13.389891  ]\n",
            " [-13.385502   -12.930469  ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-13.437004   -12.57921   ]\n",
            " [-12.901773   -13.121325  ]\n",
            " [-12.633627   -13.722897  ]\n",
            " [-11.924053   -12.326573  ]]\n",
            "Layer 4 Biases: [-13.061051 -13.061229]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.6352\n",
            "Epoch 2/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.3125\n",
            "Epoch 2 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 19.335148   -18.908913    18.85657     19.331589    19.000336\n",
            "   18.571783    -0.25082165  19.025356    19.301193    -0.58613753]\n",
            " [ 19.186129   -17.913774    19.21685     18.4815      18.590803\n",
            "   19.485106    -0.45372224  19.137392   -18.567648    -0.37114614]\n",
            " [ 19.15758    -18.62898     19.417711    18.838072    18.994265\n",
            "   19.210356    -0.18728814  18.766136    18.429813    -0.56364506]\n",
            " [ 19.268162   -19.047777    19.295504    18.925655    19.30433\n",
            "   19.47072     -0.31802675  18.801039    18.97247     -0.5551134 ]\n",
            " [ 18.464396   -18.607243    18.879725    18.24716     18.990662\n",
            "   18.504555     0.41176218  19.455252    18.943075    -0.33738226]\n",
            " [ 18.92932    -18.60602     18.515265    18.996075    19.339405\n",
            "   18.938272    -0.2837247   18.449905    18.509653    -0.29698232]]\n",
            "Layer 1 Biases: [ 18.93818  -18.829674  18.937952  18.914392  18.93862   18.938194\n",
            "   0.        18.937351 -10.337966   0.      ]\n",
            "Layer 2 Weights: [[-19.1472      18.848404    19.356188    18.49655    -19.00575   ]\n",
            " [ 14.673501    18.577305    18.804266   -18.716934    -0.5413875 ]\n",
            " [-19.072094    19.006351    19.251766    19.074215   -19.111887  ]\n",
            " [-18.570518    19.067818    18.90588     19.32035    -18.864988  ]\n",
            " [-18.657776    19.410305    19.414923    19.479889   -18.71976   ]\n",
            " [-19.075237    18.957365    19.43563     19.056963   -18.853823  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-15.701688    18.887877    19.265831    19.055483   -18.494558  ]\n",
            " [-19.079926    19.037405    18.891941    18.601778   -19.188     ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-18.930008  18.93832   18.938791  18.934484 -18.934025]\n",
            "Layer 3 Weights: [[ 19.084476    -0.24632552  18.460787    18.398705   -17.390005\n",
            "   -0.33244935 -18.426577    13.657843    18.925888   -19.211676  ]\n",
            " [ 18.908836    -0.14825454  19.08134     19.435944    18.821089\n",
            "    0.44860345 -18.837576   -18.861902    19.450994   -18.300795  ]\n",
            " [ 19.31172     -0.13441947  19.211157    18.514698    19.53096\n",
            "   -0.58227986 -19.31978    -18.720552    19.5573     -19.497728  ]\n",
            " [ 19.538944     0.09806406  18.59911     18.719334    18.631922\n",
            "   -0.4941727  -18.326801   -18.142748    18.697271   -18.949892  ]\n",
            " [ 18.30812      0.3267088   19.385038    -0.25607768 -14.574823\n",
            "   -0.6121563  -18.583403     6.51311     18.759773    -0.09522545]]\n",
            "Layer 3 Biases: [ 18.938751   0.        18.938526  18.929564  18.926567   0.\n",
            " -18.93455  -18.91867   18.93867  -18.932537]\n",
            "Layer 4 Weights: [[-19.235596   -19.374672  ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-18.644806   -19.423769  ]\n",
            " [-17.543024   -19.127586  ]\n",
            " [-19.262735   -18.807972  ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-19.294819   -18.449604  ]\n",
            " [-18.77857    -18.998621  ]\n",
            " [-18.511      -19.600464  ]\n",
            " [-17.60472    -18.131832  ]]\n",
            "Layer 4 Biases: [-18.938652 -18.93891 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.3523\n",
            "Epoch 3/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 3 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 22.93791    -22.484798    22.459087    22.929129    22.603247\n",
            "   22.174412    -0.25082165  22.627676    22.862146    -0.58613753]\n",
            " [ 22.78883    -21.211676    22.819548    22.067768    22.1937\n",
            "   23.087917    -0.45372224  22.739824   -22.159775    -0.37114614]\n",
            " [ 22.760311   -22.154934    23.020405    22.427916    22.597162\n",
            "   22.813124    -0.18728814  22.368534    22.005007    -0.56364506]\n",
            " [ 22.870808   -22.584013    22.898073    22.514725    22.907177\n",
            "   23.073408    -0.31802675  22.403309    22.49686     -0.5551134 ]\n",
            " [ 22.067162   -22.188583    22.4824      21.792173    22.593573\n",
            "   22.107277     0.41176218  23.057863    22.533356    -0.33738226]\n",
            " [ 22.532122   -22.185526    22.117935    22.58351     22.94233\n",
            "   22.54104     -0.2837247   22.052256    22.102835    -0.29698232]]\n",
            "Layer 1 Biases: [ 22.541092 -22.411873  22.540823  22.512766  22.541616  22.541111\n",
            "   0.        22.540108 -12.301629   0.      ]\n",
            "Layer 2 Weights: [[-22.742859    22.451023    22.95907     22.097006   -22.604197  ]\n",
            " [ 17.53659     22.0306      22.397556   -22.26525     -0.5413875 ]\n",
            " [-22.659327    22.608797    22.85458     22.674465   -22.707075  ]\n",
            " [-22.158955    22.66894     22.508127    22.912807   -22.452564  ]\n",
            " [-22.257957    23.012823    23.017805    23.078732   -22.320738  ]\n",
            " [-22.659933    22.559896    23.038483    22.65756    -22.451199  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-18.579922    22.489258    22.868164    22.650385   -22.007757  ]\n",
            " [-22.673489    22.639952    22.494806    22.201271   -22.781     ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-22.53136   22.54126   22.541819  22.53669  -22.536146]\n",
            "Layer 3 Weights: [[ 22.68081     -0.24632552  22.054152    21.955156   -20.67245\n",
            "   -0.33244935 -22.013       16.322758    22.521667   -22.782516  ]\n",
            " [ 22.511513    -0.14825454  22.683743    23.03365     22.414253\n",
            "    0.44860345 -22.437962   -22.447742    23.053564   -21.900177  ]\n",
            " [ 22.91467     -0.13441947  22.814014    22.111155    23.130749\n",
            "   -0.58227986 -22.920797   -22.31787     23.160212   -23.096249  ]\n",
            " [ 23.14131      0.09806406  22.201015    22.3154      22.214294\n",
            "   -0.4941727  -21.926788   -21.711018    22.299467   -22.548143  ]\n",
            " [ 21.898571     0.3267088   22.969652    -0.25607768 -17.359495\n",
            "   -0.6121563  -22.159826     7.7549195   22.348951    -0.09522545]]\n",
            "Layer 3 Biases: [ 22.54177    0.        22.541506  22.530832  22.527262   0.\n",
            " -22.53677  -22.517859  22.541676 -22.534374]\n",
            "Layer 4 Weights: [[-22.838327   -22.977592  ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-22.247229   -23.026539  ]\n",
            " [-20.916843   -22.644657  ]\n",
            " [-22.865513   -22.410915  ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-22.885668   -22.048178  ]\n",
            " [-22.381079   -22.601437  ]\n",
            " [-22.113863   -23.203447  ]\n",
            " [-21.086763   -21.6904    ]]\n",
            "Layer 4 Biases: [-22.541655 -22.541962]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.3642 \n",
            "Epoch 4/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 4 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 25.308195   -24.837368    24.829214    25.295973    24.973629\n",
            "   24.544611    -0.25082165  24.99767     25.20487     -0.58613753]\n",
            " [ 25.159077   -23.381021    25.18979     24.427181    24.564077\n",
            "   25.458237    -0.45372224  25.109894   -24.52305     -0.37114614]\n",
            " [ 25.130577   -24.474585    25.390646    24.789686    24.967537\n",
            "   25.183414    -0.18728814  24.73858     24.35712     -0.56364506]\n",
            " [ 25.241018   -24.910444    25.268232    24.875986    25.277517\n",
            "   25.443645    -0.31802675  24.77327     24.815487    -0.5551134 ]\n",
            " [ 24.43745    -24.54475     24.852633    24.124392    24.963957\n",
            "   24.477537     0.41176218  25.42805     24.895412    -0.33738226]\n",
            " [ 24.902433   -24.540482    24.488161    24.943691    25.312723\n",
            "   24.911331    -0.2837247   24.422272    24.466803    -0.29698232]]\n",
            "Layer 1 Biases: [ 24.911476 -24.768602  24.911179  24.880157  24.91206   24.911499\n",
            "   0.        24.91039  -13.59236    0.      ]\n",
            "Layer 2 Weights: [[-25.108463    24.821217    25.329437    24.465773   -24.971638  ]\n",
            " [ 19.419466    24.30236     24.761597   -24.599644    -0.5413875 ]\n",
            " [-25.019377    24.978876    25.2249      25.043095   -25.07237   ]\n",
            " [-24.519798    25.038147    24.878073    25.276299   -24.81284   ]\n",
            " [-24.626543    25.38295     25.388168    25.446434   -24.689848  ]\n",
            " [-25.018309    24.93003     25.408829    25.026423   -24.817934  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-20.472769    24.858635    25.23817     25.01549    -24.319002  ]\n",
            " [-25.03771     25.010096    24.86516     24.569403   -25.144848  ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-24.900717  24.911663  24.912281  24.906609 -24.90601 ]\n",
            "Layer 3 Weights: [[ 25.046862    -0.24632552  24.418243    24.294914   -22.831608\n",
            "   -0.33244935 -24.372517    18.07511     24.88735    -25.13176   ]\n",
            " [ 24.881742    -0.14825454  25.053791    25.400604    24.778214\n",
            "    0.44860345 -24.806679   -24.806871    25.423723   -24.268234  ]\n",
            " [ 25.285082    -0.13441947  25.184362    24.477283    25.499075\n",
            "   -0.58227986 -25.289932   -24.684565    25.530596   -25.463737  ]\n",
            " [ 25.511335     0.09806406  24.570738    24.681273    24.57114\n",
            "   -0.4941727  -24.295246   -24.058565    24.669378   -24.915455  ]\n",
            " [ 24.26074      0.3267088   25.327976    -0.25607768 -19.190718\n",
            "   -0.6121563  -24.512749     8.570842    24.71028     -0.09522545]]\n",
            "Layer 3 Biases: [ 24.912228   0.        24.911934  24.900133  24.896185   0.\n",
            " -24.906698 -24.88579   24.912123 -24.90405 ]\n",
            "Layer 4 Weights: [[-25.208593   -25.347982  ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-24.617292   -25.39683   ]\n",
            " [-23.136223   -24.958454  ]\n",
            " [-25.23581    -24.78132   ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-25.2481     -24.415705  ]\n",
            " [-24.751198   -24.971758  ]\n",
            " [-24.484217   -25.573875  ]\n",
            " [-23.377474   -24.031551  ]]\n",
            "Layer 4 Biases: [-24.912098 -24.91244 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.3601  \n",
            "Epoch 5/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 1.0000 - loss: 0.3438\n",
            "Epoch 5 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 26.91181    -26.428972    26.43272     26.897253    26.577309\n",
            "   26.148167    -0.25082165  26.601086    26.7898      -0.58613753]\n",
            " [ 26.762665   -24.84844     26.793377    26.023424    26.167753\n",
            "   27.061874    -0.45372224  26.713362   -26.121914    -0.37114614]\n",
            " [ 26.734177   -26.043875    26.99423     26.387527    26.571213\n",
            "   26.78703     -0.18728814  26.342033    25.948416    -0.56364506]\n",
            " [ 26.844582   -26.484331    26.87176     26.473482    26.88117\n",
            "   27.047226    -0.31802675  26.376665    26.38408     -0.5551134 ]\n",
            " [ 26.041065   -26.138792    26.45621     25.7022      26.567638\n",
            "   26.081135     0.41176218  27.031595    26.493448    -0.33738226]\n",
            " [ 26.506065   -26.133703    26.091734    26.540457    26.91641\n",
            "   26.514946    -0.2837247   26.025702    26.066135    -0.29698232]]\n",
            "Layer 1 Biases: [ 26.515158 -26.363026  26.51484   26.48181   26.51578   26.515182\n",
            "   0.        26.514002 -14.464805   0.      ]\n",
            "Layer 2 Weights: [[-26.708904    26.424767    26.933105    26.068357   -26.573322  ]\n",
            " [ 20.692799    25.839191    26.360975   -26.178928    -0.5413875 ]\n",
            " [-26.616053    26.582348    26.828537    26.645588   -26.6726    ]\n",
            " [-26.11701     26.64103     26.481455    26.875307   -26.40967   ]\n",
            " [-26.229004    26.986454    26.991837    27.0483     -26.292665  ]\n",
            " [-26.61385     26.53354     27.012482    26.629072   -26.419142  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-21.752855    26.461632    26.841593    26.615593   -25.882593  ]\n",
            " [-26.637213    26.613613    26.46882     26.171556   -26.7441    ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-26.503702  26.515358  26.516014  26.509975 -26.509338]\n",
            "Layer 3 Weights: [[ 26.647604    -0.24632552  26.017656    25.877834   -24.292124\n",
            "   -0.33244935 -25.96883     19.260044    26.487843   -26.72111   ]\n",
            " [ 26.48532     -0.14825454  26.657246    27.001959    26.377539\n",
            "    0.44860345 -26.409231   -26.402924    27.02725    -25.870337  ]\n",
            " [ 26.888783    -0.13441947  26.78802     26.07808     27.10136\n",
            "   -0.58227986 -26.892767   -26.285746    27.134277   -27.065458  ]\n",
            " [ 27.114773     0.09806406  26.173971    26.281895    26.165644\n",
            "   -0.4941727  -25.89762    -25.646767    26.27274    -26.517054  ]\n",
            " [ 25.858854     0.3267088   26.92348     -0.25607768 -20.429066\n",
            "   -0.6121563  -26.104593     9.122127    26.307823    -0.09522545]]\n",
            "Layer 3 Biases: [ 26.515957   0.        26.515644  26.503078  26.498877   0.\n",
            " -26.51007  -26.487808  26.515846 -26.507252]\n",
            "Layer 4 Weights: [[-26.812193   -26.95167   ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-26.220755   -27.000444  ]\n",
            " [-24.63755    -26.523779  ]\n",
            " [-26.839432   -26.385015  ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-26.84639    -26.017448  ]\n",
            " [-26.354698   -26.575396  ]\n",
            " [-26.087877   -27.177588  ]\n",
            " [-24.927149   -25.615416  ]]\n",
            "Layer 4 Biases: [-26.51582  -26.516184]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.3606 \n",
            "Epoch 6/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 1.0000 - loss: 0.3281\n",
            "Epoch 6 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 28.010023   -27.518944    27.530859    27.993868    27.675568\n",
            "   27.24634     -0.25082165  27.699165    27.875196    -0.58613753]\n",
            " [ 27.860859   -25.85321     27.891571    27.116581    27.266008\n",
            "   28.160105    -0.45372224  27.811476   -27.216866    -0.37114614]\n",
            " [ 27.832382   -27.118536    28.092424    27.48178     27.669468\n",
            "   27.885244    -0.18728814  27.440134    27.038176    -0.56364506]\n",
            " [ 27.94276    -27.562147    27.969912    27.567495    27.97941\n",
            "   28.145418    -0.31802675  27.474728    27.458263    -0.5551134 ]\n",
            " [ 27.13928    -27.230436    27.554398    26.782707    27.665897\n",
            "   27.179337     0.41176218  28.129763    27.587835    -0.33738226]\n",
            " [ 27.604292   -27.224789    27.18992     27.63397     28.014675\n",
            "   27.613161    -0.2837247   27.123789    27.16141     -0.29698232]]\n",
            "Layer 1 Biases: [ 27.613417 -27.454931  27.613087  27.578676  27.614065  27.613443\n",
            "   0.        27.612213 -15.061742   0.      ]\n",
            "Layer 2 Weights: [[-27.80494     27.522934    28.031355    27.165865   -27.670214  ]\n",
            " [ 21.56446     26.89158     27.456284   -27.260447    -0.5413875 ]\n",
            " [-27.709503    27.680464    27.926767    27.743032   -27.768492  ]\n",
            " [-27.210829    27.73874     27.57951     27.970362   -27.503227  ]\n",
            " [-27.326424    28.084593    28.090086    28.145311   -27.390333  ]\n",
            " [-27.706524    27.631683    28.110722    27.726624   -27.515703  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-22.62915     27.559423    27.939672    27.711397   -26.953344  ]\n",
            " [-27.732605    27.71176     27.567068    27.268768   -27.839321  ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-27.601482  27.613626  27.61431   27.60802  -27.607357]\n",
            "Layer 3 Weights: [[ 27.743849    -0.24632552  27.112988    26.961847   -25.292158\n",
            "   -0.33244935 -27.062033    20.071104    27.583914   -27.809534  ]\n",
            " [ 27.583506    -0.14825454  27.75535     28.098623    27.472809\n",
            "    0.44860345 -27.506716   -27.495949    28.125402   -26.967516  ]\n",
            " [ 27.987055    -0.13441947  27.88626     27.174358    28.198662\n",
            "   -0.58227986 -27.990444   -27.38229     28.232536   -28.162373  ]\n",
            " [ 28.212866     0.09806406  27.271923    27.378054    27.257605\n",
            "   -0.4941727  -26.994982   -26.734404    27.370779   -27.613884  ]\n",
            " [ 26.953293     0.3267088   28.016127    -0.25607768 -21.276745\n",
            "   -0.6121563  -27.19473      9.499171    27.401869    -0.09522545]]\n",
            "Layer 3 Biases: [ 27.61425    0.        27.613924  27.600834  27.596457   0.\n",
            " -27.608118 -27.584927  27.614136 -27.60518 ]\n",
            "Layer 4 Weights: [[-27.910398   -28.049932  ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-27.318863   -28.09866   ]\n",
            " [-25.665583   -27.595718  ]\n",
            " [-27.93765    -27.483284  ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-27.940948   -27.114378  ]\n",
            " [-27.452833   -27.673628  ]\n",
            " [-27.186125   -28.27587   ]\n",
            " [-25.988352   -26.700079  ]]\n",
            "Layer 4 Biases: [-27.614105 -27.614485]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.3585 \n",
            "Epoch 7/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 7 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 28.765827   -28.269066    28.286612    28.74857     28.431402\n",
            "   28.002117    -0.25082165  28.454876    28.622162    -0.58613753]\n",
            " [ 28.61665    -26.54459     28.647362    27.868898    28.021841\n",
            "   28.91592     -0.45372224  28.567211   -27.970423    -0.37114614]\n",
            " [ 28.58818    -27.858099    28.848215    28.234852    28.4253\n",
            "   28.64105     -0.18728814  28.195862    27.78815     -0.56364506]\n",
            " [ 28.69854    -28.303886    28.725674    28.320402    28.735233\n",
            "   28.901209    -0.31802675  28.23043     28.197498    -0.5551134 ]\n",
            " [ 27.895086   -27.981712    28.310184    27.526302    28.421732\n",
            "   27.935133     0.41176218  28.885532    28.341       -0.33738226]\n",
            " [ 28.360106   -27.975674    27.945705    28.386534    28.770515\n",
            "   28.368967    -0.2837247   27.879507    27.915188    -0.29698232]]\n",
            "Layer 1 Biases: [ 28.369251 -28.206387  28.368914  28.333551  28.36992   28.369278\n",
            "   0.        28.368017 -15.472185   0.      ]\n",
            "Layer 2 Weights: [[-28.559242    28.27871     28.787186    27.921183   -28.425104  ]\n",
            " [ 22.1641      27.615788    28.210085   -28.00474     -0.5413875 ]\n",
            " [-28.462023    28.436201    28.682583    28.498306   -28.522694  ]\n",
            " [-27.963602    28.494198    28.335205    28.723988   -28.255821  ]\n",
            " [-28.081682    28.840345    28.845917    28.900288   -28.145761  ]\n",
            " [-28.458508    28.387438    28.866545    28.481972   -28.270369  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-23.231983    28.314936    28.695389    28.46554    -27.69021   ]\n",
            " [-28.486464    28.46752     28.322893    28.02388    -28.593061  ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-28.356987  28.369469  28.37017   28.363708 -28.363026]\n",
            "Layer 3 Weights: [[ 28.498295    -0.24632552  27.866804    27.70786    -25.980272\n",
            "   -0.33244935 -27.814383    20.628994    28.338242   -28.558588  ]\n",
            " [ 28.33929     -0.14825454  28.511078    28.853357    28.226585\n",
            "    0.44860345 -28.262018   -28.248175    28.881165   -27.722605  ]\n",
            " [ 28.742897    -0.13441947  28.642084    27.92883     28.953836\n",
            "   -0.58227986 -28.745882   -28.136944    28.98837    -28.91728   ]\n",
            " [ 28.968586     0.09806406  28.027546    28.132442    28.009098\n",
            "   -0.4941727  -27.750198   -27.482914    28.126461   -28.368736  ]\n",
            " [ 27.706491     0.3267088   28.768093    -0.25607768 -21.859863\n",
            "   -0.6121563  -27.944965     9.758318    28.154802    -0.09522545]]\n",
            "Layer 3 Biases: [ 28.370108   0.        28.369774  28.356323  28.351824   0.\n",
            " -28.363808 -28.339975  28.369993 -28.36079 ]\n",
            "Layer 4 Weights: [[-28.666197   -28.80577   ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-28.074596   -28.854465  ]\n",
            " [-26.372997   -28.333405  ]\n",
            " [-28.693459   -28.239126  ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-28.694235   -27.869295  ]\n",
            " [-28.208586   -28.429443  ]\n",
            " [-27.94195    -29.03172   ]\n",
            " [-26.718636   -27.446539  ]]\n",
            "Layer 4 Biases: [-28.369963 -28.37035 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.3684\n",
            "Epoch 8/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 8 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 29.286623   -28.785936    28.807371    29.268604    28.95222\n",
            "   28.522894    -0.25082165  28.975607    29.136854    -0.58613753]\n",
            " [ 29.137438   -27.020912    29.168148    28.387285    28.542656\n",
            "   29.436724    -0.45372224  29.087961   -28.489666    -0.37114614]\n",
            " [ 29.108973   -28.367682    29.369001    28.753761    28.946115\n",
            "   29.161846    -0.18728814  28.716604    28.304924    -0.56364506]\n",
            " [ 29.219318   -28.81497     29.24644     28.839197    29.256042\n",
            "   29.421995    -0.31802675  28.751156    28.706854    -0.5551134 ]\n",
            " [ 28.415882   -28.49938     28.830969    28.038668    28.94255\n",
            "   28.455925     0.41176218  29.406307    28.859972    -0.33738226]\n",
            " [ 28.880909   -28.493074    28.466488    28.905092    29.291336\n",
            "   28.889763    -0.2837247   28.400242    28.434586    -0.29698232]]\n",
            "Layer 1 Biases: [ 28.89007  -28.72418   28.889725  28.853708  28.89075   28.890097\n",
            "   0.        28.888813 -15.754748   0.      ]\n",
            "Layer 2 Weights: [[-29.079004    28.799484    29.308       28.44164    -28.945269  ]\n",
            " [ 22.577118    28.114769    28.729498   -28.517586    -0.5413875 ]\n",
            " [-28.980553    28.956951    29.203386    29.018736   -29.042383  ]\n",
            " [-28.48231     29.014755    28.855927    29.24328    -28.774403  ]\n",
            " [-28.6021      29.361103    29.36673     29.420511   -28.666298  ]\n",
            " [-28.976667    28.908201    29.387354    29.002453   -28.790379  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-23.647203    28.835531    29.216122    28.98519    -28.197931  ]\n",
            " [-29.005919    28.988285    28.843704    28.544197   -29.11243   ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-28.877577  28.890291  28.891008  28.884424 -28.883728]\n",
            "Layer 3 Weights: [[ 29.018152    -0.24632552  28.386229    28.221895   -26.45434\n",
            "   -0.33244935 -28.332796    21.013214    28.858019   -29.074724  ]\n",
            " [ 28.860075    -0.14825454  29.03182     29.373413    28.74598\n",
            "    0.44860345 -28.782469   -28.7665      29.401932   -28.242907  ]\n",
            " [ 29.26372     -0.13441947  29.162895    28.448706    29.4742\n",
            "   -0.58227986 -29.266424   -28.656946    29.50919    -29.437458  ]\n",
            " [ 29.489325     0.09806406  28.548216    28.65226     28.526917\n",
            "   -0.4941727  -28.270588   -27.998674    28.647175   -28.888872  ]\n",
            " [ 28.22549      0.3267088   29.28624     -0.25607768 -22.261482\n",
            "   -0.6121563  -28.461916     9.93665     28.673615    -0.09522545]]\n",
            "Layer 3 Biases: [ 28.890944   0.        28.890604  28.876902  28.872318   0.\n",
            " -28.884523 -28.86025   28.890823 -28.881453]\n",
            "Layer 4 Weights: [[-29.186989   -29.32659   ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-28.595345   -29.375261  ]\n",
            " [-26.860386   -28.841692  ]\n",
            " [-29.214256   -28.759949  ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-29.213291   -28.38948   ]\n",
            " [-28.729343   -28.950247  ]\n",
            " [-28.46276    -29.55255   ]\n",
            " [-27.221811   -27.960882  ]]\n",
            "Layer 4 Biases: [-28.890793 -28.891188]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.3642\n",
            "Epoch 9/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 0.3438\n",
            "Epoch 9 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 29.645233   -29.14184     29.165956    29.626688    29.310846\n",
            "   28.88149     -0.25082165  29.334171    29.491251    -0.58613753]\n",
            " [ 29.496042   -27.34884     29.526752    28.744232    28.90128\n",
            "   29.795338    -0.45372224  29.446539   -28.847204    -0.37114614]\n",
            " [ 29.46758    -28.718555    29.727604    29.111069    29.304739\n",
            "   29.520458    -0.18728814  29.075176    28.660755    -0.56364506]\n",
            " [ 29.577917   -29.166878    29.605034    29.196426    29.614662\n",
            "   29.780596    -0.31802675  29.109716    29.057571    -0.5551134 ]\n",
            " [ 28.774492   -28.855831    29.18957     28.39146     29.301176\n",
            "   28.81453      0.41176218  29.7649      29.217323    -0.33738226]\n",
            " [ 29.239521   -28.849339    28.825089    29.26216     29.649963\n",
            "   29.248375    -0.2837247   28.75881     28.79223     -0.29698232]]\n",
            "Layer 1 Biases: [ 29.248695 -29.080719  29.248344  29.211876  29.249386  29.248724\n",
            "   0.        29.247421 -15.949137   0.      ]\n",
            "Layer 2 Weights: [[-29.4369      29.158077    29.66662     28.80002    -29.303444  ]\n",
            " [ 22.861397    28.458328    29.087154   -28.87071     -0.5413875 ]\n",
            " [-29.337597    29.315529    29.562       29.377092   -29.400229  ]\n",
            " [-28.839474    29.3732      29.214483    29.600853   -29.131483  ]\n",
            " [-28.960451    29.719688    29.72535     29.778725   -29.024729  ]\n",
            " [-29.333458    29.266787    29.745974    29.360846   -29.148443  ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-23.933       29.194002    29.57469     29.343006   -28.54752   ]\n",
            " [-29.363604    29.34687     29.202324    28.902477   -29.470057  ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-29.236044  29.248919  29.249645  29.24298  -29.242275]\n",
            "Layer 3 Weights: [[ 29.376114    -0.24632552  28.743893    28.575838   -26.780714\n",
            "   -0.33244935 -28.68976     21.277643    29.215923   -29.430119  ]\n",
            " [ 29.218676    -0.14825454  29.390392    29.731514    29.103622\n",
            "    0.44860345 -29.14084    -29.123407    29.760525   -28.601175  ]\n",
            " [ 29.622347    -0.13441947  29.521515    28.80668     29.83251\n",
            "   -0.58227986 -29.624857   -29.015007    29.867817   -29.795639  ]\n",
            " [ 29.847895     0.09806406  28.90674     29.010193    28.883472\n",
            "   -0.4941727  -28.628918   -28.353811    29.005728   -29.247025  ]\n",
            " [ 28.58286      0.3267088   29.643023    -0.25607768 -22.537903\n",
            "   -0.6121563  -28.817873    10.059285    29.030855    -0.09522545]]\n",
            "Layer 3 Biases: [ 29.249578   0.        29.249235  29.235361  29.230719   0.\n",
            " -29.243078 -29.2185    29.249458 -29.23997 ]\n",
            "Layer 4 Weights: [[-29.545597   -29.685217  ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-28.95392    -29.733873  ]\n",
            " [-27.19595    -29.191671  ]\n",
            " [-29.572868   -29.118576  ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-29.570704   -28.747667  ]\n",
            " [-29.087929   -29.30886   ]\n",
            " [-28.82138    -29.91118   ]\n",
            " [-27.568266   -28.315039  ]]\n",
            "Layer 4 Biases: [-29.249428 -29.249826]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.3595\n",
            "Epoch 10/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 1.0000 - loss: 0.4375\n",
            "Epoch 10 ended\n",
            "Loss: 0.36500000953674316, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 29.891745   -29.386488    29.41245     29.872837    29.55737\n",
            "   29.127993    -0.25082165  29.580654    29.734863    -0.58613753]\n",
            " [ 29.742552   -27.574224    29.773262    28.989601    29.147802\n",
            "   30.041855    -0.45372224  29.69303    -29.092981    -0.37114614]\n",
            " [ 29.71409    -28.95974     29.974113    29.356686    29.551262\n",
            "   29.76697     -0.18728814  29.321663    28.905355    -0.56364506]\n",
            " [ 29.824419   -29.408773    29.851534    29.44199     29.86118\n",
            "   30.027105    -0.31802675  29.356194    29.298647    -0.5551134 ]\n",
            " [ 29.021004   -29.100857    29.436077    28.633966    29.547699\n",
            "   29.061039     0.41176218  30.011402    29.462969    -0.33738226]\n",
            " [ 29.486038   -29.094238    29.071596    29.507608    29.896488\n",
            "   29.494886    -0.2837247   29.005295    29.03808     -0.29698232]]\n",
            "Layer 1 Biases: [ 29.49522  -29.325806  29.494864  29.458084  29.495914  29.495249\n",
            "   0.        29.49393  -16.08264    0.      ]\n",
            "Layer 2 Weights: [[-29.682917    29.40458     29.913141    29.046371   -29.549656  ]\n",
            " [ 23.056732    28.694477    29.333012   -29.113445    -0.5413875 ]\n",
            " [-29.58303     29.562021    29.808517    29.623432   -29.646217  ]\n",
            " [-29.084993    29.619596    29.46096     29.84665    -29.376942  ]\n",
            " [-29.206785    29.966183    29.97187     30.024967   -29.271118  ]\n",
            " [-29.578714    29.513283    29.99249     29.607208   -29.39458   ]\n",
            " [ -0.4735129   -0.5431054   -0.35797963  -0.46335575   0.15069145]\n",
            " [-24.129381    29.440416    29.821173    29.588976   -28.787819  ]\n",
            " [-29.609478    29.593369    29.448843    29.148764   -29.715893  ]\n",
            " [ -0.59019417   0.6053298    0.4448337   -0.48877338   0.05333608]]\n",
            "Layer 2 Biases: [-29.482458  29.495443  29.496176  29.489454 -29.488743]\n",
            "Layer 3 Weights: [[ 29.62218     -0.24632552  28.989752    28.81914    -27.005028\n",
            "   -0.33244935 -28.935139    21.459318    29.46195    -29.674416  ]\n",
            " [ 29.465183    -0.14825454  29.63688     29.977678    29.34947\n",
            "    0.44860345 -29.38719    -29.368744    30.007025   -28.847452  ]\n",
            " [ 29.868872    -0.13441947  29.768032    29.052757    30.078815\n",
            "   -0.58227986 -29.871248   -29.261143    30.114342   -30.041857  ]\n",
            " [ 30.09438      0.09806406  29.153193    29.256243    29.128569\n",
            "   -0.4941727  -28.875237   -28.59793     29.2522     -29.493223  ]\n",
            " [ 28.82852      0.3267088   29.888279    -0.25607768 -22.72783\n",
            "   -0.6121563  -29.062557    10.143475    29.276426    -0.09522545]]\n",
            "Layer 3 Biases: [ 29.496109   0.        29.495764  29.48177   29.477089   0.\n",
            " -29.489553 -29.464766  29.495989 -29.486418]\n",
            "Layer 4 Weights: [[-29.792107   -29.931742  ]\n",
            " [  0.4617148    0.36376804]\n",
            " [-29.200409   -29.980385  ]\n",
            " [-27.42659    -29.432241  ]\n",
            " [-29.819384   -29.3651    ]\n",
            " [ -0.6400471   -0.41882688]\n",
            " [-29.816391   -28.993889  ]\n",
            " [-29.334423   -29.555378  ]\n",
            " [-29.067898   -30.15771   ]\n",
            " [-27.806408   -28.558487  ]]\n",
            "Layer 4 Biases: [-29.495956 -29.496359]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 0.3861 \n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA18AAAIjCAYAAAD80aFnAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAYeFJREFUeJzt3Xl4VPXd/vF7JstkTwghCxgJm5CwCkhEUaGE1aooVlSUpRbaalzI41Olrciixq3I41JxQ1wLdedHFYlYXCNYYiyyRERWsxG2hASSSeb8/sAZHZNACMmczMz7dV1zlTnrZ2a+tNw93/M5FsMwDAEAAAAAWpXV7AIAAAAAwB8QvgAAAADAAwhfAAAAAOABhC8AAAAA8ADCFwAAAAB4AOELAAAAADyA8AUAAAAAHkD4AgAAAAAPIHwBAAAAgAcQvgAA8LCdO3fKYrHo4YcfbtXzrFq1SgMGDFBISIgsFosOHTrUqucDAJwY4QsA2oClS5fKYrHoP//5j9ml+ARnuGnsdf/995tdYqvbv3+/rrrqKoWGhuqJJ57QSy+9pPDw8FY7n3MMO1+BgYHq1KmTpk2bph9++KHVznsy9957ry699FIlJCTIYrFo7ty5ptUCAIFmFwAAQGu55pprNH78+HrLzz77bBOq8awvv/xSFRUVWrBggTIyMjx23vnz56tLly46duyYvvjiCy1dulSffvqpvvnmG4WEhHisDqe//vWvSkxM1Nlnn63333/f4+cHgJ8jfAEAvFJlZeVJr+QMHDhQ1113nYcqaltKS0slSTExMS12zKZ85+PGjdPgwYMlSb/73e8UFxenBx54QCtWrNBVV13VYrU01Y4dO5SSkqKysjJ16NDB4+cHgJ9j2iEAeJGvvvpK48aNU1RUlCIiIjRy5Eh98cUXbtvY7XbNmzdPPXr0UEhIiNq3b69hw4YpJyfHtU1xcbGmT5+uM844QzabTUlJSbrsssu0c+fOk9bw4Ycf6oILLlB4eLhiYmJ02WWXacuWLa71r7/+uiwWiz766KN6+z711FOyWCz65ptvXMu2bt2qK6+8UrGxsQoJCdHgwYO1YsUKt/2cU9o++ugj3XjjjYqPj9cZZ5zR1K/thFJSUvTrX/9aq1evdt0flZaWpjfffLPett9//71+85vfKDY2VmFhYTr33HP1r3/9q952x44d09y5c3XWWWcpJCRESUlJuuKKK7R9+/Z62z799NPq1q2bbDabzjnnHH355Zdu65vzWw0fPlxTp06VJJ1zzjmyWCyaNm2aa/1rr72mQYMGKTQ0VHFxcbruuuvqTQ2cNm2aIiIitH37do0fP16RkZGaPHnyib7KBl1wwQWS5PbZhw8fruHDh9fbdtq0aUpJSXG9//m9cSf7nhrz8+MBgNm48gUAXmLTpk264IILFBUVpT/96U8KCgrSU089peHDh+ujjz5Senq6JGnu3LnKzs7W7373Ow0ZMkTl5eX6z3/+o7y8PI0aNUqSNHHiRG3atEk333yzUlJSVFpaqpycHO3evfuE/1j94IMPNG7cOHXt2lVz587V0aNH9dhjj+n8889XXl6eUlJSdPHFFysiIkL//Oc/ddFFF7ntv3z5cvXu3Vt9+vRxfabzzz9fnTp10p133qnw8HD985//1IQJE/TGG2/o8ssvd9v/xhtvVIcOHTRnzhxVVlae9DurqqpSWVlZveUxMTEKDPzpfwK3bdumSZMm6Q9/+IOmTp2q559/Xr/5zW+0atUq13dWUlKi8847T1VVVbrlllvUvn17vfDCC7r00kv1+uuvu2qtq6vTr3/9a61Zs0ZXX321br31VlVUVCgnJ0fffPONunXr5jrvq6++qoqKCv3+97+XxWLRgw8+qCuuuELff/+9goKCmv1b/eUvf1HPnj319NNPu6YBOs+7dOlSTZ8+Xeecc46ys7NVUlKi//u//9Nnn32mr776yu1KWW1trcaMGaNhw4bp4YcfVlhY2Em/819yhsR27dqd8r5OTfmeAMArGAAA0z3//POGJOPLL79sdJsJEyYYwcHBxvbt213LCgsLjcjISOPCCy90Levfv79x8cUXN3qcgwcPGpKMhx566JTrHDBggBEfH2/s37/ftezrr782rFarMWXKFNeya665xoiPjzdqa2tdy4qKigyr1WrMnz/ftWzkyJFG3759jWPHjrmWORwO47zzzjN69OjhWub8foYNG+Z2zMbs2LHDkNToKzc317Vt586dDUnGG2+84Vp2+PBhIykpyTj77LNdy2677TZDkvHJJ5+4llVUVBhdunQxUlJSjLq6OsMwDGPJkiWGJGPhwoX16nI4HG71tW/f3jhw4IBr/TvvvGNIMv7f//t/hmGc3m/V0Jiqqakx4uPjjT59+hhHjx51LV+5cqUhyZgzZ45r2dSpUw1Jxp133nlK5/vggw+Mffv2GXv27DFef/11o0OHDobNZjP27Nnj2vaiiy4yLrroonrHmDp1qtG5c2fX+6Z+T02xb98+Q5Jx9913N3kfAGhpTDsEAC9QV1en1atXa8KECeratatreVJSkq699lp9+umnKi8vl3T8qs6mTZu0bdu2Bo8VGhqq4OBgrV27VgcPHmxyDUVFRcrPz9e0adMUGxvrWt6vXz+NGjVK7777rmvZpEmTVFpaqrVr17qWvf7663I4HJo0aZIk6cCBA/rwww911VVXqaKiQmVlZSorK9P+/fs1ZswYbdu2rd5UuBkzZiggIKDJNc+cOVM5OTn1XmlpaW7bdezY0e0qW1RUlKZMmaKvvvpKxcXFkqR3331XQ4YM0bBhw1zbRUREaObMmdq5c6c2b94sSXrjjTcUFxenm2++uV49FovF7f2kSZPcrgg5p+h9//33kpr/WzXmP//5j0pLS3XjjTe6Nb+4+OKL1atXrwanUP7xj388pXNkZGSoQ4cOSk5O1pVXXqnw8HCtWLHitKaJnux7AgBvQfgCAC+wb98+VVVVqWfPnvXWpaamyuFwaM+ePZKOd5s7dOiQzjrrLPXt21f/+7//q//+97+u7W02mx544AG99957SkhI0IUXXqgHH3zQFTIas2vXLklqtIaysjLXVMCxY8cqOjpay5cvd22zfPlyDRgwQGeddZYk6bvvvpNhGLrrrrvUoUMHt9fdd98t6aemEU5dunQ56Xf1cz169FBGRka9V1RUlNt23bt3rxeMnHU6p83t2rWr0c/uXC8dv7epZ8+ebtMaG3PmmWe6vXcGDGfQau5v1ZgT/Ya9evVyrXcKDAw85dD0xBNPKCcnR6+//rrGjx+vsrIy2Wy2ZtXrdLLvCQC8BeELAHzMhRdeqO3bt2vJkiXq06ePnn32WQ0cOFDPPvusa5vbbrtN3377rbKzsxUSEqK77rpLqamp+uqrr1qkBpvNpgkTJuitt95SbW2tfvjhB3322Weuq16S5HA4JEm33357g1encnJy1L17d7fjhoaGtkh9bUVjV/EMw3D9ubV/qxOx2WyyWk/tnwpDhgxRRkaGJk6cqBUrVqhPnz669tprdeTIEdc2vwy6TnV1dQ0ub8r3BADegPAFAF6gQ4cOCgsLU0FBQb11W7duldVqVXJysmtZbGyspk+frn/84x/as2eP+vXrV+/hst26ddP//M//aPXq1frmm29UU1Ojv/3tb43W0LlzZ0lqtIa4uDi3NuSTJk1SWVmZ1qxZo9dee02GYbiFL+f0yaCgoAavTmVkZCgyMrJpX9Bpcl6F+7lvv/1W0k/d8jp37tzoZ3eul45/rwUFBbLb7S1W36n+Vo050W9YUFDgWt9SAgIClJ2drcLCQj3++OOu5e3atdOhQ4fqbf/LK28A4GsIXwDgBQICAjR69Gi98847bi3GS0pK9Oqrr2rYsGGuqXT79+932zciIkLdu3dXdXW1pOMdAI8dO+a2Tbdu3RQZGenapiFJSUkaMGCAXnjhBbd/OH/zzTdavXp1vYcZZ2RkKDY2VsuXL9fy5cs1ZMgQt2mD8fHxGj58uJ566ikVFRXVO9++fftO/KW0oMLCQr311luu9+Xl5XrxxRc1YMAAJSYmSpLGjx+v9evXKzc317VdZWWlnn76aaWkpLjuI5s4caLKysrcwobTqV6pae5v1ZjBgwcrPj5eixcvdtv/vffe05YtW3TxxRef8jFPZvjw4RoyZIgWLVrk+izdunXT1q1b3X7jr7/+Wp999lmLnx8A2hJazQNAG7JkyRKtWrWq3vJbb71V99xzj3JycjRs2DDdeOONCgwM1FNPPaXq6mo9+OCDrm3T0tI0fPhwDRo0SLGxsfrPf/6j119/XZmZmZKOX9EZOXKkrrrqKqWlpSkwMFBvvfWWSkpKdPXVV5+wvoceekjjxo3T0KFDdcMNN7hazUdHR9e7shYUFKQrrrhCy5YtU2VlpR5++OF6x3viiSc0bNgw9e3bVzNmzFDXrl1VUlKi3Nxc7d27V19//XUzvsWf5OXl6eWXX663vFu3bho6dKjr/VlnnaUbbrhBX375pRISErRkyRKVlJTo+eefd21z55136h//+IfGjRunW265RbGxsXrhhRe0Y8cOvfHGG67peVOmTNGLL76orKwsrV+/XhdccIEqKyv1wQcf6MYbb9Rll13W5PpP57dqSFBQkB544AFNnz5dF110ka655hpXq/mUlBTNmjXrlI/ZFP/7v/+r3/zmN1q6dKn+8Ic/6Le//a0WLlyoMWPG6IYbblBpaakWL16s3r17uxrHtJSXXnpJu3btUlVVlSTp448/1j333CNJuv7661v8ah8AnJCZrRYBAMc523Q39nK26c7LyzPGjBljREREGGFhYcaIESOMzz//3O1Y99xzjzFkyBAjJibGCA0NNXr16mXce++9Rk1NjWEYhlFWVmbcdNNNRq9evYzw8HAjOjraSE9PN/75z382qdYPPvjAOP/8843Q0FAjKirKuOSSS4zNmzc3uG1OTo4hybBYLG6txn9u+/btxpQpU4zExEQjKCjI6NSpk/HrX//aeP311+t9Pydqxf9zJ2s1P3XqVNe2nTt3Ni6++GLj/fffN/r162fYbDajV69exmuvvdZgrVdeeaURExNjhISEGEOGDDFWrlxZb7uqqirjL3/5i9GlSxcjKCjISExMNK688krXYwKc9TXUQl4/a4d+Or/Vib6z5cuXG2effbZhs9mM2NhYY/LkycbevXvdtpk6daoRHh5+0vM05Xx1dXVGt27djG7durkeFfDyyy8bXbt2NYKDg40BAwYY77//fqOt5k/2PZ3IRRdd1Og4+Pe//93kzwcALcFiGNytCgDwXykpKerTp49WrlxpdikAAB/HPV8AAAAA4AGELwAAAADwAMIXAAAAAHgA93wBAAAAgAdw5QsAAAAAPIDwBQAAAAAewEOWm8nhcKiwsFCRkZGyWCxmlwMAAADAJIZhqKKiQh07dpTV2vj1LcJXMxUWFio5OdnsMgAAAAC0EXv27NEZZ5zR6HrCVzNFRkZKOv4FR0VFmVqL3W7X6tWrNXr0aAUFBZlaC3wf4w2expiDpzHm4EmMN99QXl6u5ORkV0ZoDOGrmZxTDaOiotpE+AoLC1NUVBR/adHqGG/wNMYcPI0xB09ivPmWk92ORMMNAAAAAPAAwhcAAAAAeADhCwAAAAA8gHu+AAAA4NcMw1Btba3q6uo8fm673a7AwEAdO3bMlPOjaQICAhQYGHjaj5gifAEAAMBv1dTUqKioSFVVVaac3zAMJSYmas+ePTw7to0LCwtTUlKSgoODm30MwhcAAAD8ksPh0I4dOxQQEKCOHTsqODjY4wHI4XDoyJEjioiIOOHDeWEewzBUU1Ojffv2aceOHerRo0ezfyvCFwAAAPxSTU2NHA6HkpOTFRYWZkoNDodDNTU1CgkJIXy1YaGhoQoKCtKuXbtcv1dz8AsDAADArxF60BQtMU4YaQAAAADgAYQvAAAAAPAAwhcAAAAApaSkaNGiRU3efu3atbJYLDp06FCr1eRrCF8AAACAF7FYLCd8zZ07t1nH/fLLLzVz5swmb3/eeeepqKhI0dHRzTpfU/lSyKPbIQAAAOBFioqKXH9evny55syZo4KCAteyiIgI158Nw1BdXZ0CA0/+z/4OHTqcUh3BwcFKTEw8pX38HVe+AAAAgB8ZhqGqmlqPvo7W1MkwjCbXmJiY6HpFR0fLYrG43m/dulWRkZF67733NGjQINlsNn366afavn27LrvsMiUkJCgiIkLnnHOOPvjgA7fj/nLaocVi0bPPPqvLL79cYWFh6tGjh1asWOFa/8srUkuXLlVMTIzef/99paamKiIiQmPHjnULi7W1tbrlllsUExOj9u3b64477tDUqVM1YcKEZv1eknTw4EFNmTJF7dq1U1hYmMaNG6dt27a51u/atUuXXHKJ2rVrp/DwcPXu3Vvvvvuua9/JkyerQ4cOCg0NVY8ePfT88883u5aT4coXAAAA8KOj9jqlzXnf4+f9Zu4oRQQEtNjx7rzzTj388MPq2rWr2rVrpz179mj8+PG69957ZbPZ9OKLL+qSSy5RQUGBzjzzzEaPM2/ePD344IN66KGH9Nhjj2ny5MnatWuXYmNjG9y+qqpKDz/8sF566SVZrVZdd911uv322/XKK69Ikh544AG98sorev7555Wamqr/+7//09tvv60RI0Y0+7NOmzZN27Zt04oVKxQVFaU77rhD48eP1+bNmxUUFKSbbrpJNTU1+vjjjxUeHq7Nmze7rg7edddd2rx5s9577z3FxcXpu+++09GjR5tdy8kQvgAAAAAfM3/+fI0aNcr1PjY2Vv3793e9X7Bggd566y2tWLFCmZmZjR5n2rRpuuaaayRJ9913nx599FGtX79eY8eObXB7u92uxYsXq1u3bpKkzMxMzZ8/37X+scce0+zZs3X55ZdLkh5//HHXVajmcIauzz77TOedd54k6ZVXXlFycrLefvtt/eY3v9Hu3bs1ceJE9e3bV5LUtWtX1/67d+/W2WefrcGDB0s6fvWvNRG+vFxlda3Wf79f+fstGm92MQAAAF4uNChAm+eP8dj5HA6HKsorFBrUcle9JLnChNORI0c0d+5c/etf/1JRUZFqa2t19OhR7d69+4TH6devn+vP4eHhioqKUmlpaaPbh4WFuYKXJCUlJbm2P3z4sEpKSjRkyBDX+oCAAA0aNEgOh+OUPp/Tli1bFBgYqPT0dNey9u3bq2fPntqyZYsk6ZZbbtEf//hHrV69WhkZGZo4caLrc/3xj3/UxIkTlZeXp9GjR2vChAmuENcauOfLy20tLtf0FzbojR38lAAAAKfLYrEoLDjQo6/Q4ABZLJYW/Rzh4eFu72+//Xa99dZbuu+++/TJJ58oPz9fffv2VU1NzQmPExQUVO/7OVFQamj7U7mfrTX87ne/0/fff6/rr79eGzdu1ODBg/XYY49JksaNG6ddu3Zp1qxZKiws1MiRI3X77be3Wi38i93L9UyMksUildst2n+k2uxyAAAA0AZ99tlnmjZtmi6//HL17dtXiYmJ2rlzp0driI6OVkJCgr788kvXsrq6OuXl5TX7mKmpqaqtrdW6detcy/bv36+CggKlpaW5liUnJ+sPf/iD3nzzTf3P//yPnnnmGde6Dh06aOrUqXr55Ze1aNEiPf30082u52SYdujlImyB6hwbpp37q7Sl+IgS20WcfCcAAAD4lR49eujNN9/UJZdcIovForvuuqvZU/1Ox80336zs7Gx1795dvXr10mOPPaaDBw826crfxo0bFRkZ6XpvsVjUv39/XXbZZZoxY4aeeuopRUZG6s4771SnTp102WWXSZJuu+02jRs3TmeddZYOHjyof//730pNTZUkzZkzR4MGDVLv3r1VXV2tlStXuta1BsKXD0hNjPwxfJVrRCrPWgAAAIC7hQsX6re//a3OO+88xcXF6Y477lB5ebnH67jjjjtUXFysKVOmKCAgQDNnztSYMWMU0IROjxdeeKHb+4CAANXW1ur555/Xrbfeql//+teqqanRhRdeqHfffdc1BbKurk433XST9u7dq6ioKI0dO1aPPPKIpOPPKps9e7Z27typ0NBQXXDBBVq2bFnLf/AfWQyzJ2F6qfLyckVHR+vw4cOKiooytZb/yynQI2u+0yX9EvXYtYNMrQW+z263691339X48ePrzesGWgNjDp7GmPMfx44d044dO9SlSxeFhISYUoPD4VB5ebmioqJktfrfHUEOh0Opqam66qqrtGDBArPLOaETjZemZgOufPmA1KTjl1+3FFWYXAkAAADQuF27dmn16tW66KKLVF1drccff1w7duzQtddea3ZpHuF/8doHOcPXjv1VOmavM7kaAAAAoGFWq1VLly7VOeeco/PPP18bN27UBx980Kr3WbUlXPnyAQmRNoUHGqqslb4tqVC/M2LMLgkAAACoJzk5WZ999pnZZZiGK18+wGKxqFP48Vv3Nhd6/sZJAAAAACdH+PIRncKO/+eWIsIXAADAqaD/HJqiJcYJ4ctHuK58Eb4AAACaxNnNsqqqyuRK4A2c4+R0uqByz5eP6BR2PHxtKaqQw2HIaj35g+oAAAD8WUBAgGJiYlRaWipJCgsLa9LDfluSw+FQTU2Njh075pet5r2BYRiqqqpSaWmpYmJimvRMssYQvnxEQqgUFGDRkepa7TlYpc7tw80uCQAAoM1LTEyUJFcA8zTDMHT06FGFhoZ6PPjh1MTExLjGS3MRvnxEgFU6KyFCmwortKWonPAFAADQBBaLRUlJSYqPj5fdbvf4+e12uz7++GNdeOGFPNS7DQsKCjqtK15OhC8f0isxUpsKK7S5sFxj+ySZXQ4AAIDXCAgIaJF/XDfnvLW1tQoJCSF8+QEmlvqQ1MTjD1um6QYAAADQ9hC+fEhq0vHwtaWowuRKAAAAAPwS4cuHOK98/XDoqA5V1ZhcDQAAAICfI3z5kMiQIJ3RLlQSUw8BAACAtobw5WPSkqIkSZsLCV8AAABAW0L48jFpHY+HL+77AgAAANoWwpePSXVe+WLaIQAAANCmmB6+nnjiCaWkpCgkJETp6elav359o9suXbpUFovF7RUSEuJab7fbdccdd6hv374KDw9Xx44dNWXKFBUWFrodJyUlpd5x7r///lb7jJ7knHb4XWmFamodJlcDAAAAwMnU8LV8+XJlZWXp7rvvVl5envr3768xY8aotLS00X2ioqJUVFTkeu3atcu1rqqqSnl5ebrrrruUl5enN998UwUFBbr00kvrHWf+/Plux7n55ptb5TN62hntQhUZEih7naHvSo+YXQ4AAACAHwWaefKFCxdqxowZmj59uiRp8eLF+te//qUlS5bozjvvbHAfi8WixMTEBtdFR0crJyfHbdnjjz+uIUOGaPfu3TrzzDNdyyMjIxs9jjezWCxKS4rSuh0HtLmo3HUPGAAAAABzmRa+ampqtGHDBs2ePdu1zGq1KiMjQ7m5uY3ud+TIEXXu3FkOh0MDBw7Ufffdp969eze6/eHDh2WxWBQTE+O2/P7779eCBQt05pln6tprr9WsWbMUGNj411FdXa3q6mrX+/Ly4/dU2e122e32k33cVuU8v/M/eyZEaN2OA/pm70Fd1i/BzNLgg3453oDWxpiDpzHm4EmMN9/Q1N/PtPBVVlamuro6JSS4h4OEhARt3bq1wX169uypJUuWqF+/fjp8+LAefvhhnXfeedq0aZPOOOOMetsfO3ZMd9xxh6655hpFRf10BeiWW27RwIEDFRsbq88//1yzZ89WUVGRFi5c2Gi92dnZmjdvXr3lq1evVlhYWFM/dqtyXvWz77NICtCnm3bqXX1vblHwWb+8ygy0NsYcPI0xB09ivHm3qqqqJm1nMQzDaOVaGlRYWKhOnTrp888/19ChQ13L//SnP+mjjz7SunXrTnoMu92u1NRUXXPNNVqwYEG9dRMnTtTevXu1du1at/D1S0uWLNHvf/97HTlyRDabrcFtGrrylZycrLKyshMe2xPsdrtycnI0atQoBQUFaVNhuSY8+YWiQwP15ewRslgsptYH3/LL8Qa0NsYcPI0xB09ivPmG8vJyxcXF6fDhwyfMBqZd+YqLi1NAQIBKSkrclpeUlDT5XqygoCCdffbZ+u6779yW2+12XXXVVdq1a5c+/PDDk4aj9PR01dbWaufOnerZs2eD29hstgaDWVBQUJv5i+KsJbVTjAKtFh0+Wqt9VXXqFBNqdmnwQW1p7MM/MObgaYw5eBLjzbs19bczrdthcHCwBg0apDVr1riWORwOrVmzxu1K2InU1dVp48aNSkpKci1zBq9t27bpgw8+UPv27U96nPz8fFmtVsXHx5/6B2mDbIEB6h4fIUnaXMjzvgAAAIC2wNRuh1lZWZo6daoGDx6sIUOGaNGiRaqsrHR1P5wyZYo6deqk7OxsScfbw5977rnq3r27Dh06pIceeki7du3S7373O0nHg9eVV16pvLw8rVy5UnV1dSouLpYkxcbGKjg4WLm5uVq3bp1GjBihyMhI5ebmatasWbruuuvUrl07c76IVpCWFKWtxRXaUlSuUWk03QAAAADMZmr4mjRpkvbt26c5c+aouLhYAwYM0KpVq1xNOHbv3i2r9aeLcwcPHtSMGTNUXFysdu3aadCgQfr888+VlpYmSfrhhx+0YsUKSdKAAQPczvXvf/9bw4cPl81m07JlyzR37lxVV1erS5cumjVrlrKysjzzoT0krWOU3vzqB658AQAAAG2EqeFLkjIzM5WZmdngurVr17q9f+SRR/TII480eqyUlBSdrH/IwIED9cUXX5xynd4mNen4fW6biwhfAAAAQFtg2j1faF3O8LX7QJUqjvHcCAAAAMBshC8fFRserKToEEnS1uIKk6sBAAAAQPjyYWnOqYfc9wUAAACYjvDlw1IJXwAAAECbQfjyYWkdj4evLcWELwAAAMBshC8f5px2uLW4QrV1DpOrAQAAAPwb4cuHnRkbpvDgANXUOvR9WaXZ5QAAAAB+jfDlw6xWi3px3xcAAADQJhC+fJxz6uEWHrYMAAAAmIrw5eOcTTc2E74AAAAAUxG+fNzP280bhmFyNQAAAID/Inz5uJ4JkbJapP2VNdpXUW12OQAAAIDfInz5uNDgAHXtECFJ2sTUQwAAAMA0hC8/kErHQwAAAMB0hC8/4Ox4SNMNAAAAwDyELz/g7HhIu3kAAADAPIQvP+C88rWjrFJVNbUmVwMAAAD4J8KXH+gQaVNchE2GIW0trjC7HAAAAMAvEb78BFMPAQAAAHMRvvxEGh0PAQAAAFMRvvxEalKkJDoeAgAAAGYhfPmJ3j9OO9xaVKE6h2FyNQAAAID/IXz5iS5xEQoJsuqovU679leaXQ4AAADgdwhffiLAalHPRB62DAAAAJiF8OVH0pz3fdF0AwAAAPA4wpcfcXY8pN08AAAA4HmELz/ifNYX0w4BAAAAzyN8+RHnPV8l5dUqO1JtcjUAAACAfyF8+ZEIW6BS2odJYuohAAAA4GmELz/jnHpI+AIAAAA8i/DlZ5xNN+h4CAAAAHgW4cvPpCbRdAMAAAAwA+HLzzinHW7fV6lj9jqTqwEAAAD8B+HLzyRGhahdWJDqHIa2lRwxuxwAAADAbxC+/IzFYvnZ1MPDJlcDAAAA+A/Clx+i6QYAAADgeYQvP/RTu/kKkysBAAAA/Afhyw85w9fmonI5HIbJ1QAAAAD+gfDlh7p1iFBwgFVHqmu19+BRs8sBAAAA/ALhyw8FBVjVIyFCEs/7AgAAADyF8OWn0njYMgAAAOBRhC8/lUrHQwAAAMCjCF9+6qeOh4QvAAAAwBMIX37KeeXrh0NHdbjKbnI1AAAAgO8jfPmp6NAgndEuVBL3fQEAAACeQPjyY6k03QAAAAA8hvDlx5wdD7nvCwAAAGh9hC8/5my6QcdDAAAAoPURvvyY88rXttIK1dQ6TK4GAAAA8G2ELz92RrtQRYYEyl5n6LvSI2aXAwAAAPg0wpcfs1gsrqYb3PcFAAAAtC7Tw9cTTzyhlJQUhYSEKD09XevXr29026VLl8pisbi9QkJC3LYxDENz5sxRUlKSQkNDlZGRoW3btrltc+DAAU2ePFlRUVGKiYnRDTfcoCNH/PPKTxodDwEAAACPMDV8LV++XFlZWbr77ruVl5en/v37a8yYMSotLW10n6ioKBUVFbleu3btclv/4IMP6tFHH9XixYu1bt06hYeHa8yYMTp27Jhrm8mTJ2vTpk3KycnRypUr9fHHH2vmzJmt9jnbMlf4oukGAAAA0KpMDV8LFy7UjBkzNH36dKWlpWnx4sUKCwvTkiVLGt3HYrEoMTHR9UpISHCtMwxDixYt0l//+ldddtll6tevn1588UUVFhbq7bffliRt2bJFq1at0rPPPqv09HQNGzZMjz32mJYtW6bCwsLW/shtjrPj4ZbichmGYXI1AAAAgO8KNOvENTU12rBhg2bPnu1aZrValZGRodzc3Eb3O3LkiDp37iyHw6GBAwfqvvvuU+/evSVJO3bsUHFxsTIyMlzbR0dHKz09Xbm5ubr66quVm5urmJgYDR482LVNRkaGrFar1q1bp8svv7zB81ZXV6u6utr1vrz8+JUiu90uu93evC+hhTjP35w6UtrZFGi16FCVXXv2H1FSdMjJd4JfO53xBjQHYw6expiDJzHefENTfz/TwldZWZnq6urcrlxJUkJCgrZu3drgPj179tSSJUvUr18/HT58WA8//LDOO+88bdq0SWeccYaKi4tdx/jlMZ3riouLFR8f77Y+MDBQsbGxrm0akp2drXnz5tVbvnr1aoWFhZ38A3tATk5Os/brEBKgoiqLXvp//1afWK5+oWmaO96A5mLMwdMYc/Akxpt3q6qqatJ2poWv5hg6dKiGDh3qen/eeecpNTVVTz31lBYsWNCq5549e7aysrJc78vLy5WcnKzRo0crKiqqVc99Mna7XTk5ORo1apSCgoJOef8Pqzbqna+LFNbpLI0f0a0VKoQvOd3xBpwqxhw8jTEHT2K8+QbnrLiTMS18xcXFKSAgQCUlJW7LS0pKlJiY2KRjBAUF6eyzz9Z3330nSa79SkpKlJSU5HbMAQMGuLb5ZUOP2tpaHThw4ITntdlsstlsDdbQVv6iNLeWPp1i9M7XRfq2tLLNfBa0fW1p7MM/MObgaYw5eBLjzbs19bczreFGcHCwBg0apDVr1riWORwOrVmzxu3q1onU1dVp48aNrqDVpUsXJSYmuh2zvLxc69atcx1z6NChOnTokDZs2ODa5sMPP5TD4VB6enpLfDSv42y6Qbt5AAAAoPWYOu0wKytLU6dO1eDBgzVkyBAtWrRIlZWVmj59uiRpypQp6tSpk7KzsyVJ8+fP17nnnqvu3bvr0KFDeuihh7Rr1y797ne/k3S8E+Jtt92me+65Rz169FCXLl101113qWPHjpowYYIkKTU1VWPHjtWMGTO0ePFi2e12ZWZm6uqrr1bHjh1N+R7M5nzQ8q79Vao4ZldkCP+vCwAAANDSTA1fkyZN0r59+zRnzhwVFxdrwIABWrVqlathxu7du2W1/nRx7uDBg5oxY4aKi4vVrl07DRo0SJ9//rnS0tJc2/zpT39SZWWlZs6cqUOHDmnYsGFatWqV28OYX3nlFWVmZmrkyJGyWq2aOHGiHn30Uc998DYmNjxYiVEhKi4/poLiCg1OiTW7JAAAAMDnmN5wIzMzU5mZmQ2uW7t2rdv7Rx55RI888sgJj2exWDR//nzNnz+/0W1iY2P16quvnnKtviytY5SKy49pc1E54QsAAABoBaY+ZBltR9qPUw83F3LfFwAAANAaCF+Q9NN9XzTdAAAAAFoH4QuSfup4WFBcodo6h8nVAAAAAL6H8AVJUufYMIUFB6i61qEdZZVmlwMAAAD4HMIXJElWq0W9EiMlMfUQAAAAaA2EL7jwsGUAAACg9RC+4JKWFC2JjocAAABAayB8wcV15auwXIZhmFwNAAAA4FsIX3DpmRApq0XaX1mjfRXVZpcDAAAA+BTCF1xCgwPUJS5cEvd9AQAAAC2N8AU3aR1/vO+L8AUAAAC0KMIX3KQm/dhunqYbAAAAQIsifMFNWtLxphtbuPIFAAAAtCjCF9w4Ox5+X1apqppak6sBAAAAfAfhC27iI0MUF2GTYUgFxRVmlwMAAAD4DMIX6nHd98XUQwAAAKDFEL5Qj3PqIfd9AQAAAC2H8IV6nE036HgIAAAAtBzCF+pxhq+txRVyOAyTqwEAAAB8A+EL9XSJC5ct0KqqmjrtOlBldjkAAACATyB8oZ7AAKt6JfKwZQAAAKAlEb7QIGfTjc1Fh02uBAAAAPANhC80KDXJ2fGQZ30BAAAALYHwhQbR8RAAAABoWYQvNKjXj+GruPyY9h+pNrkaAAAAwPsRvtCgCFugOrcPk8TUQwAAAKAlEL7QqDTXfV9MPQQAAABOF+ELjXLd90X4AgAAAE4b4QuNcrWbp+kGAAAAcNoIX2iUs9389n1HdMxeZ3I1AAAAgHcjfKFRSdEhigkLUq3D0HelR8wuBwAAAPBqhC80ymKx8LwvAAAAoIUQvnBCNN0AAAAAWgbhCyeUSvgCAAAAWgThCyfk7Hi4pbBchmGYXA0AAADgvQhfOKFuHSIUHGBVRXWt9h48anY5AAAAgNcifOGEggOt6h4fIYmphwAAAMDpIHzhpHjYMgAAAHD6CF84KToeAgAAAKeP8IWT4soXAAAAcPoIXzip1MTj4euHQ0d1+Kjd5GoAAAAA70T4wklFhwWpU0yoJGkLUw8BAACAZiF8oUmYeggAAACcHsIXmiT1x6YbXPkCAAAAmofwhSah4yEAAABweghfaJLeP0473FZyRDW1DpOrAQAAALwP4QtNcka7UEXaAlVT59D2fUfMLgcAAADwOoQvNInFYuG+LwAAAOA0EL7QZHQ8BAAAAJqP8IUmo+kGAAAA0Hymh68nnnhCKSkpCgkJUXp6utavX9+k/ZYtWyaLxaIJEya4LbdYLA2+HnroIdc2KSkp9dbff//9LfmxfNLPpx0ahmFyNQAAAIB3MTV8LV++XFlZWbr77ruVl5en/v37a8yYMSotLT3hfjt37tTtt9+uCy64oN66oqIit9eSJUtksVg0ceJEt+3mz5/vtt3NN9/cop/NF/VIiFCA1aKDVXYVlx8zuxwAAADAq5gavhYuXKgZM2Zo+vTpSktL0+LFixUWFqYlS5Y0uk9dXZ0mT56sefPmqWvXrvXWJyYmur3eeecdjRgxot62kZGRbtuFh4e3+OfzNSFBAereIUIS930BAAAApyrQrBPX1NRow4YNmj17tmuZ1WpVRkaGcnNzG91v/vz5io+P1w033KBPPvnkhOcoKSnRv/71L73wwgv11t1///1asGCBzjzzTF177bWaNWuWAgMb/zqqq6tVXV3tel9efjx82O122e32E9bR2pzn90QdvRIjVFBSoY17D+nC7rGtfj60PZ4cb4DEmIPnMebgSYw339DU38+08FVWVqa6ujolJCS4LU9ISNDWrVsb3OfTTz/Vc889p/z8/Cad44UXXlBkZKSuuOIKt+W33HKLBg4cqNjYWH3++eeaPXu2ioqKtHDhwkaPlZ2drXnz5tVbvnr1aoWFhTWpntaWk5PT+ic5ZJEUoLVffasuVQ3/TvAPHhlvwM8w5uBpjDl4EuPNu1VVVTVpO9PC16mqqKjQ9ddfr2eeeUZxcXFN2mfJkiWaPHmyQkJC3JZnZWW5/tyvXz8FBwfr97//vbKzs2Wz2Ro81uzZs932Ky8vV3JyskaPHq2oqKhmfKKWY7fblZOTo1GjRikoKKhVzxW9fb/eWbpBBxWh8eOHteq50DZ5crwBEmMOnseYgycx3nyDc1bcyZgWvuLi4hQQEKCSkhK35SUlJUpMTKy3/fbt27Vz505dcsklrmUOh0OSFBgYqIKCAnXr1s217pNPPlFBQYGWL19+0lrS09NVW1urnTt3qmfPng1uY7PZGgxmQUFBbeYviidq6XtGO0nSrgNVqnZYFGHzmvyOFtaWxj78A2MOnsaYgycx3rxbU3870xpuBAcHa9CgQVqzZo1rmcPh0Jo1azR06NB62/fq1UsbN25Ufn6+63XppZdqxIgRys/PV3Jystv2zz33nAYNGqT+/fuftJb8/HxZrVbFx8ef/gfzce0jbEqIOh5CC4ppugEAAAA0lamXLbKysjR16lQNHjxYQ4YM0aJFi1RZWanp06dLkqZMmaJOnTopOztbISEh6tOnj9v+MTExklRveXl5uV577TX97W9/q3fO3NxcrVu3TiNGjFBkZKRyc3M1a9YsXXfddWrXrl3rfFAfk5YUpZLyfdpcWK5BnWm6AQAAADSFqeFr0qRJ2rdvn+bMmaPi4mINGDBAq1atcjXh2L17t6zWU784t2zZMhmGoWuuuabeOpvNpmXLlmnu3Lmqrq5Wly5dNGvWLLf7uXBiaR2j9O+CfdpcxJUvAAAAoKlMv2EnMzNTmZmZDa5bu3btCfddunRpg8tnzpypmTNnNrhu4MCB+uKLL06lRPxCWlK0JJ71BQAAAJwKUx+yDO+UmhQpSdpaXKHaOofJ1QAAAADegfCFU9a5fbjCggNUXevQzv2VZpcDAAAAeAXCF05ZgNWiXonHr35tYuohAAAA0CSELzRLatLxB0tvKaowuRIAAADAOxC+0CxpHY+HLzoeAgAAAE1D+EKzpP145YuOhwAAAEDTEL7QLL0So2S1SGVHqlVacczscgAAAIA2j/CFZgkNDlBKXLgk7vsCAAAAmoLwhWZj6iEAAADQdIQvNBtNNwAAAICmI3yh2X5qN0/4AgAAAE6G8IVm6/1j+Pp+3xEdrakzuRoAAACgbSN8odk6RNoUFxEshyEVlNB0AwAAADgRwheazWKxuKYe0nQDAAAAODHCF05LGvd9AQAAAE1C+MJpoeMhAAAA0DSEL5yWn1/5cjgMk6sBAAAA2i7CF05Ll7hwBQdaVVVTp90HqswuBwAAAGizCF84LYEBVvVKjJTE1EMAAADgRAhfOG1pdDwEAAAATorwhdNG0w0AAADg5AhfOG2ptJsHAAAATorwhdPmvOer6PAxHaisMbkaAAAAoG0ifOG0RYYEqXP7MElc/QIAAAAaQ/hCi0hj6iEAAABwQoQvtIhUOh4CAAAAJ0T4QotwtZvnyhcAAADQIMIXWoSz3fx3pUdUXVtncjUAAABA20P4QotIig5RdGiQah2GtpUcMbscAAAAoM0hfKFFWCwWph4CAAAAJ0D4QotxTj2k6QYAAABQH+ELLYZ28wAAAEDjCF9oMak/m3ZoGIbJ1QAAAABtC+ELLaZ7fISCAiyqOFarvQePml0OAAAA0KYQvtBiggOt6hEfKYmmGwAAAMAvEb7QolK57wsAAABoEOELLYqOhwAAAEDDCF9oUTzrCwAAAGgY4Qstyhm+9h48qsNH7SZXAwAAALQdhC+0qOiwIHWKCZUkbeXqFwAAAOBC+EKLS2XqIQAAAFAP4QstjqYbAAAAQH2EL7S4tKTjz/raUkz4AgAAAJyaFb727NmjvXv3ut6vX79et912m55++ukWKwzeKy0pWpL0bfER2escJlcDAAAAtA3NCl/XXnut/v3vf0uSiouLNWrUKK1fv15/+ctfNH/+/BYtEN7njHahirQFqqbOoe37jphdDgAAANAmNCt8ffPNNxoyZIgk6Z///Kf69Omjzz//XK+88oqWLl3akvXBC1mtFlfTjS003QAAAAAkNTN82e122Ww2SdIHH3ygSy+9VJLUq1cvFRUVtVx18FqpP973RdMNAAAA4Lhmha/evXtr8eLF+uSTT5STk6OxY8dKkgoLC9W+ffsWLRDeydXxkCtfAAAAgKRmhq8HHnhATz31lIYPH65rrrlG/fv3lyStWLHCNR0R/s3ZdGNzYbkMwzC5GgAAAMB8gc3Zafjw4SorK1N5ebnatWvnWj5z5kyFhYW1WHHwXj0SIhRgtehglV0l5dVKjA4xuyQAAADAVM268nX06FFVV1e7gteuXbu0aNEiFRQUKD4+vkULhHcKCQpQtw7hkqTNRYdNrgYAAAAwX7PC12WXXaYXX3xRknTo0CGlp6frb3/7myZMmKAnn3zylI71xBNPKCUlRSEhIUpPT9f69eubtN+yZctksVg0YcIEt+XTpk2TxWJxeznvSXM6cOCAJk+erKioKMXExOiGG27QkSO0RG9paT92PKTpBgAAANDM8JWXl6cLLrhAkvT6668rISFBu3bt0osvvqhHH320ycdZvny5srKydPfddysvL0/9+/fXmDFjVFpaesL9du7cqdtvv91Vwy+NHTtWRUVFrtc//vEPt/WTJ0/Wpk2blJOTo5UrV+rjjz/WzJkzm1w3msbZdGNLUYXJlQAAAADma1b4qqqqUmTk8Vbiq1ev1hVXXCGr1apzzz1Xu3btavJxFi5cqBkzZmj69OlKS0vT4sWLFRYWpiVLljS6T11dnSZPnqx58+apa9euDW5js9mUmJjoev38vrQtW7Zo1apVevbZZ5Wenq5hw4bpscce07Jly1RYWNjk2nFyzmd90fEQAAAAaGbDje7du+vtt9/W5Zdfrvfff1+zZs2SJJWWlioqKqpJx6ipqdGGDRs0e/Zs1zKr1aqMjAzl5uY2ut/8+fMVHx+vG264QZ988kmD26xdu1bx8fFq166dfvWrX+mee+5xtcDPzc1VTEyMBg8e7No+IyNDVqtV69at0+WXX97gMaurq1VdXe16X15+PFDY7XbZ7fYmfebW4jy/2XX8Uo+4UEnSzv2VOnjkqCJszRpuaGPa6niD72LMwdMYc/AkxptvaOrv16x/Dc+ZM0fXXnutZs2apV/96lcaOnSopONXwc4+++wmHaOsrEx1dXVKSEhwW56QkKCtW7c2uM+nn36q5557Tvn5+Y0ed+zYsbriiivUpUsXbd++XX/+8581btw45ebmKiAgQMXFxfWaggQGBio2NlbFxcWNHjc7O1vz5s2rt3z16tVtpsNjTk6O2SXUEx0UoMN2i154e7W6RJpdDVpSWxxv8G2MOXgaYw6exHjzblVVVU3arlnh68orr9SwYcNUVFTkesaXJI0cObLRK0enq6KiQtdff72eeeYZxcXFNbrd1Vdf7fpz37591a9fP3Xr1k1r167VyJEjm33+2bNnKysry/W+vLxcycnJGj16dJOv9rUWu92unJwcjRo1SkFBQabW8ktvluXpo21liunSV+OHJJtdDlpAWx5v8E2MOXgaYw6exHjzDc5ZcSfT7Hlgzvup9u7dK0k644wzTukBy3FxcQoICFBJSYnb8pKSEiUmJtbbfvv27dq5c6cuueQS1zKHwyHp+JWrgoICdevWrd5+Xbt2VVxcnL777juNHDlSiYmJ9Rp61NbW6sCBAw2e18lms8lms9VbHhQU1Gb+orSlWpx6d4rWR9vKVFBS2eZqw+lpi+MNvo0xB09jzMGTGG/eram/XbMabjgcDs2fP1/R0dHq3LmzOnfurJiYGC1YsMAViE4mODhYgwYN0po1a9yOu2bNGtc0xp/r1auXNm7cqPz8fNfr0ksv1YgRI5Sfn6/k5Iavquzdu1f79+9XUlKSJGno0KE6dOiQNmzY4Nrmww8/lMPhUHp6+ql8DWgCZ8dDmm4AAADA3zXrytdf/vIXPffcc7r//vt1/vnnSzp+P9bcuXN17Ngx3XvvvU06TlZWlqZOnarBgwdryJAhWrRokSorKzV9+nRJ0pQpU9SpUydlZ2crJCREffr0cds/JiZGklzLjxw5onnz5mnixIlKTEzU9u3b9ac//Undu3fXmDFjJEmpqakaO3asZsyYocWLF8tutyszM1NXX321Onbs2JyvAyfgfNZXQXG56hyGAqwWkysCAAAAzNGs8PXCCy/o2Wef1aWXXupa1q9fP3Xq1Ek33nhjk8PXpEmTtG/fPs2ZM0fFxcUaMGCAVq1a5WrCsXv3blmtTb84FxAQoP/+97964YUXdOjQIXXs2FGjR4/WggUL3KYMvvLKK8rMzNTIkSNltVo1ceLEU3o+GZquc/twhQYF6Ki9TjvKKtU9PsLskgAAAABTNCt8HThwQL169aq3vFevXjpw4MApHSszM1OZmZkNrlu7du0J9126dKnb+9DQUL3//vsnPWdsbKxeffXVppaI0xBgtahXUqS+2n1Im4vKCV8AAADwW82656t///56/PHH6y1//PHH1a9fv9MuCr7FOfVwcyH3fQEAAMB/NevK14MPPqiLL75YH3zwgas5Rm5urvbs2aN33323RQuE90v9MXxtoekGAAAA/FizrnxddNFF+vbbb3X55Zfr0KFDOnTokK644gpt2rRJL730UkvXCC9Hx0MAAADgNJ7z1bFjx3qNNb7++ms999xzevrpp0+7MPiOXomRslikfRXV2ldRrQ6R9Z+XBgAAAPi6Zl35Ak5FWHCgusSFS2LqIQAAAPwX4Qse4bzvi6mHAAAA8FeEL3gEHQ8BAADg707pnq8rrrjihOsPHTp0OrXAh9F0AwAAAP7ulMJXdHT0SddPmTLltAqCb3Je+fp+3xEds9cpJCjA5IoAAAAAzzql8PX888+3Vh3wcfGRNrUPD9b+yhoVFFeof3KM2SUBAAAAHsU9X/AIi8XC1EMAAAD4NcIXPMY59ZB28wAAAPBHhC94TCodDwEAAODHCF/wGOe0wy1F5XI4DJOrAQAAADyL8AWP6RoXruBAqypr6rT7QJXZ5QAAAAAeRfiCxwQGWNUrMVIS930BAADA/xC+4FGpiXQ8BAAAgH8ifMGjXO3maboBAAAAP0P4gkf9vOkGAAAA4E8IX/Ao5z1fhYeP6WBljcnVAAAAAJ5D+IJHRYYE6czYMElc/QIAAIB/IXzB49KSaLoBAAAA/0P4gse5mm4QvgAAAOBHCF/wuNQkOh4CAADA/xC+4HHOK1/flR5RdW2dydUAAAAAnkH4gsd1jA5RdGiQah2Gvis9YnY5AAAAgEcQvuBxFotFqUnHW84z9RAAAAD+gvAFU6QlRUui6QYAAAD8B+ELpnB1POTKFwAAAPwE4QumcD7ra0tRuQzDMLkaAAAAoPURvmCK7vERCgqwqPxYrX44dNTscgAAAIBWR/iCKYIDreoeT9MNAAAA+A/CF0zz09TDCpMrAQAAAFof4QumcbWbLzpsciUAAABA6yN8wTSujoe0mwcAAIAfIHzBNM5ph3sOHFX5MbvJ1QAAAACti/AF08SEBatTTKgkaSv3fQEAAMDHEb5gKtd9X4Xc9wUAAADfRviCqZxTD7nvCwAAAL6O8AVTOZtu0G4eAAAAvo7wBVOl/njlq6CkQvY6h8nVAAAAAK2H8AVTJbcLU4QtUDW1Dn2/r9LscgAAAIBWQ/iCqaxWCw9bBgAAgF8gfMF0zqYb3PcFAAAAX0b4gumc931tLqTjIQAAAHwX4Qumc3Y83FxULsMwTK4GAAAAaB2EL5jurIRIBVgtOlBZo9KKarPLAQAAAFoF4QumCwkKUNe4cElMPQQAAIDvInyhTfj51EMAAADAFxG+0Cak0XQDAAAAPo7whTbBeeVrC1e+AAAA4KNMD19PPPGEUlJSFBISovT0dK1fv75J+y1btkwWi0UTJkxwLbPb7brjjjvUt29fhYeHq2PHjpoyZYoKCwvd9k1JSZHFYnF73X///S35sXCKnO3md+yvVGV1rcnVAAAAAC3P1PC1fPlyZWVl6e6771ZeXp769++vMWPGqLS09IT77dy5U7fffrsuuOACt+VVVVXKy8vTXXfdpby8PL355psqKCjQpZdeWu8Y8+fPV1FRket18803t+hnw6mJi7ApPtImw5C2FvOwZQAAAPgeU8PXwoULNWPGDE2fPl1paWlavHixwsLCtGTJkkb3qaur0+TJkzVv3jx17drVbV10dLRycnJ01VVXqWfPnjr33HP1+OOPa8OGDdq9e7fbtpGRkUpMTHS9wsPDW+UzoumYeggAAABfFmjWiWtqarRhwwbNnj3btcxqtSojI0O5ubmN7jd//nzFx8frhhtu0CeffHLS8xw+fFgWi0UxMTFuy++//34tWLBAZ555pq699lrNmjVLgYGNfx3V1dWqrv7pGVTl5ccDgt1ul91uP2kdrcl5frPrOF094yO0tmCfvvnhkOz2jmaXg0b4yniD92DMwdMYc/AkxptvaOrvZ1r4KisrU11dnRISEtyWJyQkaOvWrQ3u8+mnn+q5555Tfn5+k85x7Ngx3XHHHbrmmmsUFRXlWn7LLbdo4MCBio2N1eeff67Zs2erqKhICxcubPRY2dnZmjdvXr3lq1evVlhYWJPqaW05OTlml3BajpVZJAUod8sevRu40+xycBLePt7gfRhz8DTGHDyJ8ebdqqqqmrSdaeHrVFVUVOj666/XM888o7i4uJNub7fbddVVV8kwDD355JNu67Kyslx/7tevn4KDg/X73/9e2dnZstlsDR5v9uzZbvuVl5crOTlZo0ePdgt2ZrDb7crJydGoUaMUFBRkai2no9e+Sr2w7TOVVAdozNjRCrBazC4JDfCV8QbvwZiDpzHm4EmMN9/gnBV3MqaFr7i4OAUEBKikpMRteUlJiRITE+ttv337du3cuVOXXHKJa5nD4ZAkBQYGqqCgQN26dZP0U/DatWuXPvzww5OGo/T0dNXW1mrnzp3q2bNng9vYbLYGg1lQUFCb+YvSlmppju6J0QoNCtBRe51+KK9Rtw4RZpeEE/D28Qbvw5iDpzHm4EmMN+/W1N/OtIYbwcHBGjRokNasWeNa5nA4tGbNGg0dOrTe9r169dLGjRuVn5/vel166aUaMWKE8vPzlZycLOmn4LVt2zZ98MEHat++/Ulryc/Pl9VqVXx8fMt9QJyyAKtFPRMjJfGwZQAAAPgeU6cdZmVlaerUqRo8eLCGDBmiRYsWqbKyUtOnT5ckTZkyRZ06dVJ2drZCQkLUp08ft/2dTTScy+12u6688krl5eVp5cqVqqurU3FxsSQpNjZWwcHBys3N1bp16zRixAhFRkYqNzdXs2bN0nXXXad27dp57sOjQWkdo5S/55A2F5Xrkv403QAAAIDvMDV8TZo0Sfv27dOcOXNUXFysAQMGaNWqVa4mHLt375bV2vSLcz/88INWrFghSRowYIDbun//+98aPny4bDabli1bprlz56q6ulpdunTRrFmz3O7ngnnSkmg3DwAAAN9kesONzMxMZWZmNrhu7dq1J9x36dKlbu9TUlJkGMYJ9xk4cKC++OKLUykRHpT6Y/hi2iEAAAB8jakPWQZ+qVdipCwWqbSiWvsqqk++AwAAAOAlCF9oU8JtgerSPlwSUw8BAADgWwhfaHNSO3LfFwAAAHwP4QttjrPpxmbCFwAAAHwI4QttThpNNwAAAOCDCF9oc9J+nHb4fVmljtnrTK4GAAAAaBmEL7Q58ZE2tQ8PVp3D0LclFWaXAwAAALQIwhfaHIvFwvO+AAAA4HMIX2iTnFMPaboBAAAAX0H4QpvkbLpBu3kAAAD4CsIX2qRUV/iqkMNhmFwNAAAAcPoIX2iTunYIV3CgVUeqa7XnYJXZ5QAAAACnjfCFNikowKqeCZGSmHoIAAAA30D4QpvFw5YBAADgSwhfaLNSk45f+aLjIQAAAHwB4QttVlrHaElc+QIAAIBvIHyhzer145WvwsPHdKiqxuRqAAAAgNND+EKbFRUSpOTYUElMPQQAAID3I3yhTaPpBgAAAHwF4QttWlrS8fu+thRVmFwJAAAAcHoIX2jT0jr+eOWLaYcAAADwcoQvtGnOdvPflVaoptZhcjUAAABA8xG+0KZ1iglVVEig7HWGtpUy9RAAAADei/CFNs1isbimHnLfFwAAALwZ4QttXiodDwEAAOADCF9o81zt5osOm1wJAAAA0HyEL7R5P592aBiGydUAAAAAzUP4QpvXIz5SQQEWHT5qV+HhY2aXAwAAADQL4QttXnCgVd06REjivi8AAAB4L8IXvILrYcuELwAAAHgpwhe8grPpxpYiwhcAAAC8E+ELXuGnjoeELwAAAHgnwhe8gvNZX7sPVKnimN3kagAAAIBTR/iCV2gXHqyO0SGSpK3FFSZXAwAAAJw6whe8Bk03AAAA4M0IX/AazqmHhC8AAAB4I8IXvAZNNwAAAODNCF/wGs5phwUlFaqtc5hcDQAAAHBqCF/wGsntwhQeHKCaWoe+L6s0uxwAAADglBC+4DWsVgv3fQEAAMBrEb7gVZxTD7dw3xcAAAC8DOELXoWmGwAAAPBWhC94lZ9POzQMw+RqAAAAgKYjfMGr9EyMlNUi7a+sUWlFtdnlAAAAAE1G+IJXCQkKULcOEZKYeggAAADvQviC16HjIQAAALwR4Qtex9nxkCtfAAAA8CaEL3gdZ8dD2s0DAADAmxC+4HWc0w53lFWqqqbW5GoAAACApiF8wet0iLSpQ6RNhiFtLa4wuxwAAACgSUwPX0888YRSUlIUEhKi9PR0rV+/vkn7LVu2TBaLRRMmTHBbbhiG5syZo6SkJIWGhiojI0Pbtm1z2+bAgQOaPHmyoqKiFBMToxtuuEFHjhxpqY8ED0ij6QYAAAC8jKnha/ny5crKytLdd9+tvLw89e/fX2PGjFFpaekJ99u5c6duv/12XXDBBfXWPfjgg3r00Ue1ePFirVu3TuHh4RozZoyOHTvm2mby5MnatGmTcnJytHLlSn388ceaOXNmi38+tB5n0w3u+wIAAIC3MDV8LVy4UDNmzND06dOVlpamxYsXKywsTEuWLGl0n7q6Ok2ePFnz5s1T165d3dYZhqFFixbpr3/9qy677DL169dPL774ogoLC/X2229LkrZs2aJVq1bp2WefVXp6uoYNG6bHHntMy5YtU2FhYWt+XLQgV7t5whcAAAC8RKBZJ66pqdGGDRs0e/Zs1zKr1aqMjAzl5uY2ut/8+fMVHx+vG264QZ988onbuh07dqi4uFgZGRmuZdHR0UpPT1dubq6uvvpq5ebmKiYmRoMHD3Ztk5GRIavVqnXr1unyyy9v8LzV1dWqrq52vS8vP/6PfrvdLrvdfmofvoU5z292HZ50VocwSdLWonIdq65RgNVickX+wx/HG8zFmIOnMebgSYw339DU38+08FVWVqa6ujolJCS4LU9ISNDWrVsb3OfTTz/Vc889p/z8/AbXFxcXu47xy2M61xUXFys+Pt5tfWBgoGJjY13bNCQ7O1vz5s2rt3z16tUKCwtrdD9PysnJMbsEj3EYUpA1QEftDr301nuKDzW7Iv/jT+MNbQNjDp7GmIMnMd68W1VVVZO2My18naqKigpdf/31euaZZxQXF+fx88+ePVtZWVmu9+Xl5UpOTtbo0aMVFRXl8Xp+zm63KycnR6NGjVJQUJCptXjS0r3r9PXew4o/a6DG9000uxy/4a/jDeZhzMHTGHPwJMabb3DOijsZ08JXXFycAgICVFJS4ra8pKREiYn1/yG9fft27dy5U5dccolrmcPhkHT8ylVBQYFrv5KSEiUlJbkdc8CAAZKkxMTEeg09amtrdeDAgQbP62Sz2WSz2eotDwoKajN/UdpSLZ6Q1jFaX+89rILSSl3mR5+7rfC38QbzMebgaYw5eBLjzbs19bczreFGcHCwBg0apDVr1riWORwOrVmzRkOHDq23fa9evbRx40bl5+e7XpdeeqlGjBih/Px8JScnq0uXLkpMTHQ7Znl5udatW+c65tChQ3Xo0CFt2LDBtc2HH34oh8Oh9PT0VvzEaGnOjoc03QAAAIA3MHXaYVZWlqZOnarBgwdryJAhWrRokSorKzV9+nRJ0pQpU9SpUydlZ2crJCREffr0cds/JiZGktyW33bbbbrnnnvUo0cPdenSRXfddZc6duzoeh5Yamqqxo4dqxkzZmjx4sWy2+3KzMzU1VdfrY4dO3rkc6NlOJ/1Rbt5AAAAeANTw9ekSZO0b98+zZkzR8XFxRowYIBWrVrlapixe/duWa2ndnHuT3/6kyorKzVz5kwdOnRIw4YN06pVqxQSEuLa5pVXXlFmZqZGjhwpq9WqiRMn6tFHH23Rz4bW1ysxUhaLVFJerbIj1YqLqD8tFAAAAGgrTG+4kZmZqczMzAbXrV279oT7Ll26tN4yi8Wi+fPna/78+Y3uFxsbq1dfffVUykQbFG4LVEr7cO0oq9SWonJd0KOD2SUBAAAAjTL1IcvA6WLqIQAAALwF4QtezdV0o5DwBQAAgLaN8AWvlpoUKYmOhwAAAGj7CF/wamlJ0ZKk7fsqdcxeZ3I1AAAAQOMIX/BqCVE2xYYHq85haFvJEbPLAQAAABpF+IJXs1gsrqYbm4sOm1wNAAAA0DjCF7ye674vmm4AAACgDSN8wes5Ox5uKaowuRIAAACgcYQveD1n043NReVyOAyTqwEAAAAaRviC1+vaIVzBAVYdqa7V3oNHzS4HAAAAaBDhC14vKMCqsxIjJNF0AwAAAG0X4Qs+4aeOh9z3BQAAgLaJ8AWf4ApfdDwEAABAG0X4gk9ITXJ2PCR8AQAAoG0ifMEnpP7Ybv6HQ0d1uMpucjUAAABAfYQv+ISokCAlx4ZKOt5yHgAAAGhrCF/wGamJzqYbhC8AAAC0PYQv+Iy0jjTdAAAAQNtF+ILPSKPpBgAAANowwhd8hvPK17bSCtXUOkyuBgAAAHBH+ILP6BQTqqiQQNnrDH1XesTscgAAAAA3hC/4DIvFwvO+AAAA0GYRvuBTXE03CF8AAABoYwhf8CnOK190PAQAAEBbQ/iCT3F2PNxcVC7DMEyuBgAAAPgJ4Qs+pUdChAKtFh0+alfR4WNmlwMAAAC4EL7gU2yBAeoeHyGJqYcAAABoWwhf8Dk/n3oIAAAAtBWEL/gcZ8dD2s0DAACgLSF8wedw5QsAAABtEeELPsfZbn7X/ipVHLObXA0AAABwHOELPqddeLCSokMkSVuLK0yuBgAAADiO8AWf5Jx6yH1fAAAAaCsIX/BJzqYbtJsHAABAW0H4gk9KpekGAAAA2hjCF3ySc9phQXGFauscJlcDAAAAEL7go86MDVN4cICqax3aUVZpdjkAAAAA4Qu+yWq1qBdTDwEAANCGEL7gs1wPW6bpBgAAANoAwhd8lqvjIVe+AAAA0AYQvuCzfn7lyzAMk6sBAACAvyN8wWf1TIyU1SLtr6zRvopqs8sBAACAnyN8wWeFBAWoa4cISUw9BAAAgPkIX/BpaXQ8BAAAQBtB+IJPczXdoOMhAAAATEb4gk9L5coXAAAA2gjCF3yac9rhjrJKVdXUmlwNAAAA/BnhCz6tQ6RNHSJtMgypoLjC7HIAAADgxwhf8HlMPQQAAEBbQPiCz3NOPdxC+AIAAICJCF/weXQ8BAAAQFtgevh64oknlJKSopCQEKWnp2v9+vWNbvvmm29q8ODBiomJUXh4uAYMGKCXXnrJbRuLxdLg66GHHnJtk5KSUm/9/fff32qfEeZyXvnaWlyhOodhcjUAAADwV4Fmnnz58uXKysrS4sWLlZ6erkWLFmnMmDEqKChQfHx8ve1jY2P1l7/8Rb169VJwcLBWrlyp6dOnKz4+XmPGjJEkFRUVue3z3nvv6YYbbtDEiRPdls+fP18zZsxwvY+MjGyFT4i2oEtcuEKCrKqqqdOu/ZXq2iHC7JIAAADgh0y98rVw4ULNmDFD06dPV1pamhYvXqywsDAtWbKkwe2HDx+uyy+/XKmpqerWrZtuvfVW9evXT59++qlrm8TERLfXO++8oxEjRqhr165ux4qMjHTbLjw8vFU/K8wTYLWoZ6Lzvi86HgIAAMAcpl35qqmp0YYNGzR79mzXMqvVqoyMDOXm5p50f8Mw9OGHH6qgoEAPPPBAg9uUlJToX//6l1544YV66+6//34tWLBAZ555pq699lrNmjVLgYGNfx3V1dWqrq52vS8vP37/kN1ul91uP2m9rcl5frPraMt6JUTo6z2H9M3egxqdGmd2OV6N8QZPY8zB0xhz8CTGm29o6u9nWvgqKytTXV2dEhIS3JYnJCRo69atje53+PBhderUSdXV1QoICNDf//53jRo1qsFtX3jhBUVGRuqKK65wW37LLbdo4MCBio2N1eeff67Zs2erqKhICxcubPS82dnZmjdvXr3lq1evVlhY2Ik+qsfk5OSYXUKbVbffIilAH/13u3rZt5ldjk9gvMHTGHPwNMYcPInx5t2qqqqatJ2p93w1R2RkpPLz83XkyBGtWbNGWVlZ6tq1q4YPH15v2yVLlmjy5MkKCQlxW56VleX6c79+/RQcHKzf//73ys7Ols1ma/C8s2fPdtuvvLxcycnJGj16tKKiolrmwzWT3W5XTk6ORo0apaCgIFNraasSdx/S68+s135HqMaPv8jscrwa4w2expiDpzHm4EmMN9/gnBV3MqaFr7i4OAUEBKikpMRteUlJiRITExvdz2q1qnv37pKkAQMGaMuWLcrOzq4Xvj755BMVFBRo+fLlJ60lPT1dtbW12rlzp3r27NngNjabrcFgFhQU1Gb+orSlWtqa3me0k8UilZRXq7zaofYRDYdsNB3jDZ7GmIOnMebgSYw379bU3860hhvBwcEaNGiQ1qxZ41rmcDi0Zs0aDR06tMnHcTgcbvdiOT333HMaNGiQ+vfvf9Jj5Ofny2q1NthhEb4hwhaolPbHm6rQdAMAAABmMHXaYVZWlqZOnarBgwdryJAhWrRokSorKzV9+nRJ0pQpU9SpUydlZ2dLOn7f1eDBg9WtWzdVV1fr3Xff1UsvvaQnn3zS7bjl5eV67bXX9Le//a3eOXNzc7Vu3TqNGDFCkZGRys3N1axZs3TdddepXbt2rf+hYZrUpEjtKKvU5qLDGtaDphsAAADwLFPD16RJk7Rv3z7NmTNHxcXFGjBggFatWuVqwrF7925ZrT9dnKusrNSNN96ovXv3KjQ0VL169dLLL7+sSZMmuR132bJlMgxD11xzTb1z2mw2LVu2THPnzlV1dbW6dOmiWbNmud3PBd+UlhSldzcWc+ULAAAApjC94UZmZqYyMzMbXLd27Vq39/fcc4/uueeekx5z5syZmjlzZoPrBg4cqC+++OKU64T3S+t4vDHK5sKm3RAJAAAAtCRTH7IMeFJq0vHw9d2+IzpmrzO5GgAAAPgbwhf8RmJUiNqFBanOYei70iNmlwMAAAA/Q/iC37BYLEw9BAAAgGkIX/AraT9OPdxcRPgCAACAZxG+4Fec931x5QsAAACeRviCX3FOO9xSVC7DMEyuBgAAAP6E8AW/0q1DhIIDrKqortXeg0fNLgcAAAB+hPAFvxIUYFWPhAhJ0iamHgIAAMCDCF/wO86mG1tougEAAAAPInzB77jazRO+AAAA4EGEL/idNDoeAgAAwASEL/idXj+Grx8OHdXhKrvJ1QAAAMBfEL7gd6JDg3RGu1BJ0pZirn4BAADAMwhf8EtMPQQAAICnEb7gl1KTaLoBAAAAzyJ8wS85Ox7Sbh4AAACeEmh2AYAZnNMOvy2p0Hsbi2SxmFyQF6mtrdPX+y0K2FSiwMAAs8uBH2DMwdMYc/Akxtvp6ZEQqW4dIswuo8kIX/BLZ7QLVVRIoMqP1eqPr+SZXY4XCtCSb782uwj4FcYcPI0xB09ivDXXn8b21I3Du5tdRpMRvuCXLBaL/jw+VW/k7ZVhmF2NdzEMQwcOHlRsu3aycMkQHsCYg6cx5uBJjLfTkxQdYnYJp4TwBb919ZAzdfWQM80uw+vY7Xa9++67Gj9+iIKCgswuB36AMQdPY8zBkxhv/oWGGwAAAADgAYQvAAAAAPAAwhcAAAAAeADhCwAAAAA8gPAFAAAAAB5A+AIAAAAADyB8AQAAAIAHEL4AAAAAwAMIXwAAAADgAYQvAAAAAPAAwhcAAAAAeADhCwAAAAA8gPAFAAAAAB5A+AIAAAAADyB8AQAAAIAHEL4AAAAAwAMIXwAAAADgAYQvAAAAAPCAQLML8FaGYUiSysvLTa5EstvtqqqqUnl5uYKCgswuBz6O8QZPY8zB0xhz8CTGm29wZgJnRmgM4auZKioqJEnJyckmVwIAAACgLaioqFB0dHSj6y3GyeIZGuRwOFRYWKjIyEhZLBZTaykvL1dycrL27NmjqKgoU2uB72O8wdMYc/A0xhw8ifHmGwzDUEVFhTp27CirtfE7u7jy1UxWq1VnnHGG2WW4iYqK4i8tPIbxBk9jzMHTGHPwJMab9zvRFS8nGm4AAAAAgAcQvgAAAADAAwhfPsBms+nuu++WzWYzuxT4AcYbPI0xB09jzMGTGG/+hYYbAAAAAOABXPkCAAAAAA8gfAEAAACABxC+AAAAAMADCF8AAAAA4AGELy/3xBNPKCUlRSEhIUpPT9f69evNLgk+Kjs7W+ecc44iIyMVHx+vCRMmqKCgwOyy4Cfuv/9+WSwW3XbbbWaXAh/2ww8/6LrrrlP79u0VGhqqvn376j//+Y/ZZcFH1dXV6a677lKXLl0UGhqqbt26acGCBaIXnm8jfHmx5cuXKysrS3fffbfy8vLUv39/jRkzRqWlpWaXBh/00Ucf6aabbtIXX3yhnJwc2e12jR49WpWVlWaXBh/35Zdf6qmnnlK/fv3MLgU+7ODBgzr//PMVFBSk9957T5s3b9bf/vY3tWvXzuzS4KMeeOABPfnkk3r88ce1ZcsWPfDAA3rwwQf12GOPmV0aWhGt5r1Yenq6zjnnHD3++OOSJIfDoeTkZN1888268847Ta4Ovm7fvn2Kj4/XRx99pAsvvNDscuCjjhw5ooEDB+rvf/+77rnnHg0YMECLFi0yuyz4oDvvvFOfffaZPvnkE7NLgZ/49a9/rYSEBD333HOuZRMnTlRoaKhefvllEytDa+LKl5eqqanRhg0blJGR4VpmtVqVkZGh3NxcEyuDvzh8+LAkKTY21uRK4MtuuukmXXzxxW7/XQe0hhUrVmjw4MH6zW9+o/j4eJ199tl65plnzC4LPuy8887TmjVr9O2330qSvv76a3366acaN26cyZWhNQWaXQCap6ysTHV1dUpISHBbnpCQoK1bt5pUFfyFw+HQbbfdpvPPP199+vQxuxz4qGXLlikvL09ffvml2aXAD3z//fd68sknlZWVpT//+c/68ssvdcsttyg4OFhTp041uzz4oDvvvFPl5eXq1auXAgICVFdXp3vvvVeTJ082uzS0IsIXgFN200036ZtvvtGnn35qdinwUXv27NGtt96qnJwchYSEmF0O/IDD4dDgwYN13333SZLOPvtsffPNN1q8eDHhC63in//8p1555RW9+uqr6t27t/Lz83XbbbepY8eOjDkfRvjyUnFxcQoICFBJSYnb8pKSEiUmJppUFfxBZmamVq5cqY8//lhnnHGG2eXAR23YsEGlpaUaOHCga1ldXZ0+/vhjPf7446qurlZAQICJFcLXJCUlKS0tzW1Zamqq3njjDZMqgq/73//9X9155526+uqrJUl9+/bVrl27lJ2dTfjyYdzz5aWCg4M1aNAgrVmzxrXM4XBozZo1Gjp0qImVwVcZhqHMzEy99dZb+vDDD9WlSxezS4IPGzlypDZu3Kj8/HzXa/DgwZo8ebLy8/MJXmhx559/fr3HZ3z77bfq3LmzSRXB11VVVclqdf+neEBAgBwOh0kVwRO48uXFsrKyNHXqVA0ePFhDhgzRokWLVFlZqenTp5tdGnzQTTfdpFdffVXvvPOOIiMjVVxcLEmKjo5WaGioydXB10RGRta7nzA8PFzt27fnPkO0ilmzZum8887Tfffdp6uuukrr16/X008/raefftrs0uCjLrnkEt17770688wz1bt3b3311VdauHChfvvb35pdGloRrea93OOPP66HHnpIxcXFGjBggB599FGlp6ebXRZ8kMViaXD5888/r2nTpnm2GPil4cOH02oerWrlypWaPXu2tm3bpi5duigrK0szZswwuyz4qIqKCt1111166623VFpaqo4dO+qaa67RnDlzFBwcbHZ5aCWELwAAAADwAO75AgAAAAAPIHwBAAAAgAcQvgAAAADAAwhfAAAAAOABhC8AAAAA8ADCFwAAAAB4AOELAAAAADyA8AUAAAAAHkD4AgDAAywWi95++22zywAAmIjwBQDwedOmTZPFYqn3Gjt2rNmlAQD8SKDZBQAA4Aljx47V888/77bMZrOZVA0AwB9x5QsA4BdsNpsSExPdXu3atZN0fErgk08+qXHjxik0NFRdu3bV66+/7rb/xo0b9atf/UqhoaFq3769Zs6cqSNHjrhts2TJEvXu3Vs2m01JSUnKzMx0W19WVqbLL79cYWFh6tGjh1asWOFad/DgQU2ePFkdOnRQaGioevToUS8sAgC8G+ELAABJd911lyZOnKivv/5akydP1tVXX60tW7ZIkiorKzVmzBi1a9dOX375pV577TV98MEHbuHqySef1E033aSZM2dq48aNWrFihbp37+52jnnz5umqq67Sf//7X40fP16TJ0/WgQMHXOffvHmz3nvvPW3ZskVPPvmk4uLiPPcFAABancUwDMPsIgAAaE3Tpk3Tyy+/rJCQELflf/7zn/XnP/9ZFotFf/jDH/Tkk0+61p177rkaOHCg/v73v+uZZ57RHXfcoT179ig8PFyS9O677+qSSy5RYWGhEhIS1KlTJ02fPl333HNPgzVYLBb99a9/1YIFCyQdD3QRERF67733NHbsWF166aWKi4vTkiVLWulbAACYjXu+AAB+YcSIEW7hSpJiY2Ndfx46dKjbuqFDhyo/P1+StGXLFvXv398VvCTp/PPPl8PhUEFBgSwWiwoLCzVy5MgT1tCvXz/Xn8PDwxUVFaXS0lJJ0h//+EdNnDhReXl5Gj16tCZMmKDzzjuvWZ8VANA2Eb4AAH4hPDy83jTAlhIaGtqk7YKCgtzeWywWORwOSdK4ceO0a9cuvfvuu8rJydHIkSN100036eGHH27xegEA5uCeLwAAJH3xxRf13qempkqSUlNT9fXXX6uystK1/rPPPpPValXPnj0VGRmplJQUrVmz5rRq6NChg6ZOnaqXX35ZixYt0tNPP31axwMAtC1c+QIA+IXq6moVFxe7LQsMDHQ1tXjttdc0ePBgDRs2TK+88orWr1+v5557TpI0efJk3X333Zo6darmzp2rffv26eabb9b111+vhIQESdLcuXP1hz/8QfHx8Ro3bpwqKir02Wef6eabb25SfXPmzNGgQYPUu3dvVVdXa+XKla7wBwDwDYQvAIBfWLVqlZKSktyW9ezZU1u3bpV0vBPhsmXLdOONNyopKUn/+Mc/lJaWJkkKCwvT+++/r1tvvVXnnHOOwsLCNHHiRC1cuNB1rKlTp+rYsWN65JFHdPvttysuLk5XXnllk+sLDg7W7NmztXPnToWGhuqCCy7QsmXLWuCTAwDaCrodAgD8nsVi0VtvvaUJEyaYXQoAwIdxzxcAAAAAeADhCwAAAAA8gHu+AAB+jxn4AABP4MoXAAAAAHgA4QsAAAAAPIDwBQAAAAAeQPgCAAAAAA8gfAEAAACABxC+AAAAAMADCF8AAAAA4AGELwAAAADwgP8PN5Gj3RhGd7oAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_folder: /content/DATA/run_2/original_result\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m70\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │              \u001b[38;5;34m55\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m60\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m22\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.0312 - loss: 1.0804\n",
            "Epoch 1 ended\n",
            "Loss: 0.6257343292236328, Accuracy: 0.6899999976158142\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 13.582344    13.1249695  -12.564508    13.075167   -12.795002\n",
            "  -12.471875   -12.563851   -13.058534    -0.4028291  -12.789138  ]\n",
            " [ 13.140699   -13.430489   -13.278526    13.144651   -12.913588\n",
            "  -13.438333   -12.808283   -12.530954     0.4540499  -13.224096  ]\n",
            " [ 13.152882   -13.15611    -13.638088    13.256984   -13.525627\n",
            "  -13.459259   -12.513444   -13.474453    -0.35097772 -13.385576  ]\n",
            " [ 12.736806   -12.447146   -12.457628    12.756016   -13.297037\n",
            "  -13.601122    -9.915981   -12.848862    -0.07981962 -12.69486   ]\n",
            " [ 13.209595   -13.2288     -12.511427    13.587853   -12.638331\n",
            "  -13.098412   -11.752766   -12.584299    -0.32354778 -13.366313  ]\n",
            " [ 13.205846   -12.454629   -13.163985    13.520716   -12.757662\n",
            "  -12.869786   -11.3333235  -13.458243    -0.5438744  -13.270529  ]]\n",
            "Layer 1 Biases: [ 13.05998   -13.030859  -13.060477   13.059851  -13.057842  -13.0578985\n",
            " -12.9401    -13.059214    0.        -13.055254 ]\n",
            "Layer 2 Weights: [[-13.470468   -13.49378     -0.5119738   -0.50602865 -13.010895  ]\n",
            " [-13.069889   -13.212474    -0.35789624  -0.36685583  12.438947  ]\n",
            " [-12.443462   -13.0156555    0.27051258   0.30062366 -13.126707  ]\n",
            " [-13.425658   -12.715467    -0.34188363  -0.50979066  13.534279  ]\n",
            " [-12.8289795  -12.327206    -0.25147334  -0.09239048 -13.366635  ]\n",
            " [-12.555784    -0.06907547  -0.4242979   -0.05204761 -12.651364  ]\n",
            " [  0.54710466   0.22579998   0.13112038   0.04159611  12.60932   ]\n",
            " [-12.780564   -13.161554    -0.07956207  -0.58052915 -13.09697   ]\n",
            " [ -0.2057423   -0.60468453  -0.5911544    0.0286451    0.11136168]\n",
            " [-12.837409    -0.07878041   0.3398041   -0.60976315  -0.6184985 ]]\n",
            "Layer 2 Biases: [-13.060827 -13.030711   0.         0.        13.050522]\n",
            "Layer 3 Weights: [[-1.53810054e-01 -2.52535790e-01 -1.30321674e+01 -4.79297936e-01\n",
            "  -6.16096854e-01 -1.24596996e+01 -5.23751616e-01 -1.29209385e+01\n",
            "  -5.24579287e-01 -1.29060984e+01]\n",
            " [-1.24291515e+01 -2.22610772e-01 -3.61044258e-01 -5.74008584e-01\n",
            "   1.33256569e+01 -1.22823305e+01 -1.23031025e+01  1.18561220e+01\n",
            "  -5.46575963e-01  1.14727402e+01]\n",
            " [-3.68319094e-01 -3.33041221e-01 -3.23325396e-02 -3.75470251e-01\n",
            "  -7.33886361e-02 -3.72625917e-01 -4.30943191e-01  4.91328180e-01\n",
            "   1.25097632e-02 -5.55414796e-01]\n",
            " [-2.92057395e-02  1.93380892e-01 -9.63301659e-02 -1.49916679e-01\n",
            "   1.05350018e-01  4.16030467e-01  2.22784042e-01  3.62620950e-02\n",
            "  -4.35333252e-01  1.01056039e-01]\n",
            " [-7.06276131e+00 -4.22010720e-01 -1.27556086e+01  1.35429821e+01\n",
            "   1.36533880e+01 -1.26027660e+01 -1.25561380e+01 -1.30410023e+01\n",
            "  -3.81061524e-01 -1.26303549e+01]]\n",
            "Layer 3 Biases: [-13.039699   0.       -13.057251  13.06093   13.060946 -13.061122\n",
            " -13.06094  -13.059426   0.       -13.060941]\n",
            "Layer 4 Weights: [[-11.184511   -11.004191  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-13.484137   -12.813608  ]\n",
            " [-13.407771   -13.747181  ]\n",
            " [-13.495701   -13.707995  ]\n",
            " [-12.969984   -12.590184  ]\n",
            " [-12.499551   -12.460873  ]\n",
            " [-13.489258   -12.795589  ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-13.729958   -12.523365  ]]\n",
            "Layer 4 Biases: [-13.061035 -13.061249]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5208 - loss: 0.7449\n",
            "Epoch 2/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 2 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 19.459143    18.984049   -18.441626    18.95168    -18.67009\n",
            "  -18.347797   -18.205109   -18.934778    -0.4028291  -18.663067  ]\n",
            " [ 19.01725    -19.286186   -19.155458    19.021074   -18.788656\n",
            "  -19.312551   -18.584188   -18.406816     0.4540499  -19.09645   ]\n",
            " [ 19.028908   -19.02866    -19.514763    19.133442   -19.398708\n",
            "  -19.322124   -18.324503   -19.349443    -0.35097772 -19.256134  ]\n",
            " [ 18.613558   -18.297922   -18.334774    18.63272    -19.172327\n",
            "  -19.470102   -14.15279    -18.725163    -0.07981962 -18.569267  ]\n",
            " [ 19.085606   -19.086597   -18.38808     19.4637     -18.511856\n",
            "  -18.972727   -16.816195   -18.45944     -0.32354778 -19.227442  ]\n",
            " [ 19.082039   -18.317217   -19.040705    19.396866   -18.632357\n",
            "  -18.743683   -16.157965   -19.333532    -0.5438744  -19.139832  ]]\n",
            "Layer 1 Biases: [ 18.937098 -18.894844 -18.937819  18.93691  -18.933996 -18.934076\n",
            " -18.763157 -18.935986   0.       -18.930243]\n",
            "Layer 2 Weights: [[-19.347433   -19.291508    -0.5119738   -0.50602865 -18.869543  ]\n",
            " [-18.946938   -19.041878    -0.35789624  -0.36685583  18.291498  ]\n",
            " [-18.320766   -18.779593     0.27051258   0.30062366 -19.000067  ]\n",
            " [-19.302608   -18.565968    -0.34188363  -0.50979066  19.403913  ]\n",
            " [-18.70561    -18.145187    -0.25147334  -0.09239048 -19.239857  ]\n",
            " [-18.428452    -0.06907547  -0.4242979   -0.05204761 -18.522163  ]\n",
            " [  0.54710466   0.22579998   0.13112038   0.04159611  18.423723  ]\n",
            " [-18.657372   -18.877445    -0.07956207  -0.58052915 -18.959946  ]\n",
            " [ -0.2057423   -0.60468453  -0.5911544    0.0286451    0.11136168]\n",
            " [-18.710829    -0.07878041   0.3398041   -0.60976315  -0.6184985 ]]\n",
            "Layer 2 Biases: [-18.938328 -18.89463    0.         0.        18.923376]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -1.8859667e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -1.8336754e+01 -5.2375162e-01 -1.8795561e+01\n",
            "  -5.2457929e-01 -1.8782726e+01]\n",
            " [-1.8083229e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   1.9102180e+01 -1.7964840e+01 -1.8083858e+01  1.7181620e+01\n",
            "  -5.4657596e-01  1.6637213e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.0132111e+01 -4.2201072e-01 -1.8619223e+01  1.9419367e+01\n",
            "   1.9529758e+01 -1.8478512e+01 -1.8432484e+01 -1.8841373e+01\n",
            "  -3.8106152e-01 -1.8504520e+01]]\n",
            "Layer 3 Biases: [-18.907671   0.       -18.933138  18.938478  18.9385   -18.938755\n",
            " -18.93849  -18.936295   0.       -18.93849 ]\n",
            "Layer 4 Weights: [[-16.284119   -16.105297  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-19.351568   -18.68743   ]\n",
            " [-19.279438   -19.622595  ]\n",
            " [-19.368603   -19.58386   ]\n",
            " [-18.8464     -18.467472  ]\n",
            " [-18.371597   -18.336393  ]\n",
            " [-19.35893    -18.670628  ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-19.60443    -18.399948  ]]\n",
            "Layer 4 Biases: [-18.938631 -18.93894 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.3968 \n",
            "Epoch 3/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.4375\n",
            "Epoch 3 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 23.061651    22.575676   -22.044329    22.554014   -22.27155\n",
            "  -21.949768   -21.66295    -22.536947    -0.4028291  -22.263815  ]\n",
            " [ 22.619608   -22.875736   -22.75805     22.623352   -22.390102\n",
            "  -22.913477   -22.12473    -22.00875      0.4540499  -22.696232  ]\n",
            " [ 22.630943   -22.628561   -23.117197    22.73574    -22.998936\n",
            "  -22.916075   -21.886633   -22.950844    -0.35097772 -22.85481   ]\n",
            " [ 22.21604    -21.884449   -21.937496    22.235172   -22.773909\n",
            "  -23.067812   -16.748516   -22.327368    -0.07981962 -22.17031   ]\n",
            " [ 22.687632   -22.677435   -21.990501    23.065624   -22.112354\n",
            "  -22.573711   -19.919237   -22.060932    -0.32354778 -22.820326  ]\n",
            " [ 22.684177   -21.910997   -22.643164    22.998976   -22.233574\n",
            "  -22.344412   -19.11443    -22.935114    -0.5438744  -22.737736  ]]\n",
            "Layer 1 Biases: [ 22.539806 -22.489485 -22.540663  22.539581 -22.536112 -22.536207\n",
            " -22.332659 -22.53848    0.       -22.531641]\n",
            "Layer 2 Weights: [[-22.950045   -22.845451    -0.5119738   -0.50602865 -22.460905  ]\n",
            " [-22.5496     -22.615278    -0.35789624  -0.36685583  21.879112  ]\n",
            " [-21.923588   -22.312778     0.27051258   0.30062366 -22.600464  ]\n",
            " [-22.905212   -22.152325    -0.34188363  -0.50979066  23.002026  ]\n",
            " [-22.308018   -21.71157     -0.25147334  -0.09239048 -22.84017   ]\n",
            " [-22.028423    -0.06907547  -0.4242979   -0.05204761 -22.120989  ]\n",
            " [  0.54710466   0.22579998   0.13112038   0.04159611  21.98791   ]\n",
            " [-22.259886   -22.381124    -0.07956207  -0.58052915 -22.553967  ]\n",
            " [ -0.2057423   -0.60468453  -0.5911544    0.0286451    0.11136168]\n",
            " [-22.311262    -0.07878041   0.3398041   -0.60976315  -0.6184985 ]]\n",
            "Layer 2 Biases: [-22.541267 -22.48923    0.         0.        22.523462]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -2.2431894e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -2.1939423e+01 -5.2375162e-01 -2.2396734e+01\n",
            "  -5.2457929e-01 -2.2385132e+01]\n",
            " [-2.1548944e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   2.2643099e+01 -2.1448015e+01 -2.1627378e+01  2.0445560e+01\n",
            "  -5.4657596e-01  1.9802288e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.2011826e+01 -4.2201072e-01 -2.2213636e+01  2.3021622e+01\n",
            "   2.3132006e+01 -2.2080374e+01 -2.2034716e+01 -2.2396938e+01\n",
            "  -3.8106152e-01 -2.2105413e+01]]\n",
            "Layer 3 Biases: [-22.50476    0.       -22.535088  22.541449  22.541473 -22.541775\n",
            " -22.54146  -22.538849   0.       -22.54146 ]\n",
            "Layer 4 Weights: [[-19.409372   -19.231468  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-22.948324   -22.288113  ]\n",
            " [-22.878796   -23.224257  ]\n",
            " [-22.96872    -23.185795  ]\n",
            " [-22.448675   -22.070282  ]\n",
            " [-21.971188   -21.938118  ]\n",
            " [-22.957064   -22.272055  ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-23.205511   -22.002325  ]]\n",
            "Layer 4 Biases: [-22.541632 -22.541998]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.4135 \n",
            "Epoch 4/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 4 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 25.431772    24.93862    -24.414577    24.92402    -24.640976\n",
            "  -24.319534   -23.93771    -24.906841    -0.4028291  -24.632772  ]\n",
            " [ 24.989626   -25.237312   -25.128225    24.993319   -24.759521\n",
            "  -25.28255    -24.453999   -24.378492     0.4540499  -25.064552  ]\n",
            " [ 25.00075    -24.99696    -25.487268    25.105722   -25.36755\n",
            "  -25.280552   -24.230135   -25.320232    -0.35097772 -25.222404  ]\n",
            " [ 24.58614    -24.244034   -24.307755    24.605255   -25.143417\n",
            "  -25.434767   -18.455305   -24.697287    -0.07981962 -24.539461  ]\n",
            " [ 25.057432   -25.03986    -24.360563    25.435358   -24.48115\n",
            "  -24.942825   -21.960186   -24.430382    -0.32354778 -25.184101  ]\n",
            " [ 25.054052   -24.275362   -25.01325     25.368832   -24.60284\n",
            "  -24.713356   -21.058811   -25.304623    -0.5438744  -25.104822  ]]\n",
            "Layer 1 Biases: [ 24.910057 -24.854416 -24.911001  24.909805 -24.905972 -24.906075\n",
            " -24.681019 -24.90859    0.       -24.901026]\n",
            "Layer 2 Weights: [[-25.32023    -25.183556    -0.5119738   -0.50602865 -24.823677  ]\n",
            " [-24.91982    -24.966208    -0.35789624  -0.36685583  24.239414  ]\n",
            " [-24.293913   -24.637201     0.27051258   0.30062366 -24.969189  ]\n",
            " [-25.275394   -24.511797    -0.34188363  -0.50979066  25.369246  ]\n",
            " [-24.67807    -24.057875    -0.25147334  -0.09239048 -25.208841  ]\n",
            " [-24.396872    -0.06907547  -0.4242979   -0.05204761 -24.488682  ]\n",
            " [  0.54710466   0.22579998   0.13112038   0.04159611  24.332766  ]\n",
            " [-24.630009   -24.686096    -0.07956207  -0.58052915 -24.91849   ]\n",
            " [ -0.2057423   -0.60468453  -0.5911544    0.0286451    0.11136168]\n",
            " [-24.680014    -0.07878041   0.3398041   -0.60976315  -0.6184985 ]]\n",
            "Layer 2 Biases: [-24.91167  -24.854134   0.         0.        24.891983]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -2.4782051e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -2.4309647e+01 -5.2375162e-01 -2.4765974e+01\n",
            "  -5.2457929e-01 -2.4755182e+01]\n",
            " [-2.3828892e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   2.4972616e+01 -2.3739471e+01 -2.3958612e+01  2.2592527e+01\n",
            "  -5.4657596e-01  2.1884109e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.3247318e+01 -4.2201072e-01 -2.4578419e+01  2.5391575e+01\n",
            "   2.5501951e+01 -2.4450069e+01 -2.4404652e+01 -2.4736113e+01\n",
            "  -3.8106152e-01 -2.4474468e+01]]\n",
            "Layer 3 Biases: [-24.871305   0.       -24.904837  24.911871  24.9119   -24.912231\n",
            " -24.911884 -24.908997   0.       -24.911886]\n",
            "Layer 4 Weights: [[-21.464956   -21.287655  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-25.314651   -24.657028  ]\n",
            " [-25.246838   -25.593815  ]\n",
            " [-25.33726    -25.555536  ]\n",
            " [-24.818642   -24.4406    ]\n",
            " [-24.339384   -24.30772   ]\n",
            " [-25.3243     -24.64146   ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-25.574688   -24.372358  ]]\n",
            "Layer 4 Biases: [-24.912075 -24.91248 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3984 \n",
            "Epoch 5/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.4375\n",
            "Epoch 5 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 27.035273    26.537258   -26.018166    26.527443   -26.244007\n",
            "  -25.922796   -25.476568   -26.510191    -0.4028291  -26.235487  ]\n",
            " [ 26.593061   -26.835022   -26.731766    26.596716   -26.36255\n",
            "  -26.885344   -26.02981    -25.981735     0.4540499  -26.666832  ]\n",
            " [ 26.604038   -26.599295   -27.090736    26.709131   -26.970032\n",
            "  -26.88023    -25.81559    -26.923237    -0.35097772 -26.824192  ]\n",
            " [ 26.18963    -25.840395   -25.91135     26.20873    -26.746506\n",
            "  -27.036121   -19.609385   -26.300653    -0.07981962 -26.142307  ]\n",
            " [ 26.660717   -26.638147   -25.964025    27.0386     -26.083754\n",
            "  -26.545643   -23.340601   -26.03343     -0.32354778 -26.783302  ]\n",
            " [ 26.657387   -25.87496    -26.61673     26.972155   -26.205765\n",
            "  -26.316061   -22.373806   -26.907711    -0.5438744  -26.706266  ]]\n",
            "Layer 1 Biases: [ 26.513645 -26.454401 -26.514652  26.513378 -26.509295 -26.509405\n",
            " -26.26977  -26.512083   0.       -26.50403 ]\n",
            "Layer 2 Weights: [[-26.923779   -26.765354    -0.5119738   -0.50602865 -26.422197  ]\n",
            " [-26.523392   -26.556702    -0.35789624  -0.36685583  25.83626   ]\n",
            " [-25.897552   -26.209726     0.27051258   0.30062366 -26.571747  ]\n",
            " [-26.878937   -26.108078    -0.34188363  -0.50979066  26.970781  ]\n",
            " [-26.281525   -25.645231    -0.25147334  -0.09239048 -26.81136   ]\n",
            " [-25.999239    -0.06907547  -0.4242979   -0.05204761 -26.090538  ]\n",
            " [  0.54710466   0.22579998   0.13112038   0.04159611  25.91914   ]\n",
            " [-26.233511   -26.245434    -0.07956207  -0.58052915 -26.518196  ]\n",
            " [ -0.2057423   -0.60468453  -0.5911544    0.0286451    0.11136168]\n",
            " [-26.282587    -0.07878041   0.3398041   -0.60976315  -0.6184985 ]]\n",
            "Layer 2 Biases: [-26.515366 -26.4541     0.         0.        26.494404]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -2.6372021e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -2.5913218e+01 -5.2375162e-01 -2.6368879e+01\n",
            "  -5.2457929e-01 -2.6358637e+01]\n",
            " [-2.5371269e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   2.6548595e+01 -2.5289650e+01 -2.5535753e+01  2.4044781e+01\n",
            "  -5.4657596e-01  2.3292219e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.4082389e+01 -4.2201072e-01 -2.6178303e+01  2.6994963e+01\n",
            "   2.7105333e+01 -2.6053284e+01 -2.6008028e+01 -2.6318636e+01\n",
            "  -3.8106152e-01 -2.6077248e+01]]\n",
            "Layer 3 Biases: [-26.472383   0.       -26.508087  26.515577  26.51561  -26.51596\n",
            " -26.515594 -26.512516   0.       -26.515596]\n",
            "Layer 4 Weights: [[-22.855288   -22.678396  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-26.915583   -26.259714  ]\n",
            " [-26.84893    -27.196938  ]\n",
            " [-26.93969    -27.15878   ]\n",
            " [-26.422037   -26.044235  ]\n",
            " [-25.941582   -25.910872  ]\n",
            " [-26.925846   -26.244478  ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-27.177551   -25.975801  ]]\n",
            "Layer 4 Biases: [-26.515795 -26.516228]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.4140 \n",
            "Epoch 6/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 6 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 28.133408    27.632057   -27.116362    27.625523   -27.341822\n",
            "  -27.02077    -26.530352   -27.608223    -0.4028291  -27.333082  ]\n",
            " [ 27.691149   -27.929184   -27.829926    27.69478    -27.460361\n",
            "  -27.982994   -27.108944   -27.079695     0.4540499  -27.76413   ]\n",
            " [ 27.70203    -27.696632   -28.18885     27.807203   -28.067467\n",
            "  -27.975742   -26.901346   -28.021032    -0.35097772 -27.921152  ]\n",
            " [ 27.287758   -26.933628   -27.009552    27.306849   -27.844358\n",
            "  -28.132786   -20.399296   -27.398695    -0.07981962 -27.239992  ]\n",
            " [ 27.758705   -27.732706   -27.062134    28.136559   -27.181273\n",
            "  -27.64331    -24.285696   -27.131254    -0.32354778 -27.878489  ]\n",
            " [ 27.755407   -26.970419   -27.714851    28.07017    -27.303505\n",
            "  -27.413649   -23.274038   -28.005564    -0.5438744  -27.80299   ]]\n",
            "Layer 1 Biases: [ 27.611843 -27.550125 -27.612892  27.611565 -27.60731  -27.607426\n",
            " -27.357784 -27.610216   0.       -27.601826]\n",
            "Layer 2 Weights: [[-28.021946   -27.848597    -0.5119738   -0.50602865 -27.516916  ]\n",
            " [-27.621576   -27.645912    -0.35789624  -0.36685583  26.929829  ]\n",
            " [-26.995785   -27.286608     0.27051258   0.30062366 -27.669233  ]\n",
            " [-27.977104   -27.201262    -0.34188363  -0.50979066  28.067568  ]\n",
            " [-27.37963    -26.73229     -0.25147334  -0.09239048 -27.908821  ]\n",
            " [-27.096598    -0.06907547  -0.4242979   -0.05204761 -27.187546  ]\n",
            " [  0.54710466   0.22579998   0.13112038   0.04159611  27.005524  ]\n",
            " [-27.331648   -27.313265    -0.07956207  -0.58052915 -27.613728  ]\n",
            " [ -0.2057423   -0.60468453  -0.5911544    0.0286451    0.11136168]\n",
            " [-27.380087    -0.07878041   0.3398041   -0.60976315  -0.6184985 ]]\n",
            "Layer 2 Biases: [-27.613634 -27.54981    0.         0.        27.591797]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -2.7460873e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -2.7011402e+01 -5.2375162e-01 -2.7466606e+01\n",
            "  -5.2457929e-01 -2.7456741e+01]\n",
            " [-2.6427465e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   2.7627846e+01 -2.6351198e+01 -2.6615803e+01  2.5039150e+01\n",
            "  -5.4657596e-01  2.4256308e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.4653727e+01 -4.2201072e-01 -2.7273956e+01  2.8093019e+01\n",
            "   2.8203388e+01 -2.7151222e+01 -2.7106079e+01 -2.7402378e+01\n",
            "  -3.8106152e-01 -2.7174887e+01]]\n",
            "Layer 3 Biases: [-27.568857   0.       -27.606052  27.613855  27.613888 -27.614254\n",
            " -27.613873 -27.610666   0.       -27.613874]\n",
            "Layer 4 Weights: [[-23.807184   -23.630575  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-28.011955   -27.357288  ]\n",
            " [-27.946102   -28.294813  ]\n",
            " [-28.037092   -28.256739  ]\n",
            " [-27.520102   -27.142464  ]\n",
            " [-27.038824   -27.008766  ]\n",
            " [-28.02264    -27.342285  ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-28.275251   -27.073896  ]]\n",
            "Layer 4 Biases: [-27.61408  -27.614532]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.4005 \n",
            "Epoch 7/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.3750\n",
            "Epoch 7 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 28.88916     28.385506   -27.872152    28.381237   -28.097351\n",
            "  -27.776407   -27.255518   -28.363903    -0.4028291  -28.088459  ]\n",
            " [ 28.446867   -28.682196   -28.585695    28.450483   -28.215887\n",
            "  -28.73841    -27.851591   -27.835325     0.4540499  -28.519304  ]\n",
            " [ 28.45768    -28.45183    -28.944586    28.56291    -28.822737\n",
            "  -28.729683   -27.64856    -28.77655     -0.35097772 -28.676094  ]\n",
            " [ 28.043503   -27.685999   -27.765348    28.062586   -28.599915\n",
            "  -28.887522   -20.942617   -28.154383    -0.07981962 -27.995434  ]\n",
            " [ 28.514355   -28.48599    -27.817865    28.892187   -27.9366\n",
            "  -28.39874    -24.935942   -27.88679     -0.32354778 -28.632204  ]\n",
            " [ 28.51108    -27.724323   -28.470594    28.825836   -28.058983\n",
            "  -28.169024   -23.893366   -28.76112     -0.5438744  -28.557768  ]]\n",
            "Layer 1 Biases: [ 28.367634 -28.304213 -28.368715  28.36735  -28.362976 -28.363098\n",
            " -28.106554 -28.365963   0.       -28.357342]\n",
            "Layer 2 Weights: [[-2.8777719e+01 -2.8594080e+01 -5.1197380e-01 -5.0602865e-01\n",
            "  -2.8270309e+01]\n",
            " [-2.8377359e+01 -2.8395508e+01 -3.5789624e-01 -3.6685583e-01\n",
            "   2.7682428e+01]\n",
            " [-2.7751602e+01 -2.8027700e+01  2.7051258e-01  3.0062366e-01\n",
            "  -2.8424538e+01]\n",
            " [-2.8732874e+01 -2.7953596e+01 -3.4188363e-01 -5.0979066e-01\n",
            "   2.8822390e+01]\n",
            " [-2.8135359e+01 -2.7480402e+01 -2.5147334e-01 -9.2390478e-02\n",
            "  -2.8664108e+01]\n",
            " [-2.7851812e+01 -6.9075465e-02 -4.2429790e-01 -5.2047610e-02\n",
            "  -2.7942516e+01]\n",
            " [ 5.4710466e-01  2.2579998e-01  1.3112038e-01  4.1596115e-02\n",
            "   2.7753170e+01]\n",
            " [-2.8087400e+01 -2.8048120e+01 -7.9562068e-02 -5.8052915e-01\n",
            "  -2.8367682e+01]\n",
            " [-2.0574230e-01 -6.0468453e-01 -5.9115440e-01  2.8645098e-02\n",
            "   1.1136168e-01]\n",
            " [-2.8135399e+01 -7.8780413e-02  3.3980411e-01 -6.0976315e-01\n",
            "  -6.1849850e-01]]\n",
            "Layer 2 Biases: [-28.369476 -28.30389    0.         0.        28.347036]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -2.8210220e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -2.7767187e+01 -5.2375162e-01 -2.8222076e+01\n",
            "  -5.2457929e-01 -2.8212471e+01]\n",
            " [-2.7154299e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   2.8370573e+01 -2.7081720e+01 -2.7359081e+01  2.5723356e+01\n",
            "  -5.4657596e-01  2.4919643e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.5046551e+01 -4.2201072e-01 -2.8027996e+01  2.8848717e+01\n",
            "   2.8959084e+01 -2.7906837e+01 -2.7861771e+01 -2.8148201e+01\n",
            "  -3.8106152e-01 -2.7930296e+01]]\n",
            "Layer 3 Biases: [-28.323462   0.       -28.361683  28.369705  28.369738 -28.370111\n",
            " -28.369722 -28.366428   0.       -28.369724]\n",
            "Layer 4 Weights: [[-24.462118   -24.285702  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-28.766489   -28.112654  ]\n",
            " [-28.701187   -29.050383  ]\n",
            " [-28.792336   -29.01237   ]\n",
            " [-28.275805   -27.89828   ]\n",
            " [-27.793959   -27.76435   ]\n",
            " [-28.777468   -28.097807  ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-29.030699   -27.82962   ]]\n",
            "Layer 4 Biases: [-28.369936 -28.3704  ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.4020 \n",
            "Epoch 8/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.4219\n",
            "Epoch 8 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 29.409918    28.904676   -28.39294     28.90197    -28.617956\n",
            "  -28.29709    -27.755163   -28.884611    -0.4028291  -28.608961  ]\n",
            " [ 28.967606   -29.201063   -29.106466    28.971209   -28.73649\n",
            "  -29.25894    -28.363304   -28.356003     0.4540499  -29.039667  ]\n",
            " [ 28.97837    -28.972208   -29.465336    29.08364    -29.343163\n",
            "  -29.249193   -28.163422   -29.297148    -0.35097772 -29.196297  ]\n",
            " [ 28.56426    -28.204426   -28.28614     28.583336   -29.120539\n",
            "  -29.407581   -21.31679    -28.675098    -0.07981962 -28.51598   ]\n",
            " [ 29.035044   -29.005047   -28.338612    29.412863   -28.457066\n",
            "  -28.91928    -25.383877   -28.4074      -0.32354778 -29.151558  ]\n",
            " [ 29.031786   -28.243807   -28.991344    29.346537   -28.579556\n",
            "  -28.689524   -24.319967   -29.281744    -0.5438744  -29.077856  ]]\n",
            "Layer 1 Biases: [ 28.888422 -28.823822 -28.889524  28.888132 -28.883677 -28.883802\n",
            " -28.622492 -28.88672    0.       -28.87794 ]\n",
            "Layer 2 Weights: [[-2.9298494e+01 -2.9107750e+01 -5.1197380e-01 -5.0602865e-01\n",
            "  -2.8789442e+01]\n",
            " [-2.8898142e+01 -2.8912018e+01 -3.5789624e-01 -3.6685583e-01\n",
            "   2.8201014e+01]\n",
            " [-2.8272406e+01 -2.8538340e+01  2.7051258e-01  3.0062366e-01\n",
            "  -2.8944988e+01]\n",
            " [-2.9253647e+01 -2.8471996e+01 -3.4188363e-01 -5.0979066e-01\n",
            "   2.9342506e+01]\n",
            " [-2.8656101e+01 -2.7995888e+01 -2.5147334e-01 -9.2390478e-02\n",
            "  -2.9184546e+01]\n",
            " [-2.8372202e+01 -6.9075465e-02 -4.2429790e-01 -5.2047610e-02\n",
            "  -2.8462738e+01]\n",
            " [ 5.4710466e-01  2.2579998e-01  1.3112038e-01  4.1596115e-02\n",
            "   2.8268333e+01]\n",
            " [-2.8608158e+01 -2.8554453e+01 -7.9562068e-02 -5.8052915e-01\n",
            "  -2.8887201e+01]\n",
            " [-2.0574230e-01 -6.0468453e-01 -5.9115440e-01  2.8645098e-02\n",
            "   1.1136168e-01]\n",
            " [-2.8655857e+01 -7.8780413e-02  3.3980411e-01 -6.0976315e-01\n",
            "  -6.1849850e-01]]\n",
            "Layer 2 Biases: [-28.890299 -28.823496   0.         0.        28.867441]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -2.8726559e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -2.8287970e+01 -5.2375162e-01 -2.8742641e+01\n",
            "  -5.2457929e-01 -2.8733213e+01]\n",
            " [-2.7655092e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   2.8882339e+01 -2.7585062e+01 -2.7871227e+01  2.6194727e+01\n",
            "  -5.4657596e-01  2.5376610e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.5316972e+01 -4.2201072e-01 -2.8547573e+01  2.9369440e+01\n",
            "   2.9479805e+01 -2.8427502e+01 -2.8382488e+01 -2.8662106e+01\n",
            "  -3.8106152e-01 -2.8450819e+01]]\n",
            "Layer 3 Biases: [-28.84343    0.       -28.88236   28.890532  28.890566 -28.890947\n",
            " -28.890549 -28.887194   0.       -28.89055 ]\n",
            "Layer 4 Weights: [[-24.913284   -24.737003  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-29.28641    -28.633146  ]\n",
            " [-29.221485   -29.571018  ]\n",
            " [-29.312746   -29.533047  ]\n",
            " [-28.79653    -28.419083  ]\n",
            " [-28.314291   -28.284996  ]\n",
            " [-29.29759    -28.61841   ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-29.551249   -28.35036   ]]\n",
            "Layer 4 Biases: [-28.890766 -28.891237]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.4119 \n",
            "Epoch 9/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.4062\n",
            "Epoch 9 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 29.768503    29.262163   -28.751545    29.260538   -28.976435\n",
            "  -28.655622   -28.09918    -29.243162    -0.4028291  -28.96737   ]\n",
            " [ 29.326174   -29.558342   -29.46506     29.32977    -29.094967\n",
            "  -29.617365   -28.715645   -28.714529     0.4540499  -29.397976  ]\n",
            " [ 29.336908   -29.330528   -29.823914    29.442204   -29.701517\n",
            "  -29.606913   -28.517939   -29.65562     -0.35097772 -29.554495  ]\n",
            " [ 28.922842   -28.561398   -28.644745    28.941914   -29.479029\n",
            "  -29.765682   -21.574293   -29.033653    -0.07981962 -28.874416  ]\n",
            " [ 29.393578   -29.362455   -28.697187    29.771387   -28.815447\n",
            "  -29.277712   -25.69223    -28.76588     -0.32354778 -29.509172  ]\n",
            " [ 29.390333   -28.601511   -29.349922    29.70508    -28.93801\n",
            "  -29.047928   -24.61361    -29.640234    -0.5438744  -29.435976  ]]\n",
            "Layer 1 Biases: [ 29.247026 -29.181614 -29.248144  29.246733 -29.242222 -29.24235\n",
            " -28.97775  -29.245304   0.       -29.236412]\n",
            "Layer 2 Weights: [[-2.9657087e+01 -2.9461443e+01 -5.1197380e-01 -5.0602865e-01\n",
            "  -2.9146904e+01]\n",
            " [-2.9256742e+01 -2.9267668e+01 -3.5789624e-01 -3.6685583e-01\n",
            "   2.8558098e+01]\n",
            " [-2.8631021e+01 -2.8889940e+01  2.7051258e-01  3.0062366e-01\n",
            "  -2.9303360e+01]\n",
            " [-2.9612240e+01 -2.8828953e+01 -3.4188363e-01 -5.0979066e-01\n",
            "   2.9700647e+01]\n",
            " [-2.9014675e+01 -2.8350832e+01 -2.5147334e-01 -9.2390478e-02\n",
            "  -2.9542910e+01]\n",
            " [-2.8730530e+01 -6.9075465e-02 -4.2429790e-01 -5.2047610e-02\n",
            "  -2.8820951e+01]\n",
            " [ 5.4710466e-01  2.2579998e-01  1.3112038e-01  4.1596115e-02\n",
            "   2.8623056e+01]\n",
            " [-2.8966743e+01 -2.8903084e+01 -7.9562068e-02 -5.8052915e-01\n",
            "  -2.9244930e+01]\n",
            " [-2.0574230e-01 -6.0468453e-01 -5.9115440e-01  2.8645098e-02\n",
            "   1.1136168e-01]\n",
            " [-2.9014231e+01 -7.8780413e-02  3.3980411e-01 -6.0976315e-01\n",
            "  -6.1849850e-01]]\n",
            "Layer 2 Biases: [-29.248926 -29.181284   0.         0.        29.22578 ]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -2.9082092e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -2.8646570e+01 -5.2375162e-01 -2.9101093e+01\n",
            "  -5.2457929e-01 -2.9091787e+01]\n",
            " [-2.7999903e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   2.9234722e+01 -2.7931629e+01 -2.8223867e+01  2.6519243e+01\n",
            "  -5.4657596e-01  2.5691193e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.5502999e+01 -4.2201072e-01 -2.8905340e+01  2.9727999e+01\n",
            "   2.9838362e+01 -2.8786022e+01 -2.8741043e+01 -2.9015961e+01\n",
            "  -3.8106152e-01 -2.8809238e+01]]\n",
            "Layer 3 Biases: [-29.201468   0.       -29.240889  29.249163  29.249197 -29.249582\n",
            " -29.24918  -29.24578    0.       -29.249182]\n",
            "Layer 4 Weights: [[-25.223867   -25.047678  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-29.644415   -28.991547  ]\n",
            " [-29.579752   -29.929516  ]\n",
            " [-29.671091   -29.891573  ]\n",
            " [-29.15509    -28.777697  ]\n",
            " [-28.672583   -28.643501  ]\n",
            " [-29.655735   -28.976885  ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-29.909689   -28.708931  ]]\n",
            "Layer 4 Biases: [-29.249401 -29.249876]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.4078 \n",
            "Epoch 10/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.4219\n",
            "Epoch 10 ended\n",
            "Loss: 0.4099999964237213, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 30.014997    29.507902   -28.998055    29.50702    -29.222857\n",
            "  -28.90208    -28.335644   -29.489634    -0.4028291  -29.21374   ]\n",
            " [ 29.572659   -29.803936   -29.711561    29.576248   -29.341389\n",
            "  -29.86375    -28.95784    -28.960985     0.4540499  -29.644281  ]\n",
            " [ 29.583372   -29.576843   -30.070404    29.688684   -29.947855\n",
            "  -29.852814   -28.761633   -29.90204     -0.35097772 -29.800724  ]\n",
            " [ 29.169334   -28.806784   -28.891254    29.188406   -29.725458\n",
            "  -30.011847   -21.751204   -29.280128    -0.07981962 -29.12081   ]\n",
            " [ 29.640041   -29.60814    -28.943676    30.017843   -29.061804\n",
            "  -29.524101   -25.904137   -29.012304    -0.32354778 -29.755001  ]\n",
            " [ 29.636803   -28.8474     -29.596415    29.951548   -29.184414\n",
            "  -29.294298   -24.81539    -29.886663    -0.5438744  -29.682152  ]]\n",
            "Layer 1 Biases: [ 29.493536 -29.427563 -29.494661  29.49324  -29.48869  -29.48882\n",
            " -29.221954 -29.491798   0.       -29.482832]\n",
            "Layer 2 Weights: [[-2.9903589e+01 -2.9704569e+01 -5.1197380e-01 -5.0602865e-01\n",
            "  -2.9392626e+01]\n",
            " [-2.9503250e+01 -2.9512144e+01 -3.5789624e-01 -3.6685583e-01\n",
            "   2.8803560e+01]\n",
            " [-2.8877539e+01 -2.9131628e+01  2.7051258e-01  3.0062366e-01\n",
            "  -2.9549709e+01]\n",
            " [-2.9858742e+01 -2.9074327e+01 -3.4188363e-01 -5.0979066e-01\n",
            "   2.9946836e+01]\n",
            " [-2.9261164e+01 -2.8594820e+01 -2.5147334e-01 -9.2390478e-02\n",
            "  -2.9789251e+01]\n",
            " [-2.8976847e+01 -6.9075465e-02 -4.2429790e-01 -5.2047610e-02\n",
            "  -2.9067188e+01]\n",
            " [ 5.4710466e-01  2.2579998e-01  1.3112038e-01  4.1596115e-02\n",
            "   2.8866892e+01]\n",
            " [-2.9213238e+01 -2.9142725e+01 -7.9562068e-02 -5.8052915e-01\n",
            "  -2.9490837e+01]\n",
            " [-2.0574230e-01 -6.0468453e-01 -5.9115440e-01  2.8645098e-02\n",
            "   1.1136168e-01]\n",
            " [-2.9260580e+01 -7.8780413e-02  3.3980411e-01 -6.0976315e-01\n",
            "  -6.1849850e-01]]\n",
            "Layer 2 Biases: [-29.495451 -29.427229   0.         0.        29.472107]\n",
            "Layer 3 Weights: [[-1.5381005e-01 -2.5253579e-01 -2.9326487e+01 -4.7929794e-01\n",
            "  -6.1609685e-01 -2.8893078e+01 -5.2375162e-01 -2.9347496e+01\n",
            "  -5.2457929e-01 -2.9338276e+01]\n",
            " [-2.8236912e+01 -2.2261077e-01 -3.6104426e-01 -5.7400858e-01\n",
            "   2.9476944e+01 -2.8169847e+01 -2.8466270e+01  2.6742275e+01\n",
            "  -5.4657596e-01  2.5907387e+01]\n",
            " [-3.6831909e-01 -3.3304122e-01 -3.2332540e-02 -3.7547025e-01\n",
            "  -7.3388636e-02 -3.7262592e-01 -4.3094319e-01  4.9132818e-01\n",
            "   1.2509763e-02 -5.5541480e-01]\n",
            " [-2.9205739e-02  1.9338089e-01 -9.6330166e-02 -1.4991668e-01\n",
            "   1.0535002e-01  4.1603047e-01  2.2278404e-01  3.6262095e-02\n",
            "  -4.3533325e-01  1.0105604e-01]\n",
            " [-1.5630753e+01 -4.2201072e-01 -2.9151272e+01  2.9974476e+01\n",
            "   3.0084839e+01 -2.9032473e+01 -2.8987518e+01 -2.9259197e+01\n",
            "  -3.8106152e-01 -2.9055622e+01]]\n",
            "Layer 3 Biases: [-29.447586   0.       -29.487345  29.49569   29.495726 -29.496113\n",
            " -29.495708 -29.492281   0.       -29.49571 ]\n",
            "Layer 4 Weights: [[-25.43731    -25.261183  ]\n",
            " [  0.5540903   -0.64048016]\n",
            " [-29.890512   -29.237915  ]\n",
            " [-29.826029   -30.175953  ]\n",
            " [-29.917421   -30.13803   ]\n",
            " [-29.401567   -29.024214  ]\n",
            " [-28.918873   -28.889942  ]\n",
            " [-29.901926   -29.223305  ]\n",
            " [  0.37658483  -0.07925004]\n",
            " [-30.156086   -28.955416  ]]\n",
            "Layer 4 Biases: [-29.49593  -29.496408]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.4145  \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_folder: /content/DATA/run_3/original_result\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m70\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │              \u001b[38;5;34m55\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m60\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m22\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.9039\n",
            "Epoch 1 ended\n",
            "Loss: 0.5692482590675354, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 13.38858     13.206427    12.871164     4.1032643    0.03243005\n",
            "  -12.70233    -12.491677   -12.680555    13.137987   -12.82991   ]\n",
            " [ 13.511041   -12.817309    13.478137     7.6282725   -0.5167843\n",
            "  -12.596244    12.94436    -13.00792     12.874924   -13.1092    ]\n",
            " [ 12.91344    -12.872906    12.775378     4.9924693   -0.2567465\n",
            "  -13.401436   -13.312763   -12.802374    12.368944   -12.557425  ]\n",
            " [ 13.572279   -12.551374    12.835852     5.1825757   -0.3987161\n",
            "  -12.981247    12.828688   -12.957668    12.461325   -13.073036  ]\n",
            " [ 13.397842    12.352515    13.659643     0.3754406    0.04104543\n",
            "  -13.016597    13.144924   -12.678867    13.546568   -13.179291  ]\n",
            " [ 13.260896    12.411084    12.511926     7.6412926    0.2891739\n",
            "  -13.505801    13.2936535  -13.317607    12.878802   -13.499483  ]]\n",
            "Layer 1 Biases: [ 13.05905   -12.8639145  13.056008    7.9831944   0.        -13.059522\n",
            "  13.039973  -13.058954   13.01491   -13.058418 ]\n",
            "Layer 2 Weights: [[-13.293671    -0.29899362 -13.583198    13.169725    -0.5840534 ]\n",
            " [-12.57983      0.23943388 -12.976443   -13.41358     -0.5159884 ]\n",
            " [-13.132133     0.17592013  -0.2751973   13.444107     0.07531619]\n",
            " [  0.09712768   0.43976778  -0.5891685   12.824496     0.30657846]\n",
            " [ -0.09349352  -0.02413458  -0.56639147  -0.16170612  -0.48174524]\n",
            " [-12.504627    -0.17820278   0.3860088   13.382377     0.25295633]\n",
            " [-12.895429    -0.03726542  -0.29645282  13.575491    -0.1604481 ]\n",
            " [-12.771047    -0.5462637  -12.598576    13.111999    -0.24321368]\n",
            " [-13.103572     0.49505     -0.09771681  13.002704     0.46208066]\n",
            " [-12.65741     -0.34243885 -12.417245   -13.346726     0.36034417]]\n",
            "Layer 2 Biases: [-13.060648   0.       -13.051789  13.059231   0.      ]\n",
            "Layer 3 Weights: [[-1.3317490e+01 -1.2758675e+01 -5.7225853e-01 -1.3220405e+01\n",
            "   1.3043758e+01 -2.1580580e-01  1.2645453e+01  1.2466763e+01\n",
            "  -1.3378852e+01  1.3426526e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -1.2736937e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  1.2505892e+01]\n",
            " [-1.2660052e+01 -1.2694116e+01 -3.3204389e-01 -1.2926131e+01\n",
            "   1.3161245e+01 -1.5086198e-01  1.3562905e+01  1.3534691e+01\n",
            "  -1.2428261e+01  1.3130340e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-13.060994 -13.061161   0.       -13.060873  13.061059   0.\n",
            "  13.061121  13.060997 -13.058754  13.060593]\n",
            "Layer 4 Weights: [[-13.109766   -12.60984   ]\n",
            " [-12.559028   -12.609746  ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-13.011692   -12.7133    ]\n",
            " [-12.773749   -13.6740265 ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-13.1466675  -13.686935  ]\n",
            " [-12.819376   -13.653318  ]\n",
            " [-12.570978   -13.230526  ]\n",
            " [-12.949188   -13.2934475 ]]\n",
            "Layer 4 Biases: [-13.061065 -13.061229]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.6539\n",
            "Epoch 2/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.3750\n",
            "Epoch 2 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 19.264275    19.044569    18.74203      6.0317044    0.03243005\n",
            "  -18.578873   -18.35681    -18.556414    18.983923   -18.704086  ]\n",
            " [ 19.38697    -18.449438    19.352787    10.983204    -0.5167843\n",
            "  -18.472803    18.807789   -18.883774    18.719402   -18.984594  ]\n",
            " [ 18.78841    -18.68808     18.640728     7.2762766   -0.2567465\n",
            "  -19.27673    -19.144629   -18.677523    18.138058   -18.432611  ]\n",
            " [ 19.447424   -18.353592    18.70721      7.523025    -0.3987161\n",
            "  -18.857111    18.677275   -18.832685    18.21399    -18.946348  ]\n",
            " [ 19.272713    18.03727     19.5332       0.734576     0.04104543\n",
            "  -18.892445    19.011509   -18.553623    19.385433   -19.052225  ]\n",
            " [ 19.136631    18.183054    18.385021    11.046504     0.2891739\n",
            "  -19.381855    19.1645     -19.193073    18.697563   -19.373158  ]]\n",
            "Layer 1 Biases: [ 18.93575   -18.65262    18.931335   11.5727825   0.        -18.936432\n",
            "  18.90807   -18.935612   18.871704  -18.93483  ]\n",
            "Layer 2 Weights: [[-19.171045    -0.29899362 -19.448786    19.046417    -0.5840534 ]\n",
            " [-18.418743     0.23943388 -18.814987   -19.211657    -0.5159884 ]\n",
            " [-19.006613     0.17592013  -0.2751973   19.315525     0.07531619]\n",
            " [  0.09712768   0.43976778  -0.5891685   18.594398     0.30657846]\n",
            " [ -0.09349352  -0.02413458  -0.56639147  -0.16170612  -0.48174524]\n",
            " [-18.38096     -0.17820278   0.3860088   19.253395     0.25295633]\n",
            " [-18.77205     -0.03726542  -0.29645282  19.449614    -0.1604481 ]\n",
            " [-18.647978    -0.5462637  -18.46021     18.984072    -0.24321368]\n",
            " [-18.976908     0.49505     -0.09771681  18.870047     0.46208066]\n",
            " [-18.530035    -0.34243885 -18.280731   -19.212563     0.36034417]]\n",
            "Layer 2 Biases: [-18.938068   0.       -18.925215  18.936012   0.      ]\n",
            "Layer 3 Weights: [[-1.9191711e+01 -1.8634865e+01 -5.7225853e-01 -1.9092243e+01\n",
            "   1.8918846e+01 -2.1580580e-01  1.8520805e+01  1.8340059e+01\n",
            "  -1.9238842e+01  1.9298193e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -1.8599998e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  1.8300018e+01]\n",
            " [-1.8537210e+01 -1.8571507e+01 -3.3204389e-01 -1.8803154e+01\n",
            "   1.9038486e+01 -1.5086198e-01  1.9440275e+01  1.9411892e+01\n",
            "  -1.8302158e+01  1.9006762e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-18.93857  -18.938814   0.       -18.938395  18.938663   0.\n",
            "  18.938753  18.938576 -18.93532   18.937986]\n",
            "Layer 4 Weights: [[-18.98604    -18.486885  ]\n",
            " [-18.43563    -18.48693   ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-18.884483   -18.588713  ]\n",
            " [-18.646297   -19.549282  ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-19.023201   -19.564108  ]\n",
            " [-18.695738   -19.530415  ]\n",
            " [-18.447796   -19.10783   ]\n",
            " [-18.823532   -19.169544  ]]\n",
            "Layer 4 Biases: [-18.938671 -18.93891 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3800 \n",
            "Epoch 3/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.3438\n",
            "Epoch 3 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 22.866106    22.623335    22.340895     7.2122474    0.03243005\n",
            "  -22.181225   -21.952152   -22.158346    22.567474   -22.304985  ]\n",
            " [ 22.988949   -21.901674    22.953978    13.038015    -0.5167843\n",
            "  -22.075165    22.402086   -22.485703    22.30206    -22.58624   ]\n",
            " [ 22.389797   -22.252737    22.236206     8.674537    -0.2567465\n",
            "  -22.878315   -22.71954    -22.27902     21.674427   -22.03413   ]\n",
            " [ 23.048916   -21.910294    22.30638      8.955992    -0.3987161\n",
            "  -22.459045    22.262457   -22.434101    21.740252   -22.546717  ]\n",
            " [ 22.87404     21.521826    23.133717     0.9543114    0.04104543\n",
            "  -22.494371    22.607746   -22.154879    22.964642   -22.652363  ]\n",
            " [ 22.73849     21.721176    21.985256    13.132146     0.2891739\n",
            "  -22.983906    22.763351   -22.794765    22.264425   -22.97375   ]]\n",
            "Layer 1 Biases: [ 22.538198 -22.201021  22.532942  13.771491   0.       -22.539011\n",
            "  22.505234 -22.538034  22.461927 -22.537104]\n",
            "Layer 2 Weights: [[-22.77391     -0.29899362 -23.04441     22.648859    -0.5840534 ]\n",
            " [-21.997984     0.23943388 -22.394001   -22.765812    -0.5159884 ]\n",
            " [-22.607698     0.17592013  -0.2751973   22.91473      0.07531619]\n",
            " [  0.09712768   0.43976778  -0.5891685   22.13125      0.30657846]\n",
            " [ -0.09349352  -0.02413458  -0.56639147  -0.16170612  -0.48174524]\n",
            " [-21.983185    -0.17820278   0.3860088   22.852354     0.25295633]\n",
            " [-22.374453    -0.03726542  -0.29645282  23.05048     -0.1604481 ]\n",
            " [-22.25057     -0.5462637  -22.053408    22.58368     -0.24321368]\n",
            " [-22.57729      0.49505     -0.09771681  22.466747     0.46208066]\n",
            " [-22.12998     -0.34243885 -21.875065   -22.808338     0.36034417]]\n",
            "Layer 2 Biases: [-22.540958   0.       -22.525654  22.53851    0.      ]\n",
            "Layer 3 Weights: [[-2.2792639e+01 -2.2237003e+01 -5.7225853e-01 -2.2691708e+01\n",
            "   2.2520306e+01 -2.1580580e-01  2.2122425e+01  2.1940418e+01\n",
            "  -2.2831028e+01  2.2897551e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -2.2194071e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  2.1851749e+01]\n",
            " [-2.2139944e+01 -2.2174379e+01 -3.3204389e-01 -2.2405800e+01\n",
            "   2.2641268e+01 -1.5086198e-01  2.3043137e+01  2.3014648e+01\n",
            "  -2.1902887e+01  2.2609039e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-22.541557 -22.54185    0.       -22.54135   22.541668   0.\n",
            "  22.541773  22.541565 -22.537685  22.540861]\n",
            "Layer 4 Weights: [[-22.588228   -22.089546  ]\n",
            " [-22.038021   -22.08968   ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-22.484535   -22.190374  ]\n",
            " [-22.246197   -23.150845  ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-22.62555    -23.166847  ]\n",
            " [-22.297977   -23.133106  ]\n",
            " [-22.050318   -22.710651  ]\n",
            " [-22.424534   -22.771622  ]]\n",
            "Layer 4 Biases: [-22.541677 -22.541962]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3754 \n",
            "Epoch 4/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.3438\n",
            "Epoch 4 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 25.235777    24.977802    24.708612     7.9878883    0.03243005\n",
            "  -24.551243   -24.317549   -24.528086    24.925097   -24.674042  ]\n",
            " [ 25.358717   -24.172737    25.323227    14.388727    -0.5167843\n",
            "  -24.445189    24.766792   -24.855438    24.659092   -24.955791  ]\n",
            " [ 24.759176   -24.597902    24.601692     9.593334    -0.2567465\n",
            "  -25.247824   -25.071465   -24.64847     24.000944   -24.403595  ]\n",
            " [ 25.418365   -24.250216    24.674297     9.897614    -0.3987161\n",
            "  -24.828787    24.621153   -24.803501    24.060108   -24.915426  ]\n",
            " [ 25.24338     23.814194    25.502522     1.0986048    0.04104543\n",
            "  -24.864105    24.97373    -24.524172    25.319403   -25.020918  ]\n",
            " [ 25.10818     24.048853    24.353876    14.503146     0.2891739\n",
            "  -25.353727    25.13106    -25.164347    24.611046   -25.342606  ]]\n",
            "Layer 1 Biases: [ 24.908278 -24.535471  24.902466  15.216908   0.       -24.909178\n",
            "  24.87183  -24.908096  24.823948 -24.907068]\n",
            "Layer 2 Weights: [[-2.5144264e+01 -2.9899362e-01 -2.5409992e+01  2.5018936e+01\n",
            "  -5.8405340e-01]\n",
            " [-2.4352762e+01  2.3943388e-01 -2.4748631e+01 -2.5104057e+01\n",
            "  -5.1598841e-01]\n",
            " [-2.4976879e+01  1.7592013e-01 -2.7519730e-01  2.5282671e+01\n",
            "   7.5316191e-02]\n",
            " [ 9.7127676e-02  4.3976778e-01 -5.8916849e-01  2.4458088e+01\n",
            "   3.0657846e-01]\n",
            " [-9.3493521e-02 -2.4134576e-02 -5.6639147e-01 -1.6170612e-01\n",
            "  -4.8174524e-01]\n",
            " [-2.4353117e+01 -1.7820278e-01  3.8600880e-01  2.5220135e+01\n",
            "   2.5295633e-01]\n",
            " [-2.4744501e+01 -3.7265420e-02 -2.9645282e-01  2.5419518e+01\n",
            "  -1.6044810e-01]\n",
            " [-2.4620745e+01 -5.4626369e-01 -2.4417387e+01  2.4951887e+01\n",
            "  -2.4321368e-01]\n",
            " [-2.4946007e+01  4.9505001e-01 -9.7716808e-02  2.4833040e+01\n",
            "   4.6208066e-01]\n",
            " [-2.4498411e+01 -3.4243885e-01 -2.4239792e+01 -2.5174019e+01\n",
            "   3.6034417e-01]]\n",
            "Layer 2 Biases: [-24.91133    0.       -24.894407  24.908623   0.      ]\n",
            "Layer 3 Weights: [[-2.5161715e+01 -2.4606876e+01 -5.7225853e-01 -2.5059818e+01\n",
            "   2.4889732e+01 -2.1580580e-01  2.4491959e+01  2.4309120e+01\n",
            "  -2.5194342e+01  2.5265593e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -2.4558628e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  2.4188395e+01]\n",
            " [-2.4510210e+01 -2.4544739e+01 -3.3204389e-01 -2.4776011e+01\n",
            "   2.5011566e+01 -1.5086198e-01  2.5413486e+01  2.5384932e+01\n",
            "  -2.4271832e+01  2.4979006e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-24.911993 -24.912313   0.       -24.91176   24.912113   0.\n",
            "  24.91223   24.912    -24.907711  24.911224]\n",
            "Layer 4 Weights: [[-24.958138   -24.459764  ]\n",
            " [-24.408062   -24.459955  ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-24.853033   -24.559935  ]\n",
            " [-24.614595   -25.52034   ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-24.995564   -25.537117  ]\n",
            " [-24.667921   -25.503345  ]\n",
            " [-24.420444   -25.080976  ]\n",
            " [-24.793661   -25.141459  ]]\n",
            "Layer 4 Biases: [-24.912125 -24.91244 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3764 \n",
            "Epoch 5/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 5 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 26.838974    26.570694    26.310486     8.51194      0.03243005\n",
            "  -26.154673   -25.917847   -26.131329    26.520128   -26.276825  ]\n",
            " [ 26.961979   -25.709093    26.92614     15.301762    -0.5167843\n",
            "  -26.048624    26.366623   -26.458681    26.253721   -26.558908  ]\n",
            " [ 26.362177   -26.184488    26.202051    10.214183    -0.2567465\n",
            "  -26.850914   -26.662632   -26.25152     25.574888   -26.006653  ]\n",
            " [ 27.021414   -25.833246    26.27631     10.533897    -0.3987161\n",
            "  -26.43203     26.21691    -26.406513    25.629538   -26.517971  ]\n",
            " [ 26.84635     25.364988    27.105135     1.1960429    0.04104543\n",
            "  -26.467348    26.574427   -26.127113    26.91249    -26.623358  ]\n",
            " [ 26.71139     25.62358     25.956362    15.429912     0.2891739\n",
            "  -26.957026    26.732925   -26.767483    26.198616   -26.94525   ]]\n",
            "Layer 1 Biases: [ 26.511753 -26.114794  26.505564  16.194038   0.       -26.512712\n",
            "  26.472942 -26.511559  26.421959 -26.510464]\n",
            "Layer 2 Weights: [[-2.6747925e+01 -2.9899362e-01 -2.7010416e+01  2.6622406e+01\n",
            "  -5.8405340e-01]\n",
            " [-2.5945864e+01  2.3943388e-01 -2.6341633e+01 -2.6685953e+01\n",
            "  -5.1598841e-01]\n",
            " [-2.6579744e+01  1.7592013e-01 -2.7519730e-01  2.6884697e+01\n",
            "   7.5316191e-02]\n",
            " [ 9.7127676e-02  4.3976778e-01 -5.8916849e-01  2.6032251e+01\n",
            "   3.0657846e-01]\n",
            " [-9.3493521e-02 -2.4134576e-02 -5.6639147e-01 -1.6170612e-01\n",
            "  -4.8174524e-01]\n",
            " [-2.5956491e+01 -1.7820278e-01  3.8600880e-01  2.6822050e+01\n",
            "   2.5295633e-01]\n",
            " [-2.6347956e+01 -3.7265420e-02 -2.9645282e-01  2.7022284e+01\n",
            "  -1.6044810e-01]\n",
            " [-2.6224281e+01 -5.4626369e-01 -2.6016727e+01  2.6554092e+01\n",
            "  -2.4321368e-01]\n",
            " [-2.6548559e+01  4.9505001e-01 -9.7716808e-02  2.6433949e+01\n",
            "   4.6208066e-01]\n",
            " [-2.6100769e+01 -3.4243885e-01 -2.5839638e+01 -2.6774509e+01\n",
            "   3.6034417e-01]]\n",
            "Layer 2 Biases: [-26.515003   0.       -26.496984  26.51212    0.      ]\n",
            "Layer 3 Weights: [[-2.6764509e+01 -2.6210211e+01 -5.7225853e-01 -2.6661957e+01\n",
            "   2.6492764e+01 -2.1580580e-01  2.6095062e+01  2.5911661e+01\n",
            "  -2.6793230e+01  2.6867685e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -2.6158360e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  2.5769205e+01]\n",
            " [-2.6113810e+01 -2.6148403e+01 -3.3204389e-01 -2.6379574e+01\n",
            "   2.6615189e+01 -1.5086198e-01  2.7017145e+01  2.6988546e+01\n",
            "  -2.5874537e+01  2.6582403e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-26.515709 -26.516048   0.       -26.515463  26.515837   0.\n",
            "  26.51596   26.515717 -26.511148  26.51489 ]\n",
            "Layer 4 Weights: [[-26.561495   -26.063334  ]\n",
            " [-26.011509   -26.063562  ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-26.455435   -26.163057  ]\n",
            " [-26.21693    -27.123417  ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-26.598993   -27.14072   ]\n",
            " [-26.271303   -27.106928  ]\n",
            " [-26.023952   -26.684616  ]\n",
            " [-26.396488   -26.744766  ]]\n",
            "Layer 4 Biases: [-26.515848 -26.516184]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.3738 \n",
            "Epoch 6/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.3750\n",
            "Epoch 6 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 27.936901    27.661549    27.407505     8.870346     0.03243005\n",
            "  -27.25276    -27.013786   -27.229288    27.612452   -27.374466  ]\n",
            " [ 28.059952   -26.761158    28.023872    15.926501    -0.5167843\n",
            "  -27.146715    27.462242   -27.55664     27.34577    -27.65678   ]\n",
            " [ 27.459969   -27.271019    27.29803     10.638842    -0.2567465\n",
            "  -27.948767   -27.752304   -27.349344    26.652742   -27.104485  ]\n",
            " [ 28.119238   -26.917336    27.373423    10.969121    -0.3987161\n",
            "  -27.52999     27.309732   -27.504313    26.704296   -27.61545   ]\n",
            " [ 27.944122    26.42696     28.20266      1.2626467    0.04104543\n",
            "  -27.565306    27.670639   -27.224865    28.00348    -27.720766  ]\n",
            " [ 27.809324    26.701975    27.0538      16.064058     0.2891739\n",
            "  -28.055021    27.82994    -27.86537     27.28582    -28.042797  ]]\n",
            "Layer 1 Biases: [ 27.60987  -27.196339  27.603424  16.86269    0.       -27.610872\n",
            "  27.569439 -27.609667  27.516325 -27.608528]\n",
            "Layer 2 Weights: [[-2.7846172e+01 -2.9899362e-01 -2.8106440e+01  2.7720522e+01\n",
            "  -5.8405340e-01]\n",
            " [-2.7036861e+01  2.3943388e-01 -2.7432566e+01 -2.7769264e+01\n",
            "  -5.1598841e-01]\n",
            " [-2.7677444e+01  1.7592013e-01 -2.7519730e-01  2.7981817e+01\n",
            "   7.5316191e-02]\n",
            " [ 9.7127676e-02  4.3976778e-01 -5.8916849e-01  2.7110252e+01\n",
            "   3.0657846e-01]\n",
            " [-9.3493521e-02 -2.4134576e-02 -5.6639147e-01 -1.6170612e-01\n",
            "  -4.8174524e-01]\n",
            " [-2.7054539e+01 -1.7820278e-01  3.8600880e-01  2.7919098e+01\n",
            "   2.5295633e-01]\n",
            " [-2.7446058e+01 -3.7265420e-02 -2.9645282e-01  2.8119915e+01\n",
            "  -1.6044810e-01]\n",
            " [-2.7322441e+01 -5.4626369e-01 -2.7112009e+01  2.7651337e+01\n",
            "  -2.4321368e-01]\n",
            " [-2.7646044e+01  4.9505001e-01 -9.7716808e-02  2.7530304e+01\n",
            "   4.6208066e-01]\n",
            " [-2.7198118e+01 -3.4243885e-01 -2.6935265e+01 -2.7870581e+01\n",
            "   3.6034417e-01]]\n",
            "Layer 2 Biases: [-27.613258   0.       -27.594484  27.610252   0.      ]\n",
            "Layer 3 Weights: [[-2.7862162e+01 -2.7308231e+01 -5.7225853e-01 -2.7759157e+01\n",
            "   2.7590578e+01 -2.1580580e-01  2.7192926e+01  2.7009140e+01\n",
            "  -2.7888199e+01  2.7964855e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -2.7253906e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  2.6851768e+01]\n",
            " [-2.7212015e+01 -2.7246651e+01 -3.3204389e-01 -2.7477753e+01\n",
            "   2.7713408e+01 -1.5086198e-01  2.8115387e+01  2.8086760e+01\n",
            "  -2.6972124e+01  2.7680468e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-27.61399  -27.614347   0.       -27.613735  27.614126   0.\n",
            "  27.614254  27.613998 -27.609241  27.613138]\n",
            "Layer 4 Weights: [[-27.659533   -27.161516  ]\n",
            " [-27.10961    -27.161772  ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-27.552816   -27.260933  ]\n",
            " [-27.314268   -28.221262  ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-27.69708    -28.238926  ]\n",
            " [-27.369358   -28.20512   ]\n",
            " [-27.122093   -27.782848  ]\n",
            " [-27.494164   -27.842772  ]]\n",
            "Layer 4 Biases: [-27.614138 -27.614487]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.3811 \n",
            "Epoch 7/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.3281\n",
            "Epoch 7 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 28.692509    28.41228     28.162485     9.116673     0.03243005\n",
            "  -28.008478   -27.768023   -27.984919    28.364195   -28.129875  ]\n",
            " [ 28.81559    -27.485142    28.779343    16.356087    -0.5167843\n",
            "  -27.902433    28.216257   -28.312271    28.09732    -28.41235   ]\n",
            " [ 28.215483   -28.018768    28.052296    10.9307375   -0.2567465\n",
            "  -28.704323   -28.50222    -28.10488     27.394508   -27.860027  ]\n",
            " [ 28.874773   -27.6634      28.128468    11.268286    -0.3987161\n",
            "  -28.28562     28.06182    -28.25983     27.443926   -28.370747  ]\n",
            " [ 28.699623    27.157772    28.957989     1.3083981    0.04104543\n",
            "  -28.320934    28.425062   -27.98035     28.754307   -28.476015  ]\n",
            " [ 28.564938    27.444113    27.80907     16.500116     0.2891739\n",
            "  -28.810677    28.584919   -28.620947    28.034033   -28.798141  ]]\n",
            "Layer 1 Biases: [ 28.365608 -27.940649  28.358984  17.322506   0.       -28.366638\n",
            "  28.324059 -28.3654    28.269478 -28.364231]\n",
            "Layer 2 Weights: [[-2.8601997e+01 -2.9899362e-01 -2.8860737e+01  2.8476259e+01\n",
            "  -5.8405340e-01]\n",
            " [-2.7787691e+01  2.3943388e-01 -2.8183348e+01 -2.8514790e+01\n",
            "  -5.1598841e-01]\n",
            " [-2.8432894e+01  1.7592013e-01 -2.7519730e-01  2.8736868e+01\n",
            "   7.5316191e-02]\n",
            " [ 9.7127676e-02  4.3976778e-01 -5.8916849e-01  2.7852121e+01\n",
            "   3.0657846e-01]\n",
            " [-9.3493521e-02 -2.4134576e-02 -5.6639147e-01 -1.6170612e-01\n",
            "  -4.8174524e-01]\n",
            " [-2.7810228e+01 -1.7820278e-01  3.8600880e-01  2.8674099e+01\n",
            "   2.5295633e-01]\n",
            " [-2.8201786e+01 -3.7265420e-02 -2.9645282e-01  2.8875319e+01\n",
            "  -1.6044810e-01]\n",
            " [-2.8078211e+01 -5.4626369e-01 -2.7865791e+01  2.8406473e+01\n",
            "  -2.4321368e-01]\n",
            " [-2.8401346e+01  4.9505001e-01 -9.7716808e-02  2.8284826e+01\n",
            "   4.6208066e-01]\n",
            " [-2.7953327e+01 -3.4243885e-01 -2.7689287e+01 -2.8624908e+01\n",
            "   3.6034417e-01]]\n",
            "Layer 2 Biases: [-28.369091   0.       -28.349798  28.366      0.      ]\n",
            "Layer 3 Weights: [[-2.8617579e+01 -2.8063904e+01 -5.7225853e-01 -2.8514267e+01\n",
            "   2.8346107e+01 -2.1580580e-01  2.7948488e+01  2.7764435e+01\n",
            "  -2.8641766e+01  2.8719940e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -2.8007874e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  2.7596783e+01]\n",
            " [-2.7967813e+01 -2.8002478e+01 -3.3204389e-01 -2.8233532e+01\n",
            "   2.8469215e+01 -1.5086198e-01  2.8871212e+01  2.8842564e+01\n",
            "  -2.7727499e+01  2.8436171e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-28.36984  -28.37021    0.       -28.369577  28.369984   0.\n",
            "  28.370111  28.369848 -28.364962  28.368967]\n",
            "Layer 4 Weights: [[-28.415215   -27.9173    ]\n",
            " [-27.865335   -27.917572  ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-28.308048   -28.016502  ]\n",
            " [-28.069466   -28.976812  ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-28.452799   -28.994724  ]\n",
            " [-28.125053   -28.96091   ]\n",
            " [-27.877848   -28.538666  ]\n",
            " [-28.249596   -28.59843   ]]\n",
            "Layer 4 Biases: [-28.369995 -28.370352]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.3675 \n",
            "Epoch 8/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.3906\n",
            "Epoch 8 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 29.21317     28.929573    28.682714     9.2861805    0.03243005\n",
            "  -28.529213   -28.287735   -28.505594    28.882187   -28.650398  ]\n",
            " [ 29.336273   -27.983969    29.299908    16.651842    -0.5167843\n",
            "  -28.423172    28.735819   -28.832947    28.61518    -28.932983  ]\n",
            " [ 28.736078   -28.534002    28.57203     11.131626    -0.2567465\n",
            "  -29.224947   -29.018951   -28.62549     27.905613   -28.380642  ]\n",
            " [ 29.395384   -28.177471    28.64874     11.47418     -0.3987161\n",
            "  -28.806297    28.580048   -28.780428    27.953554   -28.891195  ]\n",
            " [ 29.22021     27.661314    29.478458     1.3398645    0.04104543\n",
            "  -28.84161     28.944904   -28.500925    29.271666   -28.996428  ]\n",
            " [ 29.0856      27.955473    28.329498    16.800335     0.2891739\n",
            "  -29.331367    29.105143   -29.141584    28.54959    -29.318619  ]]\n",
            "Layer 1 Biases: [ 28.886358 -28.45351   28.879612  17.6391     0.       -28.88741\n",
            "  28.844036 -28.886148  28.788445 -28.884956]\n",
            "Layer 2 Weights: [[-2.9122808e+01 -2.9899362e-01 -2.9380491e+01  2.8997009e+01\n",
            "  -5.8405340e-01]\n",
            " [-2.8305052e+01  2.3943388e-01 -2.8700676e+01 -2.9028490e+01\n",
            "  -5.1598841e-01]\n",
            " [-2.8953444e+01  1.7592013e-01 -2.7519730e-01  2.9257143e+01\n",
            "   7.5316191e-02]\n",
            " [ 9.7127676e-02  4.3976778e-01 -5.8916849e-01  2.8363295e+01\n",
            "   3.0657846e-01]\n",
            " [-9.3493521e-02 -2.4134576e-02 -5.6639147e-01 -1.6170612e-01\n",
            "  -4.8174524e-01]\n",
            " [-2.8330946e+01 -1.7820278e-01  3.8600880e-01  2.9194340e+01\n",
            "   2.5295633e-01]\n",
            " [-2.8722528e+01 -3.7265420e-02 -2.9645282e-01  2.9395836e+01\n",
            "  -1.6044810e-01]\n",
            " [-2.8598982e+01 -5.4626369e-01 -2.8385193e+01  2.8926809e+01\n",
            "  -2.4321368e-01]\n",
            " [-2.8921797e+01  4.9505001e-01 -9.7716808e-02  2.8804737e+01\n",
            "   4.6208066e-01]\n",
            " [-2.8473713e+01 -3.4243885e-01 -2.8208853e+01 -2.9144686e+01\n",
            "   3.6034417e-01]]\n",
            "Layer 2 Biases: [-28.889906   0.       -28.870256  28.886757   0.      ]\n",
            "Layer 3 Weights: [[-2.9138109e+01 -2.8584610e+01 -5.7225853e-01 -2.9034582e+01\n",
            "   2.8866713e+01 -2.1580580e-01  2.8469118e+01  2.8284878e+01\n",
            "  -2.9161016e+01  2.9240238e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -2.8527401e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  2.8110130e+01]\n",
            " [-2.8488605e+01 -2.8523291e+01 -3.3204389e-01 -2.8754311e+01\n",
            "   2.8990015e+01 -1.5086198e-01  2.9392023e+01  2.9363359e+01\n",
            "  -2.8247999e+01  2.8956896e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-28.89067  -28.891048   0.       -28.8904    28.890814   0.\n",
            "  28.890947  28.890678 -28.8857    28.88978 ]\n",
            "Layer 4 Weights: [[-28.935926   -28.438082  ]\n",
            " [-28.386078   -28.438368  ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-28.828447   -28.537138  ]\n",
            " [-28.589844   -29.497433  ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-28.973534   -29.515516  ]\n",
            " [-28.645773   -29.481697  ]\n",
            " [-28.398611   -29.05947   ]\n",
            " [-28.770134   -29.119127  ]]\n",
            "Layer 4 Biases: [-28.890825 -28.89119 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3847 \n",
            "Epoch 9/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 0.3906\n",
            "Epoch 9 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 29.571684    29.285765    29.040932     9.402741     0.03243005\n",
            "  -28.887781   -28.645597   -28.86412     29.238861   -29.008821  ]\n",
            " [ 29.694805   -28.327421    29.65836     16.855318    -0.5167843\n",
            "  -28.78174     29.093576   -29.191473    28.971764   -29.291481  ]\n",
            " [ 29.09455    -28.888773    28.929907    11.269783    -0.2567465\n",
            "  -29.583437   -29.374756   -28.98397     28.257536   -28.739126  ]\n",
            " [ 29.753864   -28.531443    29.006989    11.615783    -0.3987161\n",
            "  -29.164824    28.936888   -29.1389      28.304459   -29.249563  ]\n",
            " [ 29.578674    28.008022    29.836843     1.361491     0.04104543\n",
            "  -29.200134    29.302856   -28.859385    29.627903   -29.354773  ]\n",
            " [ 29.44412     28.307573    28.687853    17.006886     0.2891739\n",
            "  -29.689907    29.463358   -29.500088    28.904585   -29.677011  ]]\n",
            "Layer 1 Biases: [ 29.244936 -28.806646  29.238106  17.856926   0.       -29.246002\n",
            "  29.202082 -29.244724  29.145792 -29.243517]\n",
            "Layer 2 Weights: [[-2.9481428e+01 -2.9899362e-01 -2.9738380e+01  2.9355587e+01\n",
            "  -5.8405340e-01]\n",
            " [-2.8661291e+01  2.3943388e-01 -2.9056894e+01 -2.9382206e+01\n",
            "  -5.1598841e-01]\n",
            " [-2.9311884e+01  1.7592013e-01 -2.7519730e-01  2.9615393e+01\n",
            "   7.5316191e-02]\n",
            " [ 9.7127676e-02  4.3976778e-01 -5.8916849e-01  2.8715267e+01\n",
            "   3.0657846e-01]\n",
            " [-9.3493521e-02 -2.4134576e-02 -5.6639147e-01 -1.6170612e-01\n",
            "  -4.8174524e-01]\n",
            " [-2.8689501e+01 -1.7820278e-01  3.8600880e-01  2.9552565e+01\n",
            "   2.5295633e-01]\n",
            " [-2.9081100e+01 -3.7265420e-02 -2.9645282e-01  2.9754253e+01\n",
            "  -1.6044810e-01]\n",
            " [-2.8957575e+01 -5.4626369e-01 -2.8742840e+01  2.9285101e+01\n",
            "  -2.4321368e-01]\n",
            " [-2.9280169e+01  4.9505001e-01 -9.7716808e-02  2.9162737e+01\n",
            "   4.6208066e-01]\n",
            " [-2.8832041e+01 -3.4243885e-01 -2.8566612e+01 -2.9502590e+01\n",
            "   3.6034417e-01]]\n",
            "Layer 2 Biases: [-29.248528   0.       -29.22863   29.245342   0.      ]\n",
            "Layer 3 Weights: [[-2.9496534e+01 -2.8943157e+01 -5.7225853e-01 -2.9392860e+01\n",
            "   2.9225191e+01 -2.1580580e-01  2.8827614e+01  2.8643244e+01\n",
            "  -2.9518560e+01  2.9598505e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -2.8885136e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  2.8463598e+01]\n",
            " [-2.8847214e+01 -2.8881910e+01 -3.3204389e-01 -2.9112909e+01\n",
            "   2.9348627e+01 -1.5086198e-01  2.9750643e+01  2.9721968e+01\n",
            "  -2.8606403e+01  2.9315456e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-29.249302 -29.249685   0.       -29.249027  29.249449   0.\n",
            "  29.249582  29.249311 -29.24427   29.2484  ]\n",
            "Layer 4 Weights: [[-29.29448    -28.796682  ]\n",
            " [-28.74465    -28.796976  ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-29.186785   -28.895636  ]\n",
            " [-28.948164   -29.85592   ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-29.332102   -29.874125  ]\n",
            " [-29.00433    -29.840298  ]\n",
            " [-28.757196   -29.418085  ]\n",
            " [-29.128567   -29.477667  ]]\n",
            "Layer 4 Biases: [-29.24946  -29.249828]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.3863 \n",
            "Epoch 10/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.3125\n",
            "Epoch 10 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 29.818132    29.530613    29.287176     9.4827585    0.03243005\n",
            "  -29.134266   -28.891594   -29.110577    29.48404    -29.255205  ]\n",
            " [ 29.941263   -28.563498    29.904764    16.995068    -0.5167843\n",
            "  -29.028225    29.3395     -29.43793     29.21688    -29.537918  ]\n",
            " [ 29.340967   -29.13264     29.175913    11.364637    -0.2567465\n",
            "  -29.829866   -29.619337   -29.230394    28.499443   -28.985554  ]\n",
            " [ 30.000288   -28.77476     29.25325     11.713004    -0.3987161\n",
            "  -29.41128     29.18218    -29.38532     28.545666   -29.49591   ]\n",
            " [ 29.825087    28.246338    30.0832       1.3763293    0.04104543\n",
            "  -29.44659     29.548916   -29.105791    29.87278    -29.601105  ]\n",
            " [ 29.69057     28.549603    28.93419     17.148748     0.2891739\n",
            "  -29.93637     29.7096     -29.746525    29.148607   -29.923374  ]]\n",
            "Layer 1 Biases: [ 29.491428 -29.049387  29.484537  18.006544   0.       -29.492502\n",
            "  29.448204 -29.491213  29.391436 -29.489994]\n",
            "Layer 2 Weights: [[-2.9727947e+01 -2.9899362e-01 -2.9984398e+01  2.9602079e+01\n",
            "  -5.8405340e-01]\n",
            " [-2.8906172e+01  2.3943388e-01 -2.9301758e+01 -2.9625347e+01\n",
            "  -5.1598841e-01]\n",
            " [-2.9558281e+01  1.7592013e-01 -2.7519730e-01  2.9861658e+01\n",
            "   7.5316191e-02]\n",
            " [ 9.7127676e-02  4.3976778e-01 -5.8916849e-01  2.8957209e+01\n",
            "   3.0657846e-01]\n",
            " [-9.3493521e-02 -2.4134576e-02 -5.6639147e-01 -1.6170612e-01\n",
            "  -4.8174524e-01]\n",
            " [-2.8935976e+01 -1.7820278e-01  3.8600880e-01  2.9798813e+01\n",
            "   2.5295633e-01]\n",
            " [-2.9327587e+01 -3.7265420e-02 -2.9645282e-01  3.0000635e+01\n",
            "  -1.6044810e-01]\n",
            " [-2.9204077e+01 -5.4626369e-01 -2.8988689e+01  2.9531397e+01\n",
            "  -2.4321368e-01]\n",
            " [-2.9526518e+01  4.9505001e-01 -9.7716808e-02  2.9408829e+01\n",
            "   4.6208066e-01]\n",
            " [-2.9078358e+01 -3.4243885e-01 -2.8812542e+01 -2.9748617e+01\n",
            "   3.6034417e-01]]\n",
            "Layer 2 Biases: [-29.49505    0.       -29.474981  29.491837   0.      ]\n",
            "Layer 3 Weights: [[-2.9742920e+01 -2.9189627e+01 -5.7225853e-01 -2.9639143e+01\n",
            "   2.9471613e+01 -2.1580580e-01  2.9074047e+01  2.8889589e+01\n",
            "  -2.9764338e+01  2.9844782e+01]\n",
            " [ 2.9063225e-03  4.6493167e-01  3.1761158e-01 -8.6432517e-02\n",
            "   6.3110703e-01  4.6993917e-01  4.4863886e-01  1.5695733e-01\n",
            "  -3.4595525e-01 -3.2506657e-01]\n",
            " [-5.9871089e-01 -2.9131044e+01 -1.2951142e-01 -2.5148749e-01\n",
            "  -5.9259760e-01  1.4737815e-01 -1.9554502e-01 -6.2868673e-01\n",
            "   3.6451375e-01  2.8706570e+01]\n",
            " [-2.9093723e+01 -2.9128431e+01 -3.3204389e-01 -2.9359411e+01\n",
            "   2.9595142e+01 -1.5086198e-01  2.9997160e+01  2.9968477e+01\n",
            "  -2.8852774e+01  2.9561935e+01]\n",
            " [-1.0385299e-01 -2.9689297e-01 -4.6140045e-01 -1.7145255e-01\n",
            "  -6.0756445e-01 -3.7730008e-01 -4.8790875e-01  5.0119525e-01\n",
            "  -3.7084758e-02 -2.1837646e-01]]\n",
            "Layer 3 Biases: [-29.49583  -29.496216   0.       -29.495552  29.495977   0.\n",
            "  29.496113  29.49584  -29.490755  29.49492 ]\n",
            "Layer 4 Weights: [[-29.54095    -29.04319   ]\n",
            " [-28.991135   -29.043486  ]\n",
            " [  0.36226922   0.43533617]\n",
            " [-29.43311    -29.142073  ]\n",
            " [-29.194479   -30.10235   ]\n",
            " [  0.09340179  -0.41887072]\n",
            " [-29.578587   -30.120634  ]\n",
            " [-29.250807   -30.086807  ]\n",
            " [-29.00369    -29.664602  ]\n",
            " [-29.374956   -29.72413   ]]\n",
            "Layer 4 Biases: [-29.495989 -29.49636 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.3722 \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_folder: /content/DATA/run_4/original_result\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m70\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │              \u001b[38;5;34m55\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m60\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m22\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.3750 - loss: 1.1286\n",
            "Epoch 1 ended\n",
            "Loss: 0.6061678528785706, Accuracy: 0.800000011920929\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.51725411e-02 -1.27815132e+01 -5.70627451e-01 -1.31015091e+01\n",
            "   1.36574516e+01 -4.45825934e-01  1.35217743e+01 -1.26800146e+01\n",
            "   1.24218969e+01 -1.30471430e+01]\n",
            " [-3.05377394e-01 -1.25373383e+01 -4.89435107e-01  1.33987665e+01\n",
            "   1.27358160e+01 -5.79496264e-01  1.33210983e+01 -1.30385542e+01\n",
            "   1.27139235e+01 -1.28053370e+01]\n",
            " [-1.02790058e-01 -1.30415201e+01  8.57365727e-02  1.33854418e+01\n",
            "   1.26692886e+01  1.20871007e-01  1.31459265e+01 -1.25128345e+01\n",
            "   1.27155914e+01 -1.29631634e+01]\n",
            " [-2.13140845e-02 -1.31640568e+01 -5.58016002e-01  1.30175953e+01\n",
            "   1.30679493e+01 -3.83802384e-01  1.32005472e+01 -1.34034233e+01\n",
            "   1.32448254e+01 -1.24436913e+01]\n",
            " [-2.54215598e-02 -1.35534954e+01 -3.00290287e-01 -1.29583797e+01\n",
            "   1.32855844e+01 -4.63091433e-01  1.24504662e+01 -1.33137493e+01\n",
            "   1.22396679e+01 -1.29835539e+01]\n",
            " [-6.11068666e-01 -1.25118570e+01  1.02606416e-02  1.25748787e+01\n",
            "   1.36074533e+01  2.87896395e-02  1.33002977e+01 -1.26970491e+01\n",
            "   1.22642174e+01 -1.31220713e+01]]\n",
            "Layer 1 Biases: [  0.        -13.060088    0.         12.747627   13.059455    0.\n",
            "  13.0600815 -12.980257   13.014338  -13.058338 ]\n",
            "Layer 2 Weights: [[-7.72148967e-02 -3.33413661e-01 -5.56831777e-01  1.41625702e-01\n",
            "  -6.22492135e-01]\n",
            " [-1.24892817e+01 -9.89452600e-02 -1.80735588e-02  1.26244326e+01\n",
            "  -1.25282793e+01]\n",
            " [ 1.61857367e-01 -4.86230314e-01  5.35997331e-01  5.23719966e-01\n",
            "   1.17049932e-01]\n",
            " [-1.30951853e+01 -4.30669785e-01  1.27193928e-02  1.25794115e+01\n",
            "  -1.28506966e+01]\n",
            " [-1.35204144e+01 -3.57834548e-01 -2.45316595e-01  1.34524384e+01\n",
            "  -1.28040476e+01]\n",
            " [-3.98630410e-01  2.96043456e-01  4.07760441e-01 -1.77492708e-01\n",
            "  -4.93901253e-01]\n",
            " [-1.36363211e+01 -2.19497591e-01  2.47372389e-02  1.34183512e+01\n",
            "  -1.33345270e+01]\n",
            " [-1.30637493e+01  5.31604290e-02  1.96975589e-01  1.26513443e+01\n",
            "  -1.27861042e+01]\n",
            " [-1.27584457e+01 -2.72500724e-01 -3.38955611e-01  2.87237048e-01\n",
            "  -2.86988556e-01]\n",
            " [-1.28309708e+01 -1.17402911e-01 -3.10595185e-01  1.27856073e+01\n",
            "  -1.30981255e+01]]\n",
            "Layer 2 Biases: [-13.060531   0.         0.        13.057337 -13.00911 ]\n",
            "Layer 3 Weights: [[-1.25985498e+01 -3.97140771e-01 -5.05412519e-01 -1.29384308e+01\n",
            "  -5.99960327e-01 -1.26543350e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -1.25574265e+01  1.34552383e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-1.30171518e+01 -1.32262886e-01 -4.62086558e-01 -1.27618980e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  1.35135918e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  1.35123272e+01]\n",
            " [-1.07479582e+01 -2.99450517e-01  8.40138197e-02 -1.26809330e+01\n",
            "  -4.88672197e-01 -1.14001598e+01  3.82595837e-01  1.95535362e-01\n",
            "  -5.24184513e+00  2.54744530e+00]]\n",
            "Layer 3 Biases: [-13.060802   0.         0.       -13.061106   0.       -13.060629\n",
            "  13.060747   0.       -13.052116  13.055684]\n",
            "Layer 4 Weights: [[-12.644947   -12.856777  ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-12.486011   -12.527342  ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-13.05032    -12.559965  ]\n",
            " [-13.005222   -13.738264  ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-12.851373   -13.081325  ]\n",
            " [-13.609247   -12.887119  ]]\n",
            "Layer 4 Biases: [-13.061042  -13.0612545]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.6908 - loss: 0.7438\n",
            "Epoch 2/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.3750\n",
            "Epoch 2 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -1.8657839e+01 -5.7062745e-01 -1.8956013e+01\n",
            "   1.9533253e+01 -4.4582593e-01  1.9398075e+01 -1.8516493e+01\n",
            "   1.8081148e+01 -1.8921810e+01]\n",
            " [-3.0537739e-01 -1.8414080e+01 -4.8943511e-01  1.9255188e+01\n",
            "   1.8611837e+01 -5.7949626e-01  1.9197847e+01 -1.8663137e+01\n",
            "   1.8527845e+01 -1.8680611e+01]\n",
            " [-1.0279006e-01 -1.8918428e+01  8.5736573e-02  1.9229147e+01\n",
            "   1.8545689e+01  1.2087101e-01  1.9022835e+01 -1.8299398e+01\n",
            "   1.8412949e+01 -1.8838825e+01]\n",
            " [-2.1314085e-02 -1.9040693e+01 -5.5801600e-01  1.8790985e+01\n",
            "   1.8943909e+01 -3.8380238e-01  1.9077183e+01 -1.9215746e+01\n",
            "   1.9078495e+01 -1.8318861e+01]\n",
            " [-2.5421560e-02 -1.9429625e+01 -3.0029029e-01 -1.8816660e+01\n",
            "   1.9161249e+01 -4.6309143e-01  1.8326576e+01 -1.9134293e+01\n",
            "   1.7857420e+01 -1.8857834e+01]\n",
            " [-6.1106867e-01 -1.8388485e+01  1.0260642e-02  1.8403471e+01\n",
            "   1.9483746e+01  2.8789639e-02  1.9176914e+01 -1.8533873e+01\n",
            "   1.7855537e+01 -1.8997145e+01]]\n",
            "Layer 1 Biases: [  0.       -18.937254   0.        18.483894  18.936337   0.\n",
            "  18.937246 -18.821424  18.870874 -18.934715]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-1.8366255e+01 -9.8945260e-02 -1.8073559e-02  1.8496605e+01\n",
            "  -1.8362314e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-1.8972059e+01 -4.3066978e-01  1.2719393e-02  1.8432869e+01\n",
            "  -1.8499208e+01]\n",
            " [-1.9395515e+01 -3.5783455e-01 -2.4531659e-01  1.9327538e+01\n",
            "  -1.8443180e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-1.9513098e+01 -2.1949759e-01  2.4737239e-02  1.9289461e+01\n",
            "  -1.9140287e+01]\n",
            " [-1.8933672e+01  5.3160429e-02  1.9697559e-01  1.8449909e+01\n",
            "  -1.8578291e+01]\n",
            " [-1.8266308e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-1.8708023e+01 -1.1740291e-01 -3.1059518e-01  1.8658455e+01\n",
            "  -1.8816387e+01]]\n",
            "Layer 2 Biases: [-18.937897   0.         0.        18.933262 -18.86329 ]\n",
            "Layer 3 Weights: [[-1.84711952e+01 -3.97140771e-01 -5.05412519e-01 -1.88137932e+01\n",
            "  -5.99960327e-01 -1.85285797e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -1.83853016e+01  1.92607384e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-1.88892860e+01 -1.32262886e-01 -4.62086558e-01 -1.86370125e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  1.93885460e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  1.93179836e+01]\n",
            " [-1.58474712e+01 -2.99450517e-01  8.40138197e-02 -1.81699753e+01\n",
            "  -4.88672197e-01 -1.67309074e+01  3.82595837e-01  1.95535362e-01\n",
            "  -7.48384953e+00  3.82282186e+00]]\n",
            "Layer 3 Biases: [-18.938292   0.         0.       -18.93873    0.       -18.938038\n",
            "  18.938211   0.       -18.925688  18.930864]\n",
            "Layer 4 Weights: [[-18.51259    -18.731     ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-18.351025   -18.400608  ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-18.915903   -18.433474  ]\n",
            " [-18.871325   -19.611898  ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-18.716654   -18.954723  ]\n",
            " [-19.482592   -18.763323  ]]\n",
            "Layer 4 Biases: [-18.938639 -18.938948]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.3769\n",
            "Epoch 3/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.3906\n",
            "Epoch 3 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -2.2260059e+01 -5.7062745e-01 -2.2544828e+01\n",
            "   2.3135149e+01 -4.4582593e-01  2.3000280e+01 -2.2094236e+01\n",
            "   2.1550039e+01 -2.2523008e+01]\n",
            " [-3.0537739e-01 -2.2016554e+01 -4.8943511e-01  2.2845181e+01\n",
            "   2.2213871e+01 -5.7949626e-01  2.2800327e+01 -2.2110739e+01\n",
            "   2.2091736e+01 -2.2282185e+01]\n",
            " [-1.0279006e-01 -2.2521006e+01  8.5736573e-02  2.2811331e+01\n",
            "   2.2147953e+01  1.2087101e-01  2.2625412e+01 -2.1846481e+01\n",
            "   2.1905245e+01 -2.2440638e+01]\n",
            " [-2.1314085e-02 -2.2643103e+01 -5.5801600e-01  2.2329981e+01\n",
            "   2.2545902e+01 -3.8380238e-01  2.2679592e+01 -2.2778652e+01\n",
            "   2.2654514e+01 -2.1920372e+01]\n",
            " [-2.5421560e-02 -2.3031723e+01 -3.0029029e-01 -2.2407797e+01\n",
            "   2.2763065e+01 -4.6309143e-01  2.1928661e+01 -2.2702251e+01\n",
            "   2.1300825e+01 -2.2458797e+01]\n",
            " [-6.1106867e-01 -2.1990891e+01  1.0260642e-02  2.1976372e+01\n",
            "   2.3085943e+01  2.8789639e-02  2.2779312e+01 -2.2111830e+01\n",
            "   2.1282713e+01 -2.2598597e+01]]\n",
            "Layer 1 Biases: [  0.       -22.53999    0.        22.000088  22.538897   0.\n",
            "  22.539982 -22.402048  22.460938 -22.536968]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-2.1968874e+01 -9.8945260e-02 -1.8073559e-02  2.2096272e+01\n",
            "  -2.1938557e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-2.2574615e+01 -4.3066978e-01  1.2719393e-02  2.2021042e+01\n",
            "  -2.1961506e+01]\n",
            " [-2.2996981e+01 -3.5783455e-01 -2.4531659e-01  2.2929003e+01\n",
            "  -2.1899717e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-2.3115593e+01 -2.1949759e-01  2.4737239e-02  2.2888477e+01\n",
            "  -2.2699162e+01]\n",
            " [-2.2531960e+01  5.3160429e-02  1.9697559e-01  2.2004368e+01\n",
            "  -2.2128830e+01]\n",
            " [-2.1642229e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-2.2310692e+01 -1.1740291e-01 -3.1059518e-01  2.2258537e+01\n",
            "  -2.2321520e+01]]\n",
            "Layer 2 Biases: [-22.540754   0.         0.        22.535234 -22.451908]\n",
            "Layer 3 Weights: [[-2.20711555e+01 -3.97140771e-01 -5.05412519e-01 -2.24154205e+01\n",
            "  -5.99960327e-01 -2.21295185e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -2.19577599e+01  2.28194561e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-2.24889297e+01 -1.32262886e-01 -4.62086558e-01 -2.22384872e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  2.29899216e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  2.28767986e+01]\n",
            " [-1.89726658e+01 -2.99450517e-01  8.40138197e-02 -2.15343418e+01\n",
            "  -4.88672197e-01 -1.99980736e+01  3.82595837e-01  1.95535362e-01\n",
            "  -8.85649586e+00  4.60340023e+00]]\n",
            "Layer 3 Biases: [-22.541227   0.         0.       -22.541748   0.       -22.540924\n",
            "  22.54113    0.       -22.526215  22.532381]\n",
            "Layer 4 Weights: [[-22.109478   -22.33193   ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-21.946295   -22.00095   ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-22.511522   -22.033962  ]\n",
            " [-22.467262   -23.212463  ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-22.312088   -22.555147  ]\n",
            " [-23.082981   -22.365467  ]]\n",
            "Layer 4 Biases: [-22.54164  -22.542007]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.3847  \n",
            "Epoch 4/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 0.4219\n",
            "Epoch 4 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -2.4629988e+01 -5.7062745e-01 -2.4905922e+01\n",
            "   2.5504864e+01 -4.4582593e-01  2.5370199e+01 -2.4448030e+01\n",
            "   2.3832081e+01 -2.4892265e+01]\n",
            " [-3.0537739e-01 -2.4386652e+01 -4.8943511e-01  2.5207048e+01\n",
            "   2.4583677e+01 -5.7949626e-01  2.5170425e+01 -2.4378748e+01\n",
            "   2.4436398e+01 -2.4651686e+01]\n",
            " [-1.0279006e-01 -2.4891171e+01  8.5736573e-02  2.5168051e+01\n",
            "   2.4517912e+01  1.2087101e-01  2.4995577e+01 -2.4180067e+01\n",
            "   2.4202715e+01 -2.4810299e+01]\n",
            " [-2.1314085e-02 -2.5013157e+01 -5.5801600e-01  2.4658230e+01\n",
            "   2.4915682e+01 -3.8380238e-01  2.5049646e+01 -2.5122665e+01\n",
            "   2.5007172e+01 -2.4289833e+01]\n",
            " [-2.5421560e-02 -2.5401571e+01 -3.0029029e-01 -2.4770416e+01\n",
            "   2.5132727e+01 -4.6309143e-01  2.4298502e+01 -2.5049595e+01\n",
            "   2.3566067e+01 -2.4827898e+01]\n",
            " [-6.1106867e-01 -2.4360941e+01  1.0260642e-02  2.4326971e+01\n",
            "   2.5455858e+01  2.8789639e-02  2.5149359e+01 -2.4465765e+01\n",
            "   2.3537258e+01 -2.4968016e+01]]\n",
            "Layer 1 Biases: [  0.       -24.910257   0.        24.313309  24.909048   0.\n",
            "  24.91025  -24.75774   24.822851 -24.906918]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-2.4339066e+01 -9.8945260e-02 -1.8073559e-02  2.4464520e+01\n",
            "  -2.4291361e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-2.4944767e+01 -4.3066978e-01  1.2719393e-02  2.4381710e+01\n",
            "  -2.4239201e+01]\n",
            " [-2.5366413e+01 -3.5783455e-01 -2.4531659e-01  2.5298435e+01\n",
            "  -2.4173615e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-2.5485703e+01 -2.1949759e-01  2.4737239e-02  2.5256296e+01\n",
            "  -2.5040516e+01]\n",
            " [-2.4899296e+01  5.3160429e-02  1.9697559e-01  2.4342812e+01\n",
            "  -2.4464689e+01]\n",
            " [-2.3862995e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-2.4680916e+01 -1.1740291e-01 -3.1059518e-01  2.4627058e+01\n",
            "  -2.4627451e+01]]\n",
            "Layer 2 Biases: [-24.911102   0.         0.        24.905    -24.812868]\n",
            "Layer 3 Weights: [[-2.44395962e+01 -3.97140771e-01 -5.05412519e-01 -2.47849579e+01\n",
            "  -5.99960327e-01 -2.44986038e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -2.43080692e+01  2.51607075e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-2.48571587e+01 -1.32262886e-01 -4.62086558e-01 -2.46079273e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  2.53592949e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  2.52181129e+01]\n",
            " [-2.10282097e+01 -2.99450517e-01  8.40138197e-02 -2.37474918e+01\n",
            "  -4.88672197e-01 -2.21471672e+01  3.82595837e-01  1.95535362e-01\n",
            "  -9.75844955e+00  5.11614180e+00]]\n",
            "Layer 3 Biases: [-24.911627   0.         0.       -24.912203   0.       -24.911293\n",
            "  24.91152    0.       -24.895027  24.901846]\n",
            "Layer 4 Weights: [[-24.475891   -24.701008  ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-24.311644   -24.36964   ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-24.877102   -24.402748  ]\n",
            " [-24.83305    -25.5813    ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-24.677544   -24.923891  ]\n",
            " [-25.451702   -24.735346  ]]\n",
            "Layer 4 Biases: [-24.912083 -24.912489]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.3910  \n",
            "Epoch 5/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.3438\n",
            "Epoch 5 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -2.6233360e+01 -5.7062745e-01 -2.6503305e+01\n",
            "   2.7108091e+01 -4.4582593e-01  2.6973564e+01 -2.6040464e+01\n",
            "   2.5375879e+01 -2.6495180e+01]\n",
            " [-3.0537739e-01 -2.5990137e+01 -4.8943511e-01  2.6804956e+01\n",
            "   2.6186966e+01 -5.7949626e-01  2.6773914e+01 -2.5913033e+01\n",
            "   2.6022640e+01 -2.6254768e+01]\n",
            " [-1.0279006e-01 -2.6494703e+01  8.5736573e-02  2.6762468e+01\n",
            "   2.6121302e+01  1.2087101e-01  2.6599112e+01 -2.5758802e+01\n",
            "   2.5756969e+01 -2.6413488e+01]\n",
            " [-2.1314085e-02 -2.6616613e+01 -5.5801600e-01  2.6233349e+01\n",
            "   2.6518951e+01 -3.8380238e-01  2.6653101e+01 -2.6708469e+01\n",
            "   2.6598835e+01 -2.5892889e+01]\n",
            " [-2.5421560e-02 -2.7004887e+01 -3.0029029e-01 -2.6368837e+01\n",
            "   2.6735916e+01 -4.6309143e-01  2.5901817e+01 -2.6637657e+01\n",
            "   2.5098476e+01 -2.6430708e+01]\n",
            " [-6.1106867e-01 -2.5964396e+01  1.0260642e-02  2.5917238e+01\n",
            "   2.7059221e+01  2.8789639e-02  2.6752811e+01 -2.6058294e+01\n",
            "   2.5062418e+01 -2.6571045e+01]]\n",
            "Layer 1 Biases: [  0.       -26.51386    0.        25.878239  26.512573   0.\n",
            "  26.513853 -26.351461  26.42079  -26.510305]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-2.5942616e+01 -9.8945260e-02 -1.8073559e-02  2.6066751e+01\n",
            "  -2.5883125e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-2.6548290e+01 -4.3066978e-01  1.2719393e-02  2.5978804e+01\n",
            "  -2.5780052e+01]\n",
            " [-2.6969450e+01 -3.5783455e-01 -2.4531659e-01  2.6901472e+01\n",
            "  -2.5711895e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-2.7089199e+01 -2.1949759e-01  2.4737239e-02  2.6858236e+01\n",
            "  -2.6624519e+01]\n",
            " [-2.6500912e+01  5.3160429e-02  1.9697559e-01  2.5924841e+01\n",
            "  -2.6044968e+01]\n",
            " [-2.5365261e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-2.6284487e+01 -1.1740291e-01 -3.1059518e-01  2.6229475e+01\n",
            "  -2.6187441e+01]]\n",
            "Layer 2 Biases: [-26.51476    0.         0.        26.508263 -26.41016 ]\n",
            "Layer 3 Weights: [[-2.60419579e+01 -3.97140771e-01 -5.05412519e-01 -2.63880672e+01\n",
            "  -5.99960327e-01 -2.61014061e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -2.58981438e+01  2.67446404e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-2.64593773e+01 -1.32262886e-01 -4.62086558e-01 -2.62109680e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  2.69622898e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  2.68020878e+01]\n",
            " [-2.24185143e+01 -2.99450517e-01  8.40138197e-02 -2.52445984e+01\n",
            "  -4.88672197e-01 -2.36008625e+01  3.82595837e-01  1.95535362e-01\n",
            "  -1.03679085e+01  5.46249247e+00]]\n",
            "Layer 3 Biases: [-26.515318   0.         0.       -26.515932   0.       -26.514961\n",
            "  26.515203   0.       -26.497643  26.504904]\n",
            "Layer 4 Weights: [[-26.07688    -26.303802  ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-25.911911   -25.972174  ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-26.477526   -26.005346  ]\n",
            " [-26.433613   -27.183933  ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-26.277882   -26.52646   ]\n",
            " [-27.054255   -26.338684  ]]\n",
            "Layer 4 Biases: [-26.515802 -26.516237]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.3686\n",
            "Epoch 6/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.3438\n",
            "Epoch 6 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -2.7331408e+01 -5.7062745e-01 -2.7597242e+01\n",
            "   2.8206039e+01 -4.4582593e-01  2.8071606e+01 -2.7131006e+01\n",
            "   2.6433050e+01 -2.7592915e+01]\n",
            " [-3.0537739e-01 -2.7088263e+01 -4.8943511e-01  2.7899256e+01\n",
            "   2.7284958e+01 -5.7949626e-01  2.7872040e+01 -2.6963676e+01\n",
            "   2.7108936e+01 -2.7352617e+01]\n",
            " [-1.0279006e-01 -2.7592863e+01  8.5736573e-02  2.7854374e+01\n",
            "   2.7219364e+01  1.2087101e-01  2.7697271e+01 -2.6839945e+01\n",
            "   2.6821314e+01 -2.7511412e+01]\n",
            " [-2.1314085e-02 -2.7714720e+01 -5.5801600e-01  2.7312010e+01\n",
            "   2.7616928e+01 -3.8380238e-01  2.7751207e+01 -2.7794460e+01\n",
            "   2.7688850e+01 -2.6990721e+01]\n",
            " [-2.5421560e-02 -2.8102898e+01 -3.0029029e-01 -2.7463484e+01\n",
            "   2.7833839e+01 -4.6309143e-01  2.6999825e+01 -2.7725199e+01\n",
            "   2.6147833e+01 -2.7528370e+01]\n",
            " [-6.1106867e-01 -2.7062500e+01  1.0260642e-02  2.7006294e+01\n",
            "   2.8157263e+01  2.8789639e-02  2.7850912e+01 -2.7148901e+01\n",
            "   2.6106800e+01 -2.7668856e+01]]\n",
            "Layer 1 Biases: [  0.       -27.612066   0.        26.94991   27.610725   0.\n",
            "  27.612059 -27.442886  27.51511  -27.608362]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-2.7040783e+01 -9.8945260e-02 -1.8073559e-02  2.7164019e+01\n",
            "  -2.6973207e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-2.7646442e+01 -4.3066978e-01  1.2719393e-02  2.7072542e+01\n",
            "  -2.6835199e+01]\n",
            " [-2.8067265e+01 -3.5783455e-01 -2.4531659e-01  2.7999287e+01\n",
            "  -2.6765278e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-2.8187332e+01 -2.1949759e-01  2.4737239e-02  2.7955303e+01\n",
            "  -2.7709278e+01]\n",
            " [-2.7597754e+01  5.3160429e-02  1.9697559e-01  2.7008244e+01\n",
            "  -2.7127169e+01]\n",
            " [-2.6393938e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-2.7382671e+01 -1.1740291e-01 -3.1059518e-01  2.7326868e+01\n",
            "  -2.7255720e+01]]\n",
            "Layer 2 Biases: [-27.612999   0.         0.        27.606236 -27.504034]\n",
            "Layer 3 Weights: [[-2.71393127e+01 -3.97140771e-01 -5.05412519e-01 -2.74859333e+01\n",
            "  -5.99960327e-01 -2.71990604e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -2.69870663e+01  2.78293476e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-2.75566368e+01 -1.32262886e-01 -4.62086558e-01 -2.73087883e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  2.80600777e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  2.78868237e+01]\n",
            " [-2.33703938e+01 -2.99450517e-01  8.40138197e-02 -2.62697334e+01\n",
            "  -4.88672197e-01 -2.45962181e+01  3.82595837e-01  1.95535362e-01\n",
            "  -1.07847710e+01  5.69931412e+00]]\n",
            "Layer 3 Biases: [-27.613585   0.         0.       -27.614225   0.       -27.613213\n",
            "  27.613466   0.       -27.59517   27.602736]\n",
            "Layer 4 Weights: [[-27.173292   -27.401455  ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-27.00783    -27.069645  ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-27.57355    -27.102861  ]\n",
            " [-27.529734   -28.281473  ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-27.373848   -27.623955  ]\n",
            " [-28.151741   -27.43671   ]]\n",
            "Layer 4 Biases: [-27.614088 -27.614542]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.3764 \n",
            "Epoch 7/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.4062\n",
            "Epoch 7 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -2.8087097e+01 -5.7062745e-01 -2.8350096e+01\n",
            "   2.8961662e+01 -4.4582593e-01  2.8827293e+01 -2.7881521e+01\n",
            "   2.7160553e+01 -2.8348389e+01]\n",
            " [-3.0537739e-01 -2.7844007e+01 -4.8943511e-01  2.8652359e+01\n",
            "   2.8040607e+01 -5.7949626e-01  2.8627785e+01 -2.7686680e+01\n",
            "   2.7856520e+01 -2.8108171e+01]\n",
            " [-1.0279006e-01 -2.8348629e+01  8.5736573e-02  2.8605827e+01\n",
            "   2.7975061e+01  1.2087101e-01  2.8453037e+01 -2.7583975e+01\n",
            "   2.7553764e+01 -2.8267014e+01]\n",
            " [-2.1314085e-02 -2.8470449e+01 -5.5801600e-01  2.8054331e+01\n",
            "   2.8372570e+01 -3.8380238e-01  2.8506937e+01 -2.8541838e+01\n",
            "   2.8439001e+01 -2.7746264e+01]\n",
            " [-2.5421560e-02 -2.8858562e+01 -3.0029029e-01 -2.8216829e+01\n",
            "   2.8589441e+01 -4.6309143e-01  2.7755487e+01 -2.8473644e+01\n",
            "   2.6869949e+01 -2.8283794e+01]\n",
            " [-6.1106867e-01 -2.7818230e+01  1.0260642e-02  2.7755783e+01\n",
            "   2.8912949e+01  2.8789639e-02  2.8606640e+01 -2.7899462e+01\n",
            "   2.6825489e+01 -2.8424383e+01]]\n",
            "Layer 1 Biases: [  0.       -28.367865   0.        27.68741   28.366488   0.\n",
            "  28.367857 -28.194012  28.268229 -28.36406 ]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-2.7796558e+01 -9.8945260e-02 -1.8073559e-02  2.7919170e+01\n",
            "  -2.7723404e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-2.8402205e+01 -4.3066978e-01  1.2719393e-02  2.7825262e+01\n",
            "  -2.7561308e+01]\n",
            " [-2.8822794e+01 -3.5783455e-01 -2.4531659e-01  2.8754816e+01\n",
            "  -2.7490171e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-2.8943079e+01 -2.1949759e-01  2.4737239e-02  2.8710316e+01\n",
            "  -2.8455803e+01]\n",
            " [-2.8352612e+01  5.3160429e-02  1.9697559e-01  2.7753834e+01\n",
            "  -2.7871931e+01]\n",
            " [-2.7101797e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-2.8138456e+01 -1.1740291e-01 -3.1059518e-01  2.8082108e+01\n",
            "  -2.7990883e+01]]\n",
            "Layer 2 Biases: [-28.368824   0.         0.        28.361874 -28.256845]\n",
            "Layer 3 Weights: [[-2.78945255e+01 -3.97140771e-01 -5.05412519e-01 -2.82414989e+01\n",
            "  -5.99960327e-01 -2.79544792e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -2.77364635e+01  2.85758381e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-2.83117809e+01 -1.32262886e-01 -4.62086558e-01 -2.80643215e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  2.88155899e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  2.86333351e+01]\n",
            " [-2.40253124e+01 -2.99450517e-01  8.40138197e-02 -2.69751492e+01\n",
            "  -4.88672197e-01 -2.52811050e+01  3.82595837e-01  1.95535362e-01\n",
            "  -1.10713043e+01  5.86204195e+00]]\n",
            "Layer 3 Biases: [-28.369427   0.         0.       -28.370083   0.       -28.369043\n",
            "  28.369305   0.       -28.3505    28.358278]\n",
            "Layer 4 Weights: [[-27.927855   -28.156872  ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-27.76205    -27.824938  ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-28.327843   -27.858185  ]\n",
            " [-28.284096   -29.036814  ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-28.128103   -28.379265  ]\n",
            " [-28.907043   -28.192385  ]]\n",
            "Layer 4 Biases: [-28.369946 -28.370409]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3931 \n",
            "Epoch 8/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.4375\n",
            "Epoch 8 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -2.8607813e+01 -5.7062745e-01 -2.8868855e+01\n",
            "   2.9482332e+01 -4.4582593e-01  2.9348009e+01 -2.8398666e+01\n",
            "   2.7661812e+01 -2.8868956e+01]\n",
            " [-3.0537739e-01 -2.8364763e+01 -4.8943511e-01  2.9171293e+01\n",
            "   2.8561298e+01 -5.7949626e-01  2.9148542e+01 -2.8184830e+01\n",
            "   2.8371639e+01 -2.8628796e+01]\n",
            " [-1.0279006e-01 -2.8869400e+01  8.5736573e-02  2.9123619e+01\n",
            "   2.8495785e+01  1.2087101e-01  2.8973808e+01 -2.8096642e+01\n",
            "   2.8058435e+01 -2.8787670e+01]\n",
            " [-2.1314085e-02 -2.8991192e+01 -5.5801600e-01  2.8565817e+01\n",
            "   2.8893253e+01 -3.8380238e-01  2.9027679e+01 -2.9056816e+01\n",
            "   2.8955893e+01 -2.8266876e+01]\n",
            " [-2.5421560e-02 -2.9379261e+01 -3.0029029e-01 -2.8735931e+01\n",
            "   2.9110100e+01 -4.6309143e-01  2.8276186e+01 -2.8989359e+01\n",
            "   2.7367489e+01 -2.8804325e+01]\n",
            " [-6.1106867e-01 -2.8338972e+01  1.0260642e-02  2.8272221e+01\n",
            "   2.9433664e+01  2.8789639e-02  2.9127382e+01 -2.8416636e+01\n",
            "   2.7320660e+01 -2.8944988e+01]]\n",
            "Layer 1 Biases: [  0.       -28.888657   0.        28.195572  28.887253   0.\n",
            "  28.888649 -28.711578  28.78717  -28.884783]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-2.8317333e+01 -9.8945260e-02 -1.8073559e-02  2.8439516e+01\n",
            "  -2.8240330e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-2.8922970e+01 -4.3066978e-01  1.2719393e-02  2.8343929e+01\n",
            "  -2.8061604e+01]\n",
            " [-2.9343401e+01 -3.5783455e-01 -2.4531659e-01  2.9275421e+01\n",
            "  -2.7989626e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-2.9463837e+01 -2.1949759e-01  2.4737239e-02  2.9230566e+01\n",
            "  -2.8970190e+01]\n",
            " [-2.8872755e+01  5.3160429e-02  1.9697559e-01  2.8267576e+01\n",
            "  -2.8385103e+01]\n",
            " [-2.7589493e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-2.8659239e+01 -1.1740291e-01 -3.1059518e-01  2.8602512e+01\n",
            "  -2.8497427e+01]]\n",
            "Layer 2 Biases: [-28.889635   0.         0.        28.882557 -28.775576]\n",
            "Layer 3 Weights: [[-2.84149132e+01 -3.97140771e-01 -5.05412519e-01 -2.87621288e+01\n",
            "  -5.99960327e-01 -2.84750099e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -2.82528343e+01  2.90902042e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-2.88321228e+01 -1.32262886e-01 -4.62086558e-01 -2.85849323e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  2.93361855e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  2.91477146e+01]\n",
            " [-2.44764709e+01 -2.99450517e-01  8.40138197e-02 -2.74611607e+01\n",
            "  -4.88672197e-01 -2.57529469e+01  3.82595837e-01  1.95535362e-01\n",
            "  -1.12684994e+01  5.97399616e+00]]\n",
            "Layer 3 Biases: [-28.89025    0.         0.       -28.890915   0.       -28.889858\n",
            "  28.890123   0.       -28.87097   28.878893]\n",
            "Layer 4 Weights: [[-28.447792   -28.677402  ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-28.281754   -28.34538   ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-28.847597   -28.37865   ]\n",
            " [-28.803896   -29.55729   ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-28.647831   -28.89972   ]\n",
            " [-29.427494   -28.71309   ]]\n",
            "Layer 4 Biases: [-28.890776 -28.891247]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.4014 \n",
            "Epoch 9/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.4375\n",
            "Epoch 9 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -2.8966368e+01 -5.7062745e-01 -2.9226061e+01\n",
            "   2.9840855e+01 -4.4582593e-01  2.9706562e+01 -2.8754755e+01\n",
            "   2.8006939e+01 -2.9227407e+01]\n",
            " [-3.0537739e-01 -2.8723343e+01 -4.8943511e-01  2.9528618e+01\n",
            "   2.8919836e+01 -5.7949626e-01  2.9507124e+01 -2.8527817e+01\n",
            "   2.8726336e+01 -2.8987286e+01]\n",
            " [-1.0279006e-01 -2.9227993e+01  8.5736573e-02  2.9480156e+01\n",
            "   2.8854345e+01  1.2087101e-01  2.9332401e+01 -2.8449644e+01\n",
            "   2.8405920e+01 -2.9146183e+01]\n",
            " [-2.1314085e-02 -2.9349768e+01 -5.5801600e-01  2.8918005e+01\n",
            "   2.9251785e+01 -3.8380238e-01  2.9386255e+01 -2.9411411e+01\n",
            "   2.9311810e+01 -2.8625360e+01]\n",
            " [-2.5421560e-02 -2.9737804e+01 -3.0029029e-01 -2.9093369e+01\n",
            "   2.9468613e+01 -4.6309143e-01  2.8634726e+01 -2.9344463e+01\n",
            "   2.7710054e+01 -2.9162752e+01]\n",
            " [-6.1106867e-01 -2.8697546e+01  1.0260642e-02  2.8627825e+01\n",
            "   2.9792217e+01  2.8789639e-02  2.9485954e+01 -2.8772747e+01\n",
            "   2.7661591e+01 -2.9303467e+01]]\n",
            "Layer 1 Biases: [  0.       -29.247265   0.        28.545464  29.245842   0.\n",
            "  29.247257 -29.067957  29.144503 -29.243341]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-2.8675926e+01 -9.8945260e-02 -1.8073559e-02  2.8797815e+01\n",
            "  -2.8596268e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-2.9281557e+01 -4.3066978e-01  1.2719393e-02  2.8701069e+01\n",
            "  -2.8406071e+01]\n",
            " [-2.9701881e+01 -3.5783455e-01 -2.4531659e-01  2.9633902e+01\n",
            "  -2.8333511e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-2.9822420e+01 -2.1949759e-01  2.4737239e-02  2.9588799e+01\n",
            "  -2.9324381e+01]\n",
            " [-2.9230915e+01  5.3160429e-02  1.9697559e-01  2.8621319e+01\n",
            "  -2.8738451e+01]\n",
            " [-2.7925268e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-2.9017839e+01 -1.1740291e-01 -3.1059518e-01  2.8960852e+01\n",
            "  -2.8846205e+01]]\n",
            "Layer 2 Biases: [-29.248255   0.         0.        29.241089 -29.132761]\n",
            "Layer 3 Weights: [[-2.87732410e+01 -3.97140771e-01 -5.05412519e-01 -2.91206245e+01\n",
            "  -5.99960327e-01 -2.88334351e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -2.86083927e+01  2.94443779e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-2.91904182e+01 -1.32262886e-01 -4.62086558e-01 -2.89434128e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  2.96946564e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  2.95018978e+01]\n",
            " [-2.47870464e+01 -2.99450517e-01  8.40138197e-02 -2.77957745e+01\n",
            "  -4.88672197e-01 -2.60777855e+01  3.82595837e-01  1.95535362e-01\n",
            "  -1.14041128e+01  6.05096388e+00]]\n",
            "Layer 3 Biases: [-29.248877   0.         0.       -29.24955    0.       -29.24848\n",
            "  29.24875    0.       -29.229355  29.237377]\n",
            "Layer 4 Weights: [[-28.80581    -29.035828  ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-28.639606   -28.703747  ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-29.205486   -28.73703   ]\n",
            " [-29.16182    -29.915676  ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-29.005703   -29.258093  ]\n",
            " [-29.785866   -29.071638  ]]\n",
            "Layer 4 Biases: [-29.24941  -29.249886]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.3941 \n",
            "Epoch 10/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 1.0000 - loss: 0.4219\n",
            "Epoch 10 ended\n",
            "Loss: 0.38499999046325684, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.5172541e-02 -2.9212843e+01 -5.7062745e-01 -2.9471605e+01\n",
            "   3.0087307e+01 -4.4582593e-01  2.9953037e+01 -2.8999531e+01\n",
            "   2.8244165e+01 -2.9473812e+01]\n",
            " [-3.0537739e-01 -2.8969835e+01 -4.8943511e-01  2.9774244e+01\n",
            "   2.9166300e+01 -5.7949626e-01  2.9753616e+01 -2.8763571e+01\n",
            "   2.8970152e+01 -2.9233715e+01]\n",
            " [-1.0279006e-01 -2.9474493e+01  8.5736573e-02  2.9725241e+01\n",
            "   2.9100822e+01  1.2087101e-01  2.9578901e+01 -2.8692295e+01\n",
            "   2.8644770e+01 -2.9392630e+01]\n",
            " [-2.1314085e-02 -2.9596256e+01 -5.5801600e-01  2.9160093e+01\n",
            "   2.9498243e+01 -3.8380238e-01  2.9632744e+01 -2.9655159e+01\n",
            "   2.9556467e+01 -2.8871786e+01]\n",
            " [-2.5421560e-02 -2.9984270e+01 -3.0029029e-01 -2.9339073e+01\n",
            "   2.9715059e+01 -4.6309143e-01  2.8881191e+01 -2.9588562e+01\n",
            "   2.7945517e+01 -2.9409140e+01]\n",
            " [-6.1106867e-01 -2.8944035e+01  1.0260642e-02  2.8872267e+01\n",
            "   3.0038691e+01  2.8789639e-02  2.9732441e+01 -2.9017536e+01\n",
            "   2.7895927e+01 -2.9549889e+01]]\n",
            "Layer 1 Biases: [  0.       -29.493774   0.        28.78597   29.492342   0.\n",
            "  29.493767 -29.312933  29.390135 -29.489819]\n",
            "Layer 2 Weights: [[-7.7214897e-02 -3.3341366e-01 -5.5683178e-01  1.4162570e-01\n",
            "  -6.2249213e-01]\n",
            " [-2.8922428e+01 -9.8945260e-02 -1.8073559e-02  2.9044113e+01\n",
            "  -2.8840939e+01]\n",
            " [ 1.6185737e-01 -4.8623031e-01  5.3599733e-01  5.2371997e-01\n",
            "   1.1704993e-01]\n",
            " [-2.9528057e+01 -4.3066978e-01  1.2719393e-02  2.8946569e+01\n",
            "  -2.8642841e+01]\n",
            " [-2.9948303e+01 -3.5783455e-01 -2.4531659e-01  2.9880323e+01\n",
            "  -2.8569881e+01]\n",
            " [-3.9863041e-01  2.9604346e-01  4.0776044e-01 -1.7749271e-01\n",
            "  -4.9390125e-01]\n",
            " [-3.0068914e+01 -2.1949759e-01  2.4737239e-02  2.9835051e+01\n",
            "  -2.9567850e+01]\n",
            " [-2.9477118e+01  5.3160429e-02  1.9697559e-01  2.8864481e+01\n",
            "  -2.8981342e+01]\n",
            " [-2.8156055e+01 -2.7250072e-01 -3.3895561e-01  2.8723705e-01\n",
            "  -2.8698856e-01]\n",
            " [-2.9264347e+01 -1.1740291e-01 -3.1059518e-01  2.9207176e+01\n",
            "  -2.9085945e+01]]\n",
            "Layer 2 Biases: [-29.494772   0.         0.        29.487547 -29.37829 ]\n",
            "Layer 3 Weights: [[-2.90195580e+01 -3.97140771e-01 -5.05412519e-01 -2.93670597e+01\n",
            "  -5.99960327e-01 -2.90798206e+01 -1.92250580e-01 -1.41737580e-01\n",
            "  -2.88528023e+01  2.96878338e+01]\n",
            " [-1.07543588e-01 -3.30293387e-01 -4.55257773e-01 -4.29511577e-01\n",
            "   2.41693318e-01 -2.13461518e-02  4.82646644e-01 -3.47520143e-01\n",
            "   1.07073784e-02 -4.00155187e-01]\n",
            " [-4.84834313e-01  3.30760539e-01 -4.91448998e-01  1.03031993e-01\n",
            "  -5.99264145e-01  4.80406225e-01  1.16404235e-01 -5.57670474e-01\n",
            "  -5.96706748e-01 -7.10284710e-02]\n",
            " [-2.94367142e+01 -1.32262886e-01 -4.62086558e-01 -2.91898346e+01\n",
            "  -4.72048163e-01 -1.95465863e-01  2.99410706e+01 -1.92802638e-01\n",
            "  -5.15711546e-01  2.97453613e+01]\n",
            " [-2.50004807e+01 -2.99450517e-01  8.40138197e-02 -2.80257607e+01\n",
            "  -4.88672197e-01 -2.63010426e+01  3.82595837e-01  1.95535362e-01\n",
            "  -1.14972200e+01  6.10378933e+00]]\n",
            "Layer 3 Biases: [-29.495401   0.         0.       -29.49608    0.       -29.495\n",
            "  29.495275   0.       -29.475712  29.483805]\n",
            "Layer 4 Weights: [[-29.051916   -29.282213  ]\n",
            " [  0.4124568    0.05475062]\n",
            " [  0.47391933  -0.10882467]\n",
            " [-28.8856     -28.95009   ]\n",
            " [ -0.3800978   -0.44502166]\n",
            " [-29.451504   -28.983385  ]\n",
            " [-29.407858   -30.162035  ]\n",
            " [ -0.5335108    0.43858343]\n",
            " [-29.251707   -29.504442  ]\n",
            " [-30.032215   -29.318108  ]]\n",
            "Layer 4 Biases: [-29.49594  -29.496418]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.3962 \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_folder: /content/DATA/run_5/original_result\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │              <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m70\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │              \u001b[38;5;34m55\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │              \u001b[38;5;34m60\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m22\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207</span> (828.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207\u001b[0m (828.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.4375 - loss: 1.0094\n",
            "Epoch 1 ended\n",
            "Loss: 0.5829929113388062, Accuracy: 0.5\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.32639713e+01  1.31128874e+01 -1.35135956e+01 -1.29622993e+01\n",
            "   1.31059303e+01  1.25227814e+01 -1.34134941e+01 -1.29685993e+01\n",
            "  -1.03060305e-01 -1.27705021e+01]\n",
            " [ 1.36229944e+01  1.32797508e+01 -1.30534096e+01 -1.26983852e+01\n",
            "  -1.34360485e+01  1.24348660e+01 -1.25892143e+01 -1.31021805e+01\n",
            "  -1.02872252e-02 -1.34700298e+01]\n",
            " [ 1.25933371e+01  1.36090527e+01 -1.30649757e+01 -1.25123119e+01\n",
            "   1.21189594e+01  1.29050083e+01 -1.32300224e+01 -1.29939251e+01\n",
            "  -2.26402253e-01 -1.33955059e+01]\n",
            " [ 1.33256464e+01  1.30437841e+01 -1.25041685e+01 -1.33943224e+01\n",
            "  -1.26604137e+01  1.33828869e+01 -1.31771240e+01 -1.27426805e+01\n",
            "  -5.79387605e-01 -1.34418707e+01]\n",
            " [ 1.33544064e+01  1.25042791e+01 -1.27848587e+01 -1.33241081e+01\n",
            "   1.28749561e+01  1.30499153e+01 -1.26247540e+01 -1.27696075e+01\n",
            "  -5.47566295e-01 -1.25215168e+01]\n",
            " [ 1.24701633e+01  1.27084236e+01 -1.25525475e+01 -1.30855694e+01\n",
            "   1.27036076e+01  1.34285717e+01 -1.26667805e+01 -1.30055676e+01\n",
            "   1.12144649e-01 -1.30558262e+01]]\n",
            "Layer 1 Biases: [ 13.059834  13.060091 -13.059608 -13.060304  13.024182  13.057275\n",
            " -13.060915 -13.060675   0.       -13.051041]\n",
            "Layer 2 Weights: [[-13.59545      0.15450007 -13.59227    -12.477987   -12.578117  ]\n",
            " [-12.864334    -0.5864576  -13.543143   -13.296717   -12.675997  ]\n",
            " [-13.167936    -0.44153732 -12.8890085   12.911533   -13.147008  ]\n",
            " [-13.053618    -0.38373712 -12.84601    -12.582506   -12.999399  ]\n",
            " [-12.349963     0.2962734  -13.322334     0.56326336  -0.52018124]\n",
            " [-13.282723    -0.39181957 -13.542828    12.200073    -0.14492029]\n",
            " [-13.1027155   -0.19537267 -12.47942     13.441624   -13.304089  ]\n",
            " [-12.571839     0.26102448 -12.74843    -13.640211   -13.471797  ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 12.173654     0.04422677 -12.6494255   12.839255     0.55526584]]\n",
            "Layer 2 Biases: [-13.060325   0.       -13.061136 -13.054831 -13.059756]\n",
            "Layer 3 Weights: [[-13.6216545  -12.8102865  -12.683329   -12.667066   -12.826678\n",
            "   -0.57720083 -13.571217    13.049639    -0.34213665 -13.544741  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-12.804629   -12.712112   -12.769904   -12.558742   -13.490274\n",
            "   -0.12242901 -12.919074    13.379608    12.455481   -12.998344  ]\n",
            " [-13.587442   -13.63251    -12.497317   -12.5755005   -0.31219476\n",
            "   -0.23284167 -13.092339    12.991279    13.46508    -13.038471  ]\n",
            " [ -0.30762297 -13.434599   -12.538188   -13.48405    -13.07461\n",
            "    0.27593887 -12.546579    12.509921    12.912063   -12.78307   ]]\n",
            "Layer 3 Biases: [-13.060944 -13.061176 -13.061129 -13.060985 -13.053868   0.\n",
            " -13.061     13.060937  13.056227 -13.060352]\n",
            "Layer 4 Weights: [[-12.556569   -12.566416  ]\n",
            " [-12.454303   -12.394659  ]\n",
            " [-12.4935665  -12.754106  ]\n",
            " [-12.784388   -12.819567  ]\n",
            " [-12.3720045  -12.438333  ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-12.967849   -12.556973  ]\n",
            " [-13.583759   -13.197026  ]\n",
            " [-13.174907   -13.193011  ]\n",
            " [-13.472404   -12.412286  ]]\n",
            "Layer 4 Biases: [-13.061065 -13.061238]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4271 - loss: 0.6903\n",
            "Epoch 2/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.4219\n",
            "Epoch 2 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 1.91370468e+01  1.89885349e+01 -1.93888607e+01 -1.88391991e+01\n",
            "   1.88168411e+01  1.83913918e+01 -1.92907696e+01 -1.88454781e+01\n",
            "  -1.03060305e-01 -1.86341553e+01]\n",
            " [ 1.94992046e+01  1.91561775e+01 -1.89293613e+01 -1.85752487e+01\n",
            "  -1.92205544e+01  1.83049240e+01 -1.84665279e+01 -1.89787331e+01\n",
            "  -1.02872252e-02 -1.93004265e+01]\n",
            " [ 1.84684105e+01  1.94860649e+01 -1.89410114e+01 -1.83892117e+01\n",
            "   1.77373600e+01  1.87781277e+01 -1.91073608e+01 -1.88711452e+01\n",
            "  -2.26402253e-01 -1.92444172e+01]\n",
            " [ 1.92022133e+01  1.89203491e+01 -1.83801308e+01 -1.92710114e+01\n",
            "  -1.82897568e+01  1.92583733e+01 -1.90544338e+01 -1.86197891e+01\n",
            "  -5.79387605e-01 -1.92176743e+01]\n",
            " [ 1.92313290e+01  1.83805618e+01 -1.86610928e+01 -1.92006741e+01\n",
            "   1.87187729e+01  1.89133053e+01 -1.85020504e+01 -1.86467495e+01\n",
            "  -5.47566295e-01 -1.83900757e+01]\n",
            " [ 1.83470573e+01  1.85850697e+01 -1.84291229e+01 -1.89622631e+01\n",
            "   1.84652004e+01  1.93027725e+01 -1.85441628e+01 -1.88828583e+01\n",
            "   1.12144649e-01 -1.89166660e+01]]\n",
            "Layer 1 Biases: [ 18.936884  18.93726  -18.936558 -18.937569  18.88516   18.93317\n",
            " -18.938456 -18.938107   0.       -18.924128]\n",
            "Layer 2 Weights: [[-19.44228      0.15450007 -19.469202   -18.352903   -18.454435  ]\n",
            " [-18.73898     -0.5864576  -19.419453   -19.169506   -18.549784  ]\n",
            " [-19.044102    -0.44153732 -18.766533    18.786736   -19.02144   ]\n",
            " [-18.929499    -0.38373712 -18.723244   -18.457275   -18.874756  ]\n",
            " [-18.117039     0.2962734  -19.153875     0.56326336  -0.52018124]\n",
            " [-19.126549    -0.39181957 -19.414389    17.952854    -0.14492029]\n",
            " [-18.965881    -0.19537267 -18.356865    19.3163     -19.178911  ]\n",
            " [-18.448414     0.26102448 -18.625889   -19.496912   -19.34722   ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 17.843887     0.04422677 -18.5168      18.675995     0.55526584]]\n",
            "Layer 2 Biases: [-18.9376     0.       -18.938774 -18.929626 -18.936771]\n",
            "Layer 3 Weights: [[-19.495865   -18.687035   -18.559479   -18.542313   -18.690258\n",
            "   -0.57720083 -19.443615    18.924307    -0.34213665 -19.380861  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-18.681898   -18.589596   -18.64724    -18.435862   -19.280602\n",
            "   -0.12242901 -18.796246    19.25673     18.295906   -18.87488   ]\n",
            " [-19.461197   -19.508451   -18.373869   -18.451408    -0.31219476\n",
            "   -0.23284167 -18.968826    18.866522    19.336155   -18.913668  ]\n",
            " [ -0.30762297 -19.26159    -18.411419   -19.353441   -18.857126\n",
            "    0.27593887 -18.419422    18.321945    18.771816   -18.415344  ]]\n",
            "Layer 3 Biases: [-18.938498 -18.938833 -18.938765 -18.938555 -18.92823    0.\n",
            " -18.93858   18.938486  18.93165  -18.93764 ]\n",
            "Layer 4 Weights: [[-18.428007   -18.441145  ]\n",
            " [-18.330242   -18.27168   ]\n",
            " [-18.370462   -18.631496  ]\n",
            " [-18.661411   -18.697006  ]\n",
            " [-17.994375   -18.22454   ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-18.840328   -18.43232   ]\n",
            " [-19.459728   -19.07402   ]\n",
            " [-19.041018   -19.066143  ]\n",
            " [-19.332256   -18.280664  ]]\n",
            "Layer 4 Biases: [-18.938671 -18.938925]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3829 \n",
            "Epoch 3/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.4062\n",
            "Epoch 3 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 2.27372684e+01  2.25903358e+01 -2.29904308e+01 -2.24417706e+01\n",
            "   2.23174610e+01  2.19888706e+01 -2.28935699e+01 -2.24480362e+01\n",
            "  -1.03060305e-01 -2.22285919e+01]\n",
            " [ 2.31013546e+01  2.27584572e+01 -2.25313511e+01 -2.21777992e+01\n",
            "  -2.27663746e+01  2.19032936e+01 -2.20693531e+01 -2.25810909e+01\n",
            "  -1.02872252e-02 -2.28744335e+01]\n",
            " [ 2.20698624e+01  2.30887070e+01 -2.25430527e+01 -2.19917850e+01\n",
            "   2.11811638e+01  2.23783779e+01 -2.27102013e+01 -2.24739132e+01\n",
            "  -2.26402253e-01 -2.28297977e+01]\n",
            " [ 2.28045826e+01  2.25227146e+01 -2.19821281e+01 -2.28734531e+01\n",
            "  -2.17402802e+01  2.28600750e+01 -2.26572590e+01 -2.22224903e+01\n",
            "  -5.79387605e-01 -2.27581501e+01]\n",
            " [ 2.28339157e+01  2.19827576e+01 -2.22632542e+01 -2.28030396e+01\n",
            "   2.23010254e+01  2.25075817e+01 -2.21048679e+01 -2.22494698e+01\n",
            "  -5.47566295e-01 -2.19875240e+01]\n",
            " [ 2.19496269e+01  2.21874847e+01 -2.20314980e+01 -2.25647068e+01\n",
            "   2.19969501e+01  2.29036846e+01 -2.21470337e+01 -2.24856701e+01\n",
            "   1.12144649e-01 -2.25093727e+01]]\n",
            "Layer 1 Biases: [ 22.539549  22.539995 -22.539162 -22.540363  22.477951  22.535126\n",
            " -22.541418 -22.541004   0.       -22.524357]\n",
            "Layer 2 Weights: [[-23.026379     0.15450007 -23.071795   -21.954256   -22.05665   ]\n",
            " [-22.340168    -0.5864576  -23.02166    -22.76955    -22.150446  ]\n",
            " [-22.646223    -0.44153732 -22.369488    22.388266   -22.622498  ]\n",
            " [-22.531445    -0.38373712 -22.326021   -22.058538   -22.476383  ]\n",
            " [-21.652155     0.2962734  -22.728586     0.56326336  -0.52018124]\n",
            " [-22.708807    -0.39181957 -23.013681    21.479189    -0.14492029]\n",
            " [-22.560017    -0.19537267 -21.95977     22.917503   -22.780209  ]\n",
            " [-22.050787     0.26102448 -22.228806   -23.087074   -22.948885  ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 21.319523     0.04422677 -22.11352     22.2539       0.55526584]]\n",
            "Layer 2 Biases: [-22.5404     0.       -22.541801 -22.530907 -22.539415]\n",
            "Layer 3 Weights: [[-23.096786   -22.289513   -22.161589   -22.143867   -22.284649\n",
            "   -0.57720083 -23.04342     22.52551     -0.34213665 -22.958385  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-22.284698   -22.192526   -22.250078   -22.03857    -22.829998\n",
            "   -0.12242901 -22.398981    22.859438    21.876072   -22.47723   ]\n",
            " [-23.061838   -23.110437   -21.976227   -22.05337     -0.31219476\n",
            "   -0.23284167 -22.571146    22.468077    22.93515    -22.515192  ]\n",
            " [ -0.30762297 -22.833506   -22.011738   -22.9514     -22.401728\n",
            "    0.27593887 -22.019503    21.884668    22.363855   -21.86767   ]]\n",
            "Layer 3 Biases: [-22.541471 -22.54187  -22.541792 -22.541538 -22.529243   0.\n",
            " -22.541569  22.541456  22.533316 -22.540447]\n",
            "Layer 4 Weights: [[-22.027227   -22.042383  ]\n",
            " [-21.932222   -21.874329  ]\n",
            " [-21.973032   -22.23437   ]\n",
            " [-22.264057   -22.29991   ]\n",
            " [-21.440617   -21.77141   ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-22.440184   -22.033937  ]\n",
            " [-23.061728   -22.67665   ]\n",
            " [-22.636963   -22.666397  ]\n",
            " [-22.924356   -21.878002  ]]\n",
            "Layer 4 Biases: [-22.54168  -22.541979]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.3798 \n",
            "Epoch 4/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.3906\n",
            "Epoch 4 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 2.51058807e+01  2.49599895e+01 -2.53599300e+01 -2.48119297e+01\n",
            "   2.46204147e+01  2.43556728e+01 -2.52638817e+01 -2.48181877e+01\n",
            "  -1.03060305e-01 -2.45933914e+01]\n",
            " [ 2.54712353e+01  2.51284256e+01 -2.49011288e+01 -2.45479450e+01\n",
            "  -2.50991249e+01  2.42706833e+01 -2.44396820e+01 -2.49511108e+01\n",
            "  -1.02872252e-02 -2.52257652e+01]\n",
            " [ 2.44392815e+01  2.54589138e+01 -2.49128647e+01 -2.43619442e+01\n",
            "   2.34466686e+01  2.47470074e+01 -2.50805397e+01 -2.48442039e+01\n",
            "  -2.26402253e-01 -2.51886234e+01]\n",
            " [ 2.51746082e+01  2.48927402e+01 -2.43519115e+01 -2.52435284e+01\n",
            "  -2.40102158e+01  2.52296638e+01 -2.50275879e+01 -2.45927353e+01\n",
            "  -5.79387605e-01 -2.50873756e+01]\n",
            " [ 2.52040863e+01  2.43526688e+01 -2.46331482e+01 -2.51730652e+01\n",
            "   2.46577911e+01  2.48722706e+01 -2.44751892e+01 -2.46197281e+01\n",
            "  -5.47566295e-01 -2.43543072e+01]\n",
            " [ 2.43197861e+01  2.45575428e+01 -2.44015312e+01 -2.49347858e+01\n",
            "   2.43204250e+01  2.52727528e+01 -2.45173893e+01 -2.48559895e+01\n",
            "   1.12144649e-01 -2.48730316e+01]]\n",
            "Layer 1 Biases: [ 24.909773  24.910263 -24.909342 -24.910673  24.841663  24.904882\n",
            " -24.911839 -24.91138    0.       -24.892975]\n",
            "Layer 2 Weights: [[-25.384363     0.15450007 -25.441969   -24.323612   -24.426577  ]\n",
            " [-24.709417    -0.5864576  -25.39158    -25.138046   -24.519348  ]\n",
            " [-25.016087    -0.44153732 -24.739902    24.757738   -24.99166   ]\n",
            " [-24.901194    -0.38373712 -24.696318   -24.427835   -24.84592   ]\n",
            " [-23.97785      0.2962734  -25.080381     0.56326336  -0.52018124]\n",
            " [-25.065575    -0.39181957 -25.38168     23.799097    -0.14492029]\n",
            " [-24.924618    -0.19537267 -24.330153    25.286762   -25.149529  ]\n",
            " [-24.420815     0.26102448 -24.599194   -25.449059   -25.318449  ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 23.606012     0.04422677 -24.479824    24.6078       0.55526584]]\n",
            "Layer 2 Biases: [-24.910711   0.       -24.912262 -24.900215 -24.909626]\n",
            "Layer 3 Weights: [[-25.46586    -24.65961    -24.531445   -24.51336    -24.649416\n",
            "   -0.57720083 -25.411757    24.89477     -0.34213665 -25.312035  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-24.655008   -24.562923   -24.620417   -24.408821   -25.165104\n",
            "   -0.12242901 -24.769249    25.229689    24.231464   -24.847244  ]\n",
            " [-25.430725   -25.480211   -24.346245   -24.423128    -0.31219476\n",
            "   -0.23284167 -24.94114     24.837568    25.302952   -24.884663  ]\n",
            " [ -0.30762297 -25.183458   -24.380413   -25.318521   -24.733675\n",
            "    0.27593887 -24.388023    24.22856     24.727074   -24.138792  ]]\n",
            "Layer 3 Biases: [-24.911898 -24.912338 -24.912252 -24.911972 -24.898378   0.\n",
            " -24.912004  24.91188   24.90288  -24.910765]\n",
            "Layer 4 Weights: [[-24.395178   -24.411663  ]\n",
            " [-24.301992   -24.24454   ]\n",
            " [-24.343191   -24.604729  ]\n",
            " [-24.634268   -24.670288  ]\n",
            " [-23.70773    -24.104849  ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-24.808556   -24.403467  ]\n",
            " [-25.43151    -25.046848  ]\n",
            " [-25.002756   -25.035032  ]\n",
            " [-25.287615   -24.244713  ]]\n",
            "Layer 4 Biases: [-24.912128 -24.912458]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.3808 \n",
            "Epoch 5/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.2969\n",
            "Epoch 5 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 2.67083607e+01  2.65631752e+01 -2.69630127e+01 -2.64154568e+01\n",
            "   2.61783886e+01  2.59569263e+01 -2.68675137e+01 -2.64217110e+01\n",
            "  -1.03060305e-01 -2.61932831e+01]\n",
            " [ 2.70745754e+01  2.67318249e+01 -2.65043983e+01 -2.61514645e+01\n",
            "  -2.66772957e+01  2.58723354e+01 -2.60433254e+01 -2.65545464e+01\n",
            "  -1.02872252e-02 -2.68165302e+01]\n",
            " [ 2.60423107e+01  2.70624733e+01 -2.65161572e+01 -2.59654732e+01\n",
            "   2.49792595e+01  2.63494987e+01 -2.66841908e+01 -2.64478188e+01\n",
            "  -2.26402253e-01 -2.67844715e+01]\n",
            " [ 2.67780457e+01  2.64961777e+01 -2.59551830e+01 -2.68469982e+01\n",
            "  -2.55458088e+01  2.68328037e+01 -2.66312313e+01 -2.61963234e+01\n",
            "  -5.79387605e-01 -2.66631565e+01]\n",
            " [ 2.68076210e+01  2.59560299e+01 -2.62364960e+01 -2.67765026e+01\n",
            "   2.62522411e+01  2.64720936e+01 -2.60788288e+01 -2.62233238e+01\n",
            "  -5.47566295e-01 -2.59555473e+01]\n",
            " [ 2.59233131e+01  2.61610050e+01 -2.60049725e+01 -2.65382595e+01\n",
            "   2.58923073e+01  2.68755417e+01 -2.61210499e+01 -2.64596272e+01\n",
            "   1.12144649e-01 -2.64721527e+01]]\n",
            "Layer 1 Biases: [ 26.513344  26.513866 -26.512884 -26.514303  26.44082   26.508137\n",
            " -26.515543 -26.515057   0.       -26.495459]\n",
            "Layer 2 Weights: [[-26.979637     0.15450007 -27.045507   -25.926598   -26.029945  ]\n",
            " [-26.312328    -0.5864576  -26.994947   -26.740446   -26.122025  ]\n",
            " [-26.619415    -0.44153732 -26.343605    26.360804   -26.594511  ]\n",
            " [-26.504444    -0.38373712 -26.29994    -26.030779   -26.449026  ]\n",
            " [-25.551237     0.2962734  -26.671461     0.56326336  -0.52018124]\n",
            " [-26.660027    -0.39181957 -26.983744    25.36856     -0.14492029]\n",
            " [-26.524376    -0.19537267 -25.933832    26.88968    -26.752487  ]\n",
            " [-26.024254     0.26102448 -26.202877   -27.047045   -26.921574  ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 25.152824     0.04422677 -26.08074     26.200304     0.55526584]]\n",
            "Layer 2 Biases: [-26.514343   0.       -26.515995 -26.503166 -26.51319 ]\n",
            "Layer 3 Weights: [[-27.068651   -26.263098   -26.134768   -26.116434   -26.249289\n",
            "   -0.57720083 -27.01405     26.497686    -0.34213665 -26.904371  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-26.258638   -26.166613   -26.224066   -26.012411   -26.74487\n",
            "   -0.12242901 -26.372852    26.833279    25.824984   -26.450674  ]\n",
            " [-27.03339    -27.083479   -25.94968    -26.026386    -0.31219476\n",
            "   -0.23284167 -26.544556    26.440643    26.904884   -26.487722  ]\n",
            " [ -0.30762297 -26.773287   -25.982937   -26.919989   -26.311298\n",
            "    0.27593887 -25.99044     25.81428     26.325895   -25.67519   ]]\n",
            "Layer 3 Biases: [-26.515608 -26.516075 -26.515985 -26.515684 -26.501211   0.\n",
            " -26.51572   26.51559   26.506004 -26.5144  ]\n",
            "Layer 4 Weights: [[-25.997208   -26.014597  ]\n",
            " [-25.905258   -25.848103  ]\n",
            " [-25.946718   -26.208393  ]\n",
            " [-26.237831   -26.273964  ]\n",
            " [-25.241407   -25.683487  ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-26.410872   -26.00657   ]\n",
            " [-27.034784   -26.650404  ]\n",
            " [-26.603327   -26.637526  ]\n",
            " [-26.886463   -25.845903  ]]\n",
            "Layer 4 Biases: [-26.515852 -26.516205]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3470 \n",
            "Epoch 6/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.4219\n",
            "Epoch 6 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 2.78057957e+01  2.76610947e+01 -2.80608597e+01 -2.75136089e+01\n",
            "   2.72452850e+01  2.70535183e+01 -2.79657402e+01 -2.75198631e+01\n",
            "  -1.03060305e-01 -2.72889442e+01]\n",
            " [ 2.81726017e+01  2.78298893e+01 -2.76023750e+01 -2.72496147e+01\n",
            "  -2.77580509e+01  2.69692001e+01 -2.71415577e+01 -2.76526356e+01\n",
            "  -1.02872252e-02 -2.79059277e+01]\n",
            " [ 2.71401215e+01  2.81606483e+01 -2.76141491e+01 -2.70636253e+01\n",
            "   2.60287399e+01  2.74469433e+01 -2.77824306e+01 -2.75460339e+01\n",
            "  -2.26402253e-01 -2.78773556e+01]\n",
            " [ 2.78761387e+01  2.75942707e+01 -2.70531616e+01 -2.79451141e+01\n",
            "  -2.65973473e+01  2.79306908e+01 -2.77294636e+01 -2.72945175e+01\n",
            "  -5.79387605e-01 -2.77422714e+01]\n",
            " [ 2.79057808e+01  2.70540695e+01 -2.73345261e+01 -2.78745956e+01\n",
            "   2.73441639e+01  2.75677052e+01 -2.71770611e+01 -2.73215256e+01\n",
            "  -5.47566295e-01 -2.70521317e+01]\n",
            " [ 2.70214653e+01  2.72591133e+01 -2.71030674e+01 -2.76363754e+01\n",
            "   2.69687481e+01  2.79731884e+01 -2.72192974e+01 -2.75578556e+01\n",
            "   1.12144649e-01 -2.75672836e+01]]\n",
            "Layer 1 Biases: [ 27.611528  27.612072 -27.611052 -27.612528  27.535976  27.606104\n",
            " -27.613817 -27.613312   0.       -27.592896]\n",
            "Layer 2 Weights: [[-28.072128     0.15450007 -28.143667   -27.024378   -27.12799   ]\n",
            " [-27.410059    -0.5864576  -28.092993   -27.837828   -27.219593  ]\n",
            " [-27.717434    -0.44153732 -27.44188     27.45864    -27.692204  ]\n",
            " [-27.60241     -0.38373712 -27.398159   -27.128534   -27.546892  ]\n",
            " [-26.62871      0.2962734  -27.761074     0.56326336  -0.52018124]\n",
            " [-27.751951    -0.39181957 -28.080893    26.44334     -0.14492029]\n",
            " [-27.619944    -0.19537267 -27.03209     27.987415   -27.850252  ]\n",
            " [-27.122349     0.26102448 -27.301138   -28.141396   -28.019451  ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 26.212063     0.04422677 -27.1771      27.290894     0.55526584]]\n",
            "Layer 2 Biases: [-27.612568   0.       -27.614288 -27.600925 -27.611366]\n",
            "Layer 3 Weights: [[-28.166298   -27.361225   -27.23278    -27.21428    -27.344934\n",
            "   -0.57720083 -28.111357    27.59542     -0.34213665 -27.994844  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-27.356863   -27.264877   -27.322306   -27.11061    -27.82672\n",
            "   -0.12242901 -27.471058    27.931477    26.91627    -27.548761  ]\n",
            " [-28.130953   -28.181454   -27.047768   -27.124353    -0.31219476\n",
            "   -0.23284167 -27.642635    27.538488    28.001944   -27.585558  ]\n",
            " [ -0.30762297 -27.862043   -27.080402   -28.01673    -27.391678\n",
            "    0.27593887 -27.087831    26.900215    27.42082    -26.727282  ]]\n",
            "Layer 3 Biases: [-27.613886 -27.614374 -27.614279 -27.613966 -27.598888   0.\n",
            " -27.614004  27.613869  27.603882 -27.612629]\n",
            "Layer 4 Weights: [[-27.094334   -27.112345  ]\n",
            " [-27.003233   -26.946281  ]\n",
            " [-27.04487    -27.30664   ]\n",
            " [-27.33601    -27.372221  ]\n",
            " [-26.291636   -26.764563  ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-27.508194   -27.104431  ]\n",
            " [-28.132765   -27.748577  ]\n",
            " [-27.699451   -27.73497   ]\n",
            " [-27.981407   -26.942455  ]]\n",
            "Layer 4 Biases: [-27.614141 -27.61451 ]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.3740 \n",
            "Epoch 7/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 7 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 2.85610657e+01  2.84166965e+01 -2.88164101e+01 -2.82693710e+01\n",
            "   2.79794960e+01  2.78082047e+01 -2.87215519e+01 -2.82756252e+01\n",
            "  -1.03060305e-01 -2.80429859e+01]\n",
            " [ 2.89282761e+01  2.85855923e+01 -2.83580170e+01 -2.80053749e+01\n",
            "  -2.85018158e+01  2.77240753e+01 -2.78973751e+01 -2.84083538e+01\n",
            "  -1.02872252e-02 -2.86556530e+01]\n",
            " [ 2.78956490e+01  2.89164257e+01 -2.83698006e+01 -2.78193874e+01\n",
            "   2.67509403e+01  2.82022171e+01 -2.85382538e+01 -2.83018398e+01\n",
            "  -2.26402253e-01 -2.86294842e+01]\n",
            " [ 2.86318607e+01  2.83499908e+01 -2.78088036e+01 -2.87008514e+01\n",
            "  -2.73209686e+01  2.86862717e+01 -2.84852810e+01 -2.80503082e+01\n",
            "  -5.79387605e-01 -2.84849072e+01]\n",
            " [ 2.86615505e+01  2.78097515e+01 -2.80902042e+01 -2.86303158e+01\n",
            "   2.80956326e+01  2.83217144e+01 -2.79328766e+01 -2.80773220e+01\n",
            "  -5.47566295e-01 -2.78068123e+01]\n",
            " [ 2.77772274e+01  2.80148430e+01 -2.78587894e+01 -2.83921127e+01\n",
            "   2.77095394e+01  2.87286034e+01 -2.79751244e+01 -2.83136711e+01\n",
            "   1.12144649e-01 -2.83209648e+01]]\n",
            "Layer 1 Biases: [ 28.367311  28.36787  -28.366821 -28.36834   28.289673  28.361738\n",
            " -28.369667 -28.369144   0.       -28.348166]\n",
            "Layer 2 Weights: [[-28.823986     0.15450007 -28.899437   -27.779884   -27.88368   ]\n",
            " [-28.16553     -0.5864576  -28.848682   -28.593058   -27.974953  ]\n",
            " [-28.473103    -0.44153732 -28.197723    28.214182   -28.447649  ]\n",
            " [-28.35804     -0.38373712 -28.153967   -27.884024   -28.302458  ]\n",
            " [-27.37021      0.2962734  -28.510948     0.56326336  -0.52018124]\n",
            " [-28.503422    -0.39181957 -28.835964    27.182983    -0.14492029]\n",
            " [-28.373924    -0.19537267 -27.787924    28.74289    -28.60575   ]\n",
            " [-27.87807      0.26102448 -28.056974   -28.894537   -28.775024  ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 26.940992     0.04422677 -27.931625    28.041441     0.55526584]]\n",
            "Layer 2 Biases: [-28.36838    0.       -28.37015  -28.356415 -28.367146]\n",
            "Layer 3 Weights: [[-28.921715   -28.11697    -27.988447   -27.96983    -28.098969\n",
            "   -0.57720083 -28.866539    28.350895    -0.34213665 -28.745312  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-28.112675   -28.02072    -28.078127   -27.8664     -28.571241\n",
            "   -0.12242901 -28.226856    28.68727     27.667295   -28.30448   ]\n",
            " [-28.88631    -28.937094   -27.803486   -27.87999     -0.31219476\n",
            "   -0.23284167 -28.398346    28.294039    28.75695    -28.3411    ]\n",
            " [ -0.30762297 -28.611326   -27.83569    -28.771519   -28.135185\n",
            "    0.27593887 -27.843067    27.647554    28.174356   -27.451284  ]]\n",
            "Layer 3 Biases: [-28.369736 -28.370237 -28.370136 -28.369816 -28.354322   0.\n",
            " -28.369854  28.369719  28.359453 -28.368444]\n",
            "Layer 4 Weights: [[-27.849388   -27.867826  ]\n",
            " [-27.758871   -27.702063  ]\n",
            " [-27.800632   -28.062468  ]\n",
            " [-28.091791   -28.128056  ]\n",
            " [-27.014353   -27.508549  ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-28.263384   -27.859993  ]\n",
            " [-28.888409   -28.504354  ]\n",
            " [-28.453814   -28.490246  ]\n",
            " [-28.734957   -27.697113  ]]\n",
            "Layer 4 Biases: [-28.369999 -28.370375]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.3662 \n",
            "Epoch 8/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 8 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 2.90814915e+01  2.89373512e+01 -2.93370304e+01 -2.87901382e+01\n",
            "   2.84853821e+01  2.83282299e+01 -2.92423553e+01 -2.87963905e+01\n",
            "  -1.03060305e-01 -2.85625668e+01]\n",
            " [ 2.94489822e+01  2.91063175e+01 -2.88787003e+01 -2.85261402e+01\n",
            "  -2.90142994e+01  2.82442303e+01 -2.84181805e+01 -2.89290924e+01\n",
            "  -1.02872252e-02 -2.91722507e+01]\n",
            " [ 2.84162521e+01  2.94372044e+01 -2.88904915e+01 -2.83401566e+01\n",
            "   2.72485352e+01  2.87226467e+01 -2.90590630e+01 -2.88226357e+01\n",
            "  -2.26402253e-01 -2.91477432e+01]\n",
            " [ 2.91525993e+01  2.88707294e+01 -2.83294888e+01 -2.92216015e+01\n",
            "  -2.78195438e+01  2.92069149e+01 -2.90060863e+01 -2.85710964e+01\n",
            "  -5.79387605e-01 -2.89966125e+01]\n",
            " [ 2.91823215e+01  2.83304653e+01 -2.86109123e+01 -2.91510544e+01\n",
            "   2.86134319e+01  2.88412724e+01 -2.84536800e+01 -2.85981121e+01\n",
            "  -5.47566295e-01 -2.83268318e+01]\n",
            " [ 2.82979946e+01  2.85355873e+01 -2.83795300e+01 -2.89128628e+01\n",
            "   2.82199707e+01  2.92491283e+01 -2.84959354e+01 -2.88344746e+01\n",
            "   1.12144649e-01 -2.88402939e+01]]\n",
            "Layer 1 Biases: [ 28.888094  28.888662 -28.887594 -28.889141  28.809013  28.882416\n",
            " -28.890493 -28.88996    0.       -28.868591]\n",
            "Layer 2 Weights: [[-29.342058     0.15450007 -29.420208   -28.300474   -28.404396  ]\n",
            " [-28.686094    -0.5864576  -29.369398   -29.113457   -28.495441  ]\n",
            " [-28.993807    -0.44153732 -28.718546    28.734798   -28.968197  ]\n",
            " [-28.878717    -0.38373712 -28.674765   -28.4046     -28.823088  ]\n",
            " [-27.881134     0.2962734  -29.027649     0.56326336  -0.52018124]\n",
            " [-29.021223    -0.39181957 -29.356255    27.692623    -0.14492029]\n",
            " [-28.89346     -0.19537267 -28.30874     29.263458   -29.126331  ]\n",
            " [-28.398811     0.26102448 -28.577793   -29.413494   -29.29566   ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 27.443233     0.04422677 -28.45154     28.558607     0.55526584]]\n",
            "Layer 2 Biases: [-28.889183   0.       -28.890987 -28.876995 -28.887924]\n",
            "Layer 3 Weights: [[-29.442244   -28.637728   -28.509148   -28.490448   -28.618544\n",
            "   -0.57720083 -29.386902    28.871464    -0.34213665 -29.262423  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-28.633478   -28.541542   -28.598936   -28.387188   -29.084246\n",
            "   -0.12242901 -28.747648    29.208057    28.184793   -28.825214  ]\n",
            " [-29.406796   -29.457777   -28.324224   -28.400667    -0.31219476\n",
            "   -0.23284167 -28.919077    28.814657    29.277195   -28.861717  ]\n",
            " [ -0.30762297 -29.127617   -28.356129   -29.291616   -28.647491\n",
            "    0.27593887 -28.363472    28.162504    28.693586   -27.950125  ]]\n",
            "Layer 3 Biases: [-28.890562 -28.891075 -28.890974 -28.890646 -28.874863   0.\n",
            " -28.890684  28.890545  28.880089 -28.889248]\n",
            "Layer 4 Weights: [[-28.369665   -28.388401  ]\n",
            " [-28.279554   -28.222841  ]\n",
            " [-28.3214     -28.58328   ]\n",
            " [-28.61257    -28.648872  ]\n",
            " [-27.512308   -28.021183  ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-28.783754   -28.380621  ]\n",
            " [-29.409094   -29.025131  ]\n",
            " [-28.973614   -29.010675  ]\n",
            " [-29.254198   -28.217115  ]]\n",
            "Layer 4 Biases: [-28.89083  -28.891212]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.3699  \n",
            "Epoch 9/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 0.3906\n",
            "Epoch 9 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 2.94398441e+01  2.92958641e+01 -2.96955185e+01 -2.91487312e+01\n",
            "   2.88337078e+01  2.86863098e+01 -2.96009674e+01 -2.91549797e+01\n",
            "  -1.03060305e-01 -2.89203396e+01]\n",
            " [ 2.98075294e+01  2.94648781e+01 -2.92372322e+01 -2.88847275e+01\n",
            "  -2.93671741e+01  2.86023979e+01 -2.87767963e+01 -2.92876606e+01\n",
            "  -1.02872252e-02 -2.95279617e+01]\n",
            " [ 2.87747307e+01  2.97958012e+01 -2.92490292e+01 -2.86987495e+01\n",
            "   2.75911388e+01  2.90810032e+01 -2.94176826e+01 -2.91812477e+01\n",
            "  -2.26402253e-01 -2.95046005e+01]\n",
            " [ 2.95111694e+01  2.92292995e+01 -2.86880207e+01 -2.95801792e+01\n",
            "  -2.81628246e+01  2.95654202e+01 -2.93647022e+01 -2.89297009e+01\n",
            "  -5.79387605e-01 -2.93489494e+01]\n",
            " [ 2.95409145e+01  2.86890182e+01 -2.89694595e+01 -2.95096245e+01\n",
            "   2.89699764e+01  2.91990280e+01 -2.88122940e+01 -2.89567165e+01\n",
            "  -5.47566295e-01 -2.86849041e+01]\n",
            " [ 2.86565876e+01  2.88941631e+01 -2.87381001e+01 -2.92714405e+01\n",
            "   2.85714302e+01  2.96075535e+01 -2.88545551e+01 -2.91930885e+01\n",
            "   1.12144649e-01 -2.91978912e+01]]\n",
            "Layer 1 Biases: [ 29.246695  29.24727  -29.246187 -29.247753  29.166618  29.240944\n",
            " -29.249123 -29.248583   0.       -29.226946]\n",
            "Layer 2 Weights: [[-29.69879      0.15450007 -29.778801   -28.658941   -28.76295   ]\n",
            " [-29.044546    -0.5864576  -29.727953   -29.471794   -28.853842  ]\n",
            " [-29.352354    -0.44153732 -29.077173    29.093283   -29.326635  ]\n",
            " [-29.237246    -0.38373712 -29.033377   -28.76306    -29.181583  ]\n",
            " [-28.23293      0.2962734  -29.383432     0.56326336  -0.52018124]\n",
            " [-29.377768    -0.39181957 -29.714514    28.043535    -0.14492029]\n",
            " [-29.2512      -0.19537267 -28.667366    29.62191    -29.484793  ]\n",
            " [-28.757381     0.26102448 -28.93642    -29.770834   -29.654158  ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 27.789042     0.04422677 -28.80954     28.914711     0.55526584]]\n",
            "Layer 2 Biases: [-29.247795   0.       -29.249624 -29.235455 -29.246523]\n",
            "Layer 3 Weights: [[-29.800669   -28.99631    -28.867691   -28.848936   -28.976309\n",
            "   -0.57720083 -29.745214    29.229916    -0.34213665 -29.618492  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-28.99209    -28.90017    -28.957554   -28.745792   -29.437479\n",
            "   -0.12242901 -29.106256    29.566662    28.541128   -29.183783  ]\n",
            " [-29.76519    -29.816309   -28.682793   -28.759195    -0.31219476\n",
            "   -0.23284167 -29.277641    29.173145    29.635427   -29.220201  ]\n",
            " [ -0.30762297 -29.48312    -28.714493   -29.649742   -29.000244\n",
            "    0.27593887 -28.721811    28.517082    29.051117   -28.293587  ]]\n",
            "Layer 3 Biases: [-29.249193 -29.249712 -29.249609 -29.249277 -29.233295   0.\n",
            " -29.249317  29.249176  29.238586 -29.247862]\n",
            "Layer 4 Weights: [[-28.727919   -28.746857  ]\n",
            " [-28.638086   -28.581442  ]\n",
            " [-28.679993   -28.9419    ]\n",
            " [-28.97117    -29.007496  ]\n",
            " [-27.85516    -28.374165  ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-29.142073   -28.739117  ]\n",
            " [-29.767626   -29.383728  ]\n",
            " [-29.331537   -29.369034  ]\n",
            " [-29.611734   -28.57518   ]]\n",
            "Layer 4 Biases: [-29.249464 -29.249851]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.3704 \n",
            "Epoch 10/10\n",
            "\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.3594\n",
            "Epoch 10 ended\n",
            "Loss: 0.3700000047683716, Accuracy: 1.0\n",
            "Learning Rate: 5.0\n",
            "Layer 1 Weights: [[ 2.96861782e+01  2.95423088e+01 -2.99419479e+01 -2.93952312e+01\n",
            "   2.90731373e+01  2.89324551e+01 -2.98474846e+01 -2.94014797e+01\n",
            "  -1.03060305e-01 -2.91662769e+01]\n",
            " [ 3.00540009e+01  2.97113571e+01 -2.94836903e+01 -2.91312275e+01\n",
            "  -2.96097374e+01  2.88486061e+01 -2.90233135e+01 -2.95341454e+01\n",
            "  -1.02872252e-02 -2.97724781e+01]\n",
            " [ 2.90211525e+01  3.00423031e+01 -2.94954929e+01 -2.89452496e+01\n",
            "   2.78266296e+01  2.93273430e+01 -2.96641998e+01 -2.94277592e+01\n",
            "  -2.26402253e-01 -2.97499065e+01]\n",
            " [ 2.97576542e+01  2.94757843e+01 -2.89344788e+01 -2.98266716e+01\n",
            "  -2.83987789e+01  2.98118572e+01 -2.96112194e+01 -2.91762104e+01\n",
            "  -5.79387605e-01 -2.95911407e+01]\n",
            " [ 2.97874165e+01  2.89354897e+01 -2.92159309e+01 -2.97561092e+01\n",
            "   2.92150650e+01  2.94449520e+01 -2.90588112e+01 -2.92032261e+01\n",
            "  -5.47566295e-01 -2.89310493e+01]\n",
            " [ 2.89030876e+01  2.91406517e+01 -2.89845848e+01 -2.95179329e+01\n",
            "   2.88130188e+01  2.98539371e+01 -2.91010742e+01 -2.94396057e+01\n",
            "   1.12144649e-01 -2.94437065e+01]]\n",
            "Layer 1 Biases: [ 29.493202  29.49378  -29.49269  -29.49427   29.41244   29.4874\n",
            " -29.495647 -29.495106   0.       -29.473284]\n",
            "Layer 2 Weights: [[-29.944008     0.15450007 -30.025303   -28.905355   -29.009426  ]\n",
            " [-29.29095     -0.5864576  -29.974428   -29.718119   -29.100208  ]\n",
            " [-29.598822    -0.44153732 -29.323698    29.339712   -29.57303   ]\n",
            " [-29.483702    -0.38373712 -29.279892   -29.009468   -29.428019  ]\n",
            " [-28.47475      0.2962734  -29.627998     0.56326336  -0.52018124]\n",
            " [-29.622856    -0.39181957 -29.960785    28.284746    -0.14492029]\n",
            " [-29.497116    -0.19537267 -28.913889    29.868315   -29.731205  ]\n",
            " [-29.003866     0.26102448 -29.182945   -30.016472   -29.900595  ]\n",
            " [  0.5108065    0.3556202    0.4569195    0.10567272  -0.62227213]\n",
            " [ 28.026735     0.04422677 -29.055632    29.1595       0.55526584]]\n",
            "Layer 2 Biases: [-29.494312   0.       -29.496155 -29.481865 -29.493025]\n",
            "Layer 3 Weights: [[-30.047052   -29.242802   -29.114159   -29.095366   -29.22224\n",
            "   -0.57720083 -29.991524    29.47632     -0.34213665 -29.863255  ]\n",
            " [ -0.0603267    0.28936255  -0.05875361  -0.17484802  -0.5796308\n",
            "   -0.06931853  -0.57717353  -0.40043536   0.5453004   -0.11732602]\n",
            " [-29.238607   -29.146694   -29.204071   -28.992302   -29.68029\n",
            "   -0.12242901 -29.352766    29.813171    28.786072   -29.430267  ]\n",
            " [-30.011555   -30.062767   -28.929277   -29.005651    -0.31219476\n",
            "   -0.23284167 -29.524124    29.419575    29.88168    -29.466629  ]\n",
            " [ -0.30762297 -29.727491   -28.960835   -29.895922   -29.24272\n",
            "    0.27593887 -28.968136    28.760818    29.296886   -28.529665  ]]\n",
            "Layer 3 Biases: [-29.495722 -29.496243 -29.49614  -29.495806 -29.479687   0.\n",
            " -29.495846  29.495705  29.485023 -29.494379]\n",
            "Layer 4 Weights: [[-28.974184   -28.993263  ]\n",
            " [-28.884544   -28.827944  ]\n",
            " [-28.926493   -29.188421  ]\n",
            " [-29.217672   -29.254019  ]\n",
            " [-28.09082    -28.6168    ]\n",
            " [ -0.49446496  -0.3248917 ]\n",
            " [-29.388382   -28.98555   ]\n",
            " [-30.014084   -29.63023   ]\n",
            " [-29.577578   -29.615374  ]\n",
            " [-29.857506   -28.821318  ]]\n",
            "Layer 4 Biases: [-29.495995 -29.496384]\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.3647 \n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAIjCAYAAAAAxIqtAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUjNJREFUeJzt3Xl4VOXd//HPzCSZ7CGQFYjsQsIuCGVTLMjmAoo7lqUKPxVUSt2wFVlUqrVIFR8RK6BVKy7V8lhFAn1wQRQUUZQQWQOYjQAhG1nn/P7AGU2TQMgyZ5b367rmupgz55z5zszt8+TT+z7fYzEMwxAAAAAAoMGsZhcAAAAAAN6OYAUAAAAAjUSwAgAAAIBGIlgBAAAAQCMRrAAAAACgkQhWAAAAANBIBCsAAAAAaCSCFQAAAAA0EsEKAAAAABqJYAUAQBM6ePCgLBaLnnzyyWZ9n3Xr1qlPnz4KDg6WxWJRfn5+s74fAODMCFYA0MxWr14ti8WiL7/80uxSfIIzuNT1+NOf/mR2ic3u2LFjuu666xQSEqJnn31Wf//73xUWFtZs7+ccw85HQECA2rRpo6lTp+rHH39stvc9kzONg9dff92UmgD4twCzCwAAoCFuvPFGjRs3rsb2vn37mlCNe23btk2FhYVatGiRRo4c6bb3XbhwoTp06KDS0lJ9/vnnWr16tT799FN99913Cg4Odlsdv1TbOBg0aJAptQDwbwQrAIDHKS4uPusMzAUXXKCbb77ZTRV5ltzcXElSixYtmuyc9fnOx44dq/79+0uSbr31VsXExOjxxx/X2rVrdd111zVZLefCn8cBAM/CUkAA8BBff/21xo4dq8jISIWHh2vEiBH6/PPPq+1TUVGhBQsWqEuXLgoODlarVq00dOhQpaamuvbJzs7WtGnT1LZtW9ntdiUmJmr8+PE6ePDgWWv4z3/+o2HDhiksLEwtWrTQ+PHjlZaW5nr9rbfeksVi0UcffVTj2Oeff14Wi0Xfffeda9vu3bt1zTXXqGXLlgoODlb//v21du3aasc5l5l99NFHuuOOOxQXF6e2bdvW92s7o/bt2+vyyy/X+vXrXdcjpaSk6J///GeNfffv369rr71WLVu2VGhoqH71q1/p3//+d439SktLNX/+fJ1//vkKDg5WYmKirr76au3bt6/GvitWrFCnTp1kt9t14YUXatu2bdVeb8hvNXz4cE2ZMkWSdOGFF8pisWjq1Kmu1998803169dPISEhiomJ0c0331xjud7UqVMVHh6uffv2ady4cYqIiNCkSZPO9FXWatiwYZJU7bMPHz5cw4cPr7Hv1KlT1b59e9fzX16Ldrbv6WyKi4tVXl5+zvUDQFNixgoAPMD333+vYcOGKTIyUvfdd58CAwP1/PPPa/jw4froo480cOBASdL8+fO1ePFi3XrrrRowYIAKCgr05Zdfavv27br00kslSRMnTtT333+vO++8U+3bt1dubq5SU1N16NChan/Y/rcNGzZo7Nix6tixo+bPn69Tp07pmWee0ZAhQ7R9+3a1b99el112mcLDw/XGG2/o4osvrnb8mjVr1L17d/Xo0cP1mYYMGaI2bdrogQceUFhYmN544w1NmDBBb7/9tq666qpqx99xxx2KjY3VvHnzVFxcfNbvrKSkRHl5eTW2t2jRQgEBP/+/tz179uj666/XbbfdpilTpmjVqlW69tprtW7dOtd3lpOTo8GDB6ukpER33XWXWrVqpZdeeklXXnml3nrrLVetVVVVuvzyy7Vx40bdcMMNuvvuu1VYWKjU1FR999136tSpk+t9X3vtNRUWFur//b//J4vFoieeeEJXX3219u/fr8DAwAb/Vn/4wx/UtWtXrVixwrU0z/m+q1ev1rRp03ThhRdq8eLFysnJ0V//+ldt3rxZX3/9dbUZrsrKSo0ePVpDhw7Vk08+qdDQ0LN+5//NGQCjo6PP+Vin+nxPZ7JgwQLde++9slgs6tevnx599FGNGjWqwfUAQIMZAIBmtWrVKkOSsW3btjr3mTBhghEUFGTs27fPtS0zM9OIiIgwLrroIte23r17G5dddlmd5zlx4oQhyfjzn/98znX26dPHiIuLM44dO+ba9s033xhWq9WYPHmya9uNN95oxMXFGZWVla5tWVlZhtVqNRYuXOjaNmLECKNnz55GaWmpa5vD4TAGDx5sdOnSxbXN+f0MHTq02jnrcuDAAUNSnY8tW7a49m3Xrp0hyXj77bdd206ePGkkJiYaffv2dW2bPXu2Icn45JNPXNsKCwuNDh06GO3btzeqqqoMwzCMlStXGpKMJUuW1KjL4XBUq69Vq1bG8ePHXa//61//MiQZ//u//2sYRuN+q9rGVHl5uREXF2f06NHDOHXqlGv7e++9Z0gy5s2b59o2ZcoUQ5LxwAMPnNP7bdiwwTh69Khx+PBh46233jJiY2MNu91uHD582LXvxRdfbFx88cU1zjFlyhSjXbt2ruf1/Z7qkpGRYYwaNcp47rnnjLVr1xpLly41zjvvPMNqtRrvvfdevT4XADQllgICgMmqqqq0fv16TZgwQR07dnRtT0xM1E033aRPP/1UBQUFkk7Pxnz//ffas2dPrecKCQlRUFCQNm3apBMnTtS7hqysLO3YsUNTp05Vy5YtXdt79eqlSy+9VO+//75r2/XXX6/c3Fxt2rTJte2tt96Sw+HQ9ddfL0k6fvy4/vOf/+i6665TYWGh8vLylJeXp2PHjmn06NHas2dPjeVp06dPl81mq3fNM2bMUGpqao1HSkpKtf1at25dbXYsMjJSkydP1tdff63s7GxJ0vvvv68BAwZo6NChrv3Cw8M1Y8YMHTx4ULt27ZIkvf3224qJidGdd95Zox6LxVLt+fXXX19tJse5bG7//v2SGv5b1eXLL79Ubm6u7rjjjmqNJC677DJ169at1mWNt99++zm9x8iRIxUbG6ukpCRdc801CgsL09q1axu1dPNs31NdzjvvPH344Ye67bbbdMUVV+juu+/W119/rdjYWP3+979vcD0A0FAEKwAw2dGjR1VSUqKuXbvWeC05OVkOh0OHDx+WdLorW35+vs4//3z17NlT9957r7799lvX/na7XY8//rg++OADxcfH66KLLtITTzzhChB1ycjIkKQ6a8jLy3MtzxszZoyioqK0Zs0a1z5r1qxRnz59dP7550uS9u7dK8Mw9NBDDyk2Nrba4+GHH5b0cwMGpw4dOpz1u/qlLl26aOTIkTUekZGR1fbr3LlzjdDjrNO5lC0jI6POz+58XTp9LVHXrl2rLTWsy3nnnVftuTM8OENUQ3+rupzpN+zWrZvrdaeAgIBzDkTPPvusUlNT9dZbb2ncuHHKy8uT3W5vUL1OZ/uezkXLli01bdo0paen68iRI42qCwDOFcEKALzIRRddpH379mnlypXq0aOH/va3v+mCCy7Q3/72N9c+s2fP1g8//KDFixcrODhYDz30kJKTk/X11183SQ12u10TJkzQO++8o8rKSv3444/avHmza7ZKkhwOhyTpnnvuqXVWKTU1VZ07d6523pCQkCapz1PUNftmGIbr3839W52J3W6X1XpufwYMGDBAI0eO1MSJE7V27Vr16NFDN910k4qKilz7/HeIdaqqqqp1e32+p3ORlJQk6fSsKQC4E8EKAEwWGxur0NBQpaen13ht9+7dslqtrj8WpZ//V/l//OMfOnz4sHr16qX58+dXO65Tp076/e9/r/Xr1+u7775TeXm5/vKXv9RZQ7t27SSpzhpiYmKqteK+/vrrlZeXp40bN+rNN9+UYRjVgpVzSWNgYGCts0ojR45URERE/b6gRnLOnv3SDz/8IEmuBhHt2rWr87M7X5dOf6/p6emqqKhosvrO9beqy5l+w/T0dNfrTcVms2nx4sXKzMzUsmXLXNujo6OVn59fY///njFrLs4lhLGxsW55PwBwIlgBgMlsNptGjRqlf/3rX9XabOfk5Oi1117T0KFDXcvbjh07Vu3Y8PBwde7cWWVlZZJOd8orLS2ttk+nTp0UERHh2qc2iYmJ6tOnj1566aVqfxR/9913Wr9+fY0bsI4cOVItW7bUmjVrtGbNGg0YMKDaUr64uDgNHz5czz//vLKysmq839GjR8/8pTShzMxMvfPOO67nBQUFevnll9WnTx8lJCRIksaNG6etW7dqy5Ytrv2Ki4u1YsUKtW/f3nXd1sSJE5WXl1ctSDid6wxLQ3+ruvTv319xcXFavnx5teM/+OADpaWl6bLLLjvnc57N8OHDNWDAAC1dutT1WTp16qTdu3dX+42/+eYbbd68uUnfu7Yx9OOPP2rlypXq1auXEhMTm/T9AOBsaLcOAG6ycuVKrVu3rsb2u+++W4888ohSU1M1dOhQ3XHHHQoICNDzzz+vsrIyPfHEE659U1JSNHz4cPXr108tW7bUl19+qbfeekuzZs2SdHomZsSIEbruuuuUkpKigIAAvfPOO8rJydENN9xwxvr+/Oc/a+zYsRo0aJBuueUWV7v1qKioGjNigYGBuvrqq/X666+ruLhYTz75ZI3zPfvssxo6dKh69uyp6dOnq2PHjsrJydGWLVt05MgRffPNNw34Fn+2fft2vfLKKzW2d+rUSYMGDXI9P//883XLLbdo27Ztio+P18qVK5WTk6NVq1a59nnggQf0j3/8Q2PHjtVdd92lli1b6qWXXtKBAwf09ttvu5bMTZ48WS+//LLmzJmjrVu3atiwYSouLtaGDRt0xx13aPz48fWuvzG/VW0CAwP1+OOPa9q0abr44ot14403utqtt2/fXr/73e/O+Zz1ce+99+raa6/V6tWrddttt+m3v/2tlixZotGjR+uWW25Rbm6uli9fru7du7uasDSF++67T/v27dOIESPUunVrHTx4UM8//7yKi4v117/+tcneBwDqzcyWhADgD5ytqut6OFtVb9++3Rg9erQRHh5uhIaGGpdcconx2WefVTvXI488YgwYMMBo0aKFERISYnTr1s149NFHjfLycsMwDCMvL8+YOXOm0a1bNyMsLMyIiooyBg4caLzxxhv1qnXDhg3GkCFDjJCQECMyMtK44oorjF27dtW6b2pqqiHJsFgs1dpt/9K+ffuMyZMnGwkJCUZgYKDRpk0b4/LLLzfeeuutGt/PmdrR/9LZ2q1PmTLFtW+7du2Myy67zPjwww+NXr16GXa73ejWrZvx5ptv1lrrNddcY7Ro0cIIDg42BgwYUGvb7pKSEuMPf/iD0aFDByMwMNBISEgwrrnmGlerfGd9tbVRl2Q8/PDDhmE07rc603e2Zs0ao2/fvobdbjdatmxpTJo0yThy5Ei1faZMmWKEhYWd9X3q835VVVVGp06djE6dOrna5b/yyitGx44djaCgIKNPnz7Ghx9+WGe79bN9T3V57bXXjIsuusiIjY01AgICjJiYGOOqq64yvvrqq3p/LgBoShbDaODVoQAAeLj27durR48eeu+998wuBQDg47jGCgAAAAAaiWAFAAAAAI1EsAIAAACARuIaKwAAAABoJGasAAAAAKCRCFYAAAAA0EjcILgWDodDmZmZioiIkMViMbscAAAAACYxDEOFhYVq3bq164bxtSFY1SIzM1NJSUlmlwEAAADAQxw+fFht27at83WCVS0iIiIknf7yIiMjTa2loqJC69ev16hRoxQYGGhqLfAPjDm4E+MN7saYgzsx3nxDQUGBkpKSXBmhLgSrWjiX/0VGRnpEsAoNDVVkZCT/QcItGHNwJ8Yb3I0xB3divPmWs10iRPMKAAAAAGgkghUAAAAANBLBCgAAAAAaiWusAAAA4LMMw1BlZaWqqqrc/t4VFRUKCAhQaWmpKe+P+rHZbAoICGj0bZYIVgAAAPBJ5eXlysrKUklJiSnvbxiGEhISdPjwYe6N6uFCQ0OVmJiooKCgBp+DYAUAAACf43A4dODAAdlsNrVu3VpBQUFuDzcOh0NFRUUKDw8/441lYR7DMFReXq6jR4/qwIED6tKlS4N/K4IVAAAAfE55ebkcDoeSkpIUGhpqSg0Oh0Pl5eUKDg4mWHmwkJAQBQYGKiMjw/V7NQS/MAAAAHwWgQb10RTjhJEGAAAAAI1EsAIAAACARiJYAQAAAD6uffv2Wrp0ab3337RpkywWi/Lz85utJl9DsAIAAAA8hMViOeNj/vz5DTrvtm3bNGPGjHrvP3jwYGVlZSkqKqpB71dfvhTg6AoIAAAAeIisrCzXv9esWaN58+YpPT3dtS08PNz1b8MwVFVVpYCAs/9JHxsbe051BAUFKSEh4ZyO8XfMWAEAAMAvGIahkvJKtz5OlVeppLxShmHUq8aEhATXIyoqShaLxfV89+7dioiI0AcffKB+/frJbrfr008/1b59+zR+/HjFx8crPDxcF154oTZs2FDtvP+9FNBisehvf/ubrrrqKoWGhqpLly5au3at6/X/nklavXq1WrRooQ8//FDJyckKDw/XmDFjqgXByspK3XXXXWrRooVatWql+++/X1OmTNGECRMa/JudOHFCkydPVnR0tEJDQzV27Fjt2bPH9XpGRoauuOIKRUdHKywsTN27d9f777/vOnbSpEmKjY1VSEiIunTpolWrVjW4lrNhxgoAAAB+4VRFlVLmfWjKe+9aOFqhQU3zp/cDDzygJ598Uh07dlR0dLQOHz6scePG6dFHH5XdbtfLL7+sK664Qunp6TrvvPPqPM+CBQv0xBNP6M9//rOeeeYZTZo0SRkZGWrZsmWt+5eUlOjJJ5/U3//+d1mtVt18882655579Oqrr0qSHn/8cb366qtatWqVkpOT9de//lXvvvuuLrnkkgZ/1qlTp2rPnj1au3atIiMjdf/992vcuHHatWuXAgMDNXPmTJWXl+vjjz9WWFiYdu3a5ZrVe+ihh7Rr1y598MEHiomJ0d69e3Xq1KkG13I2BCsAAADAiyxcuFCXXnqp63nLli3Vu3dv1/NFixbpnXfe0dq1azVr1qw6zzN16lTdeOONkqTHHntMTz/9tLZu3aoxY8bUun9FRYWWL1+uTp06SZJmzZqlhQsXul5/5plnNHfuXF111VWSpGXLlrlmjxrCGag2b96swYMHS5JeffVVJSUl6d1339W1116rQ4cOaeLEierZs6ckqWPHjq7jDx06pL59+6p///6STs/aNSeClQerchj6+lC+NudYNNphKNDsggAAALxYSKBNuxaOdtv7ORwOFRYUKiIyQiGBtiY7rzMoOBUVFWn+/Pn697//raysLFVWVurUqVM6dOjQGc/Tq1cv17/DwsIUGRmp3NzcOvcPDQ11hSpJSkxMdO1/8uRJ5eTkaMCAAa7XbTab+vXrJ4fDcU6fzyktLU0BAQEaOHCga1urVq3UtWtXpaWlSZLuuusu3X777Vq/fr1GjhypiRMnuj7X7bffrokTJ2r79u0aNWqUJkyY4ApozYFrrDyYYRj6zaov9cZ+mw4dLzG7HAAAAK9msVgUGhTg1kdIkE2hQQGyWCxN9jnCwsKqPb/nnnv0zjvv6LHHHtMnn3yiHTt2qGfPniovLz/jeQIDq//P9haL5YwhqLb963vtWHO59dZbtX//fv3mN7/Rzp071b9/fz3zzDOSpLFjxyojI0O/+93vlJmZqREjRuiee+5ptloIVh4swGZV1/jTa0TTsgpNrgYAAACeaPPmzZo6daquuuoq9ezZUwkJCTp48KBba4iKilJ8fLy2bdvm2lZVVaXt27c3+JzJycmqrKzUF1984dp27NgxpaenKyUlxbUtKSlJt912m/75z3/q97//vV544QXXa7GxsZoyZYpeeeUVLV26VCtWrGhwPWfDUkAPl5wYoW9/LNDu7EKNN7sYAAAAeJwuXbron//8p6644gpZLBY99NBDDV5+1xh33nmnFi9erM6dO6tbt2565plndOLEiXrN1u3cuVMRERGu5xaLRb1799b48eM1ffp0Pf/884qIiNADDzygNm3aaPz4038Zz549W2PHjtX555+vEydO6P/+7/+UnJwsSZo3b5769eun7t27q6ysTO+9957rteZAsPJwyQmnB9iubGasAAAAUNOSJUv029/+VoMHD1ZMTIzuv/9+FRQUuL2O+++/X9nZ2Zo8ebJsNptmzJih0aNHy2Y7+/VlF110UbXnNptNlZWVWrVqle6++25dfvnlKi8v10UXXaT333/ftSyxqqpKM2fO1JEjRxQZGakxY8boqaeeknT6Xlxz587VwYMHFRISomHDhun1119v+g/+E4th9sJID1RQUKCoqCidPHlSkZGRptby+d5c3fC3bYqPsOuLP4w0tRb4h4qKCr3//vsaN25cjbXUQFNjvMHdGHP+o7S0VAcOHFCHDh0UHBxsSg0Oh0MFBQWKjIyU1ep/V+A4HA4lJyfruuuu06JFi8wu54zONF7qmw2YsfJwXX+ascopLNOxojK1CrebXBEAAABQU0ZGhtavX6+LL75YZWVlWrZsmQ4cOKCbbrrJ7NLcwv+is5cJtwcoJvj0pCINLAAAAOCprFarVq9erQsvvFBDhgzRzp07tWHDhma9rsmTMGPlBdqEGsortWhX1kkN7RJjdjkAAABADUlJSdq8ebPZZZiGGSsv0Cbs9IzVrkz3X4QIAAAA4OwIVl6gzU/3gGMpIAAAwLmhTxvqoynGCcHKC7QNPf1D7z1apNKKKpOrAQAA8HzOro8lJSUmVwJv4BwnjekWyjVWXiAqSGoREqj8UxXak1Oknm2jzC4JAADAo9lsNrVo0UK5ubmSpNDQ0HrdqLYpORwOlZeXq7S01C/brXsDwzBUUlKi3NxctWjRol733KoLwcoLWCxScmKEtuw/rl1ZJwlWAAAA9ZCQkCBJrnDlboZh6NSpUwoJCXF7qMO5adGihWu8NBTBykskJ5wOVlxnBQAAUD8Wi0WJiYmKi4tTRUWF29+/oqJCH3/8sS666CJuSO3BAgMDGzVT5USw8hLdfrpRMJ0BAQAAzo3NZmuSP5wb8r6VlZUKDg4mWPkBFnt6ieTE08EqLauA7jYAAACAhyFYeYmOMWEKsllVWFapIydOmV0OAAAAgF8gWHmJoACrusSHS5K+ZzkgAAAA4FEIVl4kOTFSkrQri2AFAAAAeBKClRdJcQYrZqwAAAAAj0Kw8iIprU8HqzRmrAAAAACPQrDyIskJp4PVj/mndLLE/fdiAAAAAFA7gpUXiQoNVJsWIZK4zgoAAADwJAQrL8NyQAAAAMDzEKy8TAqdAQEAAACPQ7DyMsl0BgQAAAA8DsHKy3T/aSngntxClVc6TK4GAAAAgESw8jpto0MUYQ9QRZWhfUeLzC4HAAAAgAhWXsdisSi5NcsBAQAAAE9CsPJCNLAAAAAAPAvBygs5gxUt1wEAAADPQLDyQs57We3KKpBhGCZXAwAAAIBg5YU6x4XLZrUov6RCWSdLzS4HAAAA8HsEKy8UHGhT59hwSTSwAAAAADwBwcpLOZcDcp0VAAAAYD6ClZeiMyAAAADgOQhWXiqZYAUAAAB4DIKVl0pOjJAkZRwrUVFZpcnVAAAAAP6NYOWlWoXblRAZLEnazawVAAAAYCqClRdzzlqxHBAAAAAwF8HKi7luFEzLdQAAAMBUBCsvlpIYJYmW6wAAAIDZCFZezDljtTu7UJVVDpOrAQAAAPwXwcqLtWsZqtAgm8oqHTqQV2x2OQAAAIDfIlh5MavVom4JNLAAAAAAzEaw8nKuBhYEKwAAAMA0BCsvl5xIZ0AAAADAbAQrL5fyU7CiMyAAAABgHoKVl+uWECmrRcorKlduYanZ5QAAAAB+iWDl5UKCbOoQEyaJ5YAAAACAWQhWPsB1nRXLAQEAAABTEKx8gLMzYFpWocmVAAAAAP6JYOUDUlydAU+aXAkAAADgnwhWPsAZrPbnFaukvNLkagAAAAD/Q7DyAbERdsWEB8kwpPRslgMCAAAA7kaw8gEWi8XVwILrrAAAAAD3I1j5CGcDi11ZXGcFAAAAuBvBykf83MCClusAAACAuxGsfIQzWO3OLpTDYZhcDQAAAOBfCFY+okNMmOwBVpWUVynjeInZ5QAAAAB+hWDlIwJsVnVNiJDEckAAAADA3QhWPsR1nRUNLAAAAAC3Mj1YPfvss2rfvr2Cg4M1cOBAbd26tc59V69eLYvFUu0RHBxcbZ+pU6fW2GfMmDHN/TE8grMzIC3XAQAAAPcKMPPN16xZozlz5mj58uUaOHCgli5dqtGjRys9PV1xcXG1HhMZGan09HTXc4vFUmOfMWPGaNWqVa7ndru96Yv3QHQGBAAAAMxh6ozVkiVLNH36dE2bNk0pKSlavny5QkNDtXLlyjqPsVgsSkhIcD3i4+Nr7GO326vtEx0d3Zwfw2N0+ylYZReU6nhxucnVAAAAAP7DtBmr8vJyffXVV5o7d65rm9Vq1ciRI7Vly5Y6jysqKlK7du3kcDh0wQUX6LHHHlP37t2r7bNp0ybFxcUpOjpav/71r/XII4+oVatWdZ6zrKxMZWVlrucFBadnfCoqKlRRUdHQj9gknO9fnzrsVum8liE6dPyUdh4+rsGd6v7MQF3OZcwBjcV4g7sx5uBOjDffUN/fz2IYhik3PcrMzFSbNm302WefadCgQa7t9913nz766CN98cUXNY7ZsmWL9uzZo169eunkyZN68skn9fHHH+v7779X27ZtJUmvv/66QkND1aFDB+3bt08PPvigwsPDtWXLFtlstlprmT9/vhYsWFBj+2uvvabQ0NAm+sTusTLdqm+OWzW+XZV+3Zr7WQEAAACNUVJSoptuukknT55UZGRknft5VbD6bxUVFUpOTtaNN96oRYsW1brP/v371alTJ23YsEEjRoyodZ/aZqySkpKUl5d3xi/PHSoqKpSamqpLL71UgYGBZ91/2f/t01//s0/jeyfqyWt6uqFC+JpzHXNAYzDe4G6MObgT4803FBQUKCYm5qzByrSlgDExMbLZbMrJyam2PScnRwkJCfU6R2BgoPr27au9e/fWuU/Hjh0VExOjvXv31hms7HZ7rQ0uAgMDPeY/gvrW0rPt6evJdmcXeUzt8E6eNP7h+xhvcDfGHNyJ8ebd6vvbmda8IigoSP369dPGjRtd2xwOhzZu3FhtButMqqqqtHPnTiUmJta5z5EjR3Ts2LEz7uNLnC3X9x0tUmlFlcnVAAAAAP7B1K6Ac+bM0QsvvKCXXnpJaWlpuv3221VcXKxp06ZJkiZPnlytucXChQu1fv167d+/X9u3b9fNN9+sjIwM3XrrrZJON7a499579fnnn+vgwYPauHGjxo8fr86dO2v06NGmfEZ3S4wKVovQQFU6DO3NLTK7HAAAAMAvmHofq+uvv15Hjx7VvHnzlJ2drT59+mjdunWuFuqHDh2S1fpz9jtx4oSmT5+u7OxsRUdHq1+/fvrss8+UkpIiSbLZbPr222/10ksvKT8/X61bt9aoUaO0aNEiv7mXlcViUXJCpLbsP6ZdmQXq0SbK7JIAAAAAn2dqsJKkWbNmadasWbW+tmnTpmrPn3rqKT311FN1niskJEQffvhhU5bnlVJa/xSssrhRMAAAAOAOpi4FRPNI+elGwQQrAAAAwD0IVj4o+adglZZZIJO66QMAAAB+hWDlgzrHhSvQZlFhWaWOnDhldjkAAACAzyNY+aCgAKu6xEVIYjkgAAAA4A4EKx/lvJ/VrkyCFQAAANDcCFY+KpkGFgAAAIDbEKx8lLMzYBrBCgAAAGh2BCsf5QxWR06c0slTFSZXAwAAAPg2gpWPigoNVJsWIZKYtQIAAACaG8HKh7mus6KBBQAAANCsCFY+zNkZkBkrAAAAoHkRrHxYCp0BAQAAALcgWPkwZ7Dak1OkiiqHydUAAAAAvotg5cPaRocowh6g8iqH9h0tMrscAAAAwGcRrHyY1WqhgQUAAADgBgQrH+dsYEGwAgAAAJoPwcrHJSdGSKKBBQAAANCcCFY+LiUxStLpluuGYZhcDQAAAOCbCFY+rkt8uGxWi06UVCi7oNTscgAAAACfRLDyccGBNnWKDZPEdVYAAABAcyFY+QHn/azSuM4KAAAAaBYEKz/g6gxIsAIAAACaBcHKDzgbWLAUEAAAAGgeBCs/4Gy5fvBYiYrKKk2uBgAAAPA9BCs/0CrcrvhIuyQpPZtZKwAAAKCpEaz8hLOBBcsBAQAAgKZHsPITyYk0sAAAAACaC8HKT/zcGbDQ5EoAAAAA30Ow8hPOpYC7swpUWeUwuRoAAADAtxCs/ES7VmEKDbKprNKhg8eKzS4HAAAA8CkEKz9hs1rUNeF02/XvaWABAAAANCmClR9xLgdM4zorAAAAoEkRrPzIzw0smLECAAAAmhLByo8kcy8rAAAAoFkQrPxIt4QIWSxSXlGZcgtLzS4HAAAA8BkEKz8SGhSgDjFhkrjOCgAAAGhKBCs/k8JyQAAAAKDJEaz8jOs6KxpYAAAAAE2GYOVnnJ0B0whWAAAAQJMhWPmZ7j/NWO0/WqRT5VUmVwMAAAD4BoKVn4mNsKtVWJAchpSeQwMLAAAAoCkQrPyMxWJhOSAAAADQxAhWfojOgAAAAEDTIlj5IeeMFZ0BAQAAgKZBsPJDzpbru7MK5HAYJlcDAAAAeD+ClR/qGBOmoACrisurdOh4idnlAAAAAF6PYOWHAmxWdUuIkMRyQAAAAKApEKz8VHICDSwAAACApkKw8lO0XAcAAACaDsHKT9EZEAAAAGg6BCs/5bzGKutkqU4Ul5tcDQAAAODdCFZ+KiI4UOe1DJXEckAAAACgsQhWfiwlkeWAAAAAQFMgWPkx13VWdAYEAAAAGoVg5ceYsQIAAACaBsHKjyX/NGO1N7dIZZVVJlcDAAAAeC+ClR9rHRWsqJBAVToM7ckpMrscAAAAwGsRrPyYxWJhOSAAAADQBAhWfi75p2BFy3UAAACg4QhWfo7OgAAAAEDjEaz83C+XAhqGYXI1AAAAgHciWPm5znHhCrRZVFhaqSMnTpldDgAAAOCVCFZ+LijAqs5xEZK4zgoAAABoKIIV6AwIAAAANBLBCjSwAAAAABqJYAUlJ/60FDCbYAUAAAA0BMEKrqWAh4+f0slTFSZXAwAAAHgfghXUIjRIbVqESJJ2c50VAAAAcM4IVpAkJdPAAgAAAGgwghUkSSmJtFwHAAAAGopgBUm/6AxIsAIAAADOGcEKkqSUxChJ0g/ZRaqocphcDQAAAOBdCFaQJLWNDlG4PUDlVQ7tP1psdjkAAACAVyFYQZJktVpc97PalXXS5GoAAAAA70Kwgovzfla7MrnOCgAAADgXBCu40MACAAAAaBiCFVyc97JKyyqUYRgmVwMAAAB4D4IVXM6Pj5DNatHx4nLlFJSZXQ4AAADgNQhWcAkOtKlTbJgkGlgAAAAA54JghWp+uRwQAAAAQP0QrFANnQEBAACAc0ewQjV0BgQAAADOHcEK1TiXAh48VqziskqTqwEAAAC8A8EK1cSE2xUXYZdhSLuzuc4KAAAAqA+CFWpgOSAAAABwbghWqIEGFgAAAMC5IVihhp9brhOsAAAAgPowPVg9++yzat++vYKDgzVw4EBt3bq1zn1Xr14ti8VS7REcHFxtH8MwNG/ePCUmJiokJEQjR47Unj17mvtj+BTnUsDd2QWqchgmVwMAAAB4PlOD1Zo1azRnzhw9/PDD2r59u3r37q3Ro0crNze3zmMiIyOVlZXlemRkZFR7/YknntDTTz+t5cuX64svvlBYWJhGjx6t0tLS5v44PqN9qzCFBNpUWuHQgbxis8sBAAAAPJ6pwWrJkiWaPn26pk2bppSUFC1fvlyhoaFauXJlncdYLBYlJCS4HvHx8a7XDMPQ0qVL9cc//lHjx49Xr1699PLLLyszM1PvvvuuGz6Rb7BZLeqWGCGJBhYAAABAfQSY9cbl5eX66quvNHfuXNc2q9WqkSNHasuWLXUeV1RUpHbt2snhcOiCCy7QY489pu7du0uSDhw4oOzsbI0cOdK1f1RUlAYOHKgtW7bohhtuqPWcZWVlKisrcz0vKDgdJioqKlRRUdGoz9lYzvd3dx1d48P19aF8fX8kX2NTYt363jCXWWMO/onxBndjzMGdGG++ob6/n2nBKi8vT1VVVdVmnCQpPj5eu3fvrvWYrl27auXKlerVq5dOnjypJ598UoMHD9b333+vtm3bKjs723WO/z6n87XaLF68WAsWLKixff369QoNDT3Xj9YsUlNT3fp+VXkWSTZ9vHOfUiq5Rs0fuXvMwb8x3uBujDm4E+PNu5WUlNRrP9OCVUMMGjRIgwYNcj0fPHiwkpOT9fzzz2vRokUNPu/cuXM1Z84c1/OCggIlJSVp1KhRioyMbFTNjVVRUaHU1FRdeumlCgwMdNv7Jh7O15srtiqvMljjxg132/vCfGaNOfgnxhvcjTEHd2K8+QbnarazMS1YxcTEyGazKScnp9r2nJwcJSQk1OscgYGB6tu3r/bu3StJruNycnKUmJhY7Zx9+vSp8zx2u112u73W83vKfwTurqVH22hZLNLRonLllzoUG1Hz+4Fv86TxD9/HeIO7MebgTow371bf38605hVBQUHq16+fNm7c6NrmcDi0cePGarNSZ1JVVaWdO3e6QlSHDh2UkJBQ7ZwFBQX64osv6n1OnBYaFKAOrcIkcT8rAAAA4GxM7Qo4Z84cvfDCC3rppZeUlpam22+/XcXFxZo2bZokafLkydWaWyxcuFDr16/X/v37tX37dt18883KyMjQrbfeKul0x8DZs2frkUce0dq1a7Vz505NnjxZrVu31oQJE8z4iF4t+af7WdEZEAAAADgzU6+xuv7663X06FHNmzdP2dnZ6tOnj9atW+dqPnHo0CFZrT9nvxMnTmj69OnKzs5WdHS0+vXrp88++0wpKSmufe677z4VFxdrxowZys/P19ChQ7Vu3boaNxLG2aUkRurf32ZpVybBCgAAADgT05tXzJo1S7Nmzar1tU2bNlV7/tRTT+mpp5464/ksFosWLlyohQsXNlWJfisl8fSMFUsBAQAAgDMzdSkgPFvKT0sB9x0tUmlFlcnVAAAAAJ6LYIU6xUXY1SosSA5DSs8uNLscAAAAwGMRrFAni8WiZJYDAgAAAGdFsMIZpdAZEAAAADgrghXOyNnAgs6AAAAAQN0IVjgj54xVWlaBHA7D5GoAAAAAz0Swwhl1jAlTUIBVxeVVOnyixOxyAAAAAI9EsMIZBdis6hofIYnlgAAAAEBdCFY4K9d1VjSwAAAAAGpFsMJZ/fI6KwAAAAA1EaxwVsl0BgQAAADOiGCFs+qWePoaq8yTpTpRXG5yNQAAAIDnIVjhrCKDA3Vey1BJLAcEAAAAakOwQr0k/zRrRQMLAAAAoCaCFeolJTFKEsEKAAAAqA3BCvXi7AxIAwsAAACgJoIV6sUZrPYdLVJ5pcPkagAAAADPQrBCvbSOClZkcIAqqgztyS00uxwAAADAoxCsUC8Wi4XlgAAAAEAdCFaoNxpYAAAAALUjWKHenC3XuZcVAAAAUB3BCvX2y6WAhmGYXA0AAADgOQhWqLcucREKtFlUUFqpH/NPmV0OAAAA4DEIVqi3oACrOsc5lwPSGRAAAABwIljhnDivs6IzIAAAAPAzghXOSUriT9dZZZ00uRIAAADAcxCscE5cDSzoDAgAAAC4EKxwTpwzVoePn1JBaYXJ1QAAAACeoUHB6vDhwzpy5Ijr+datWzV79mytWLGiyQqDZ2oRGqTWUcGSpN00sAAAAAAkNTBY3XTTTfq///s/SVJ2drYuvfRSbd26VX/4wx+0cOHCJi0Qnufn+1lxnRUAAAAgNTBYfffddxowYIAk6Y033lCPHj302Wef6dVXX9Xq1aubsj54IOdyQFquAwAAAKc1KFhVVFTIbrdLkjZs2KArr7xSktStWzdlZWU1XXXwSMmJNLAAAAAAfqlBwap79+5avny5PvnkE6WmpmrMmDGSpMzMTLVq1apJC4TncS4FTM8pVEWVw+RqAAAAAPM1KFg9/vjjev755zV8+HDdeOON6t27tyRp7dq1riWC8F1J0aEKtweovNKh/UeLzS4HAAAAMF1AQw4aPny48vLyVFBQoOjoaNf2GTNmKDQ0tMmKg2eyWi3qlhChLzNOKC2rQF0TIswuCQAAADBVg2asTp06pbKyMleoysjI0NKlS5Wenq64uLgmLRCeiRsFAwAAAD9rULAaP368Xn75ZUlSfn6+Bg4cqL/85S+aMGGCnnvuuSYtEJ7J2RlwVybBCgAAAGhQsNq+fbuGDRsmSXrrrbcUHx+vjIwMvfzyy3r66aebtEB4JueMVVpWgQzDMLkaAAAAwFwNClYlJSWKiDh9Xc369et19dVXy2q16le/+pUyMjKatEB4pvPjI2S1SMeKy5VbWGZ2OQAAAICpGhSsOnfurHfffVeHDx/Whx9+qFGjRkmScnNzFRkZ2aQFwjMFB9rUKTZcEssBAQAAgAYFq3nz5umee+5R+/btNWDAAA0aNEjS6dmrvn37NmmB8Fw0sAAAAABOa1C79WuuuUZDhw5VVlaW6x5WkjRixAhdddVVTVYcPFtyYqT+tSOTYAUAAAC/16BgJUkJCQlKSEjQkSNHJElt27bl5sB+xtkZMI2lgAAAAPBzDVoK6HA4tHDhQkVFRaldu3Zq166dWrRooUWLFsnhcDR1jfBQyT8FqwPHilVSXmlyNQAAAIB5GjRj9Yc//EEvvvii/vSnP2nIkCGSpE8//VTz589XaWmpHn300SYtEp4pNsKuuAi7cgvLtDu7UBecF212SQAAAIApGhSsXnrpJf3tb3/TlVde6drWq1cvtWnTRnfccQfByo8kJ0Yqt/CodmUWEKwAAADgtxq0FPD48ePq1q1bje3dunXT8ePHG10UvAedAQEAAIAGBqvevXtr2bJlNbYvW7ZMvXr1anRR8B7OBhbcywoAAAD+rEFLAZ944glddtll2rBhg+seVlu2bNHhw4f1/vvvN2mB8GzOBhbp2YWqchiyWS0mVwQAAAC4X4NmrC6++GL98MMPuuqqq5Sfn6/8/HxdffXV+v777/X3v/+9qWuEB+sQE6bgQKtOVVTp4LFis8sBAAAATNHg+1i1bt26RpOKb775Ri+++KJWrFjR6MLgHWxWi7olRGrH4XztyixQp9hws0sCAAAA3K5BM1bALzkbWKTRwAIAAAB+imCFRnNeZ0VnQAAAAPgrghUajc6AAAAA8HfndI3V1VdffcbX8/PzG1MLvFS3hAhZLFJuYZnyisoUE243uyQAAADArc4pWEVFRZ319cmTJzeqIHifMHuA2rcK04G8YqVlFWhYl1izSwIAAADc6pyC1apVq5qrDni5lMRIHcgr1q5MghUAAAD8D9dYoUk4OwPSwAIAAAD+iGCFJuFsYEHLdQAAAPgjghWahLPl+r6jxSqtqDK5GgAAAMC9CFZoEvGRdrUMC1KVw9APOYVmlwMAAAC4FcEKTcJisXA/KwAAAPgtghWajLOBBddZAQAAwN8QrNBkkhMjJNEZEAAAAP6HYIUmk5J4+gbSaVmFcjgMk6sBAAAA3IdghSbTMTZMQQFWFZVV6siJU2aXAwAAALgNwQpNJtBm1fnx4ZKkXVknTa4GAAAAcB+CFZoUnQEBAADgjwhWaFKuYEUDCwAAAPgRghWaVErrnxtYAAAAAP6CYIUm1e2nlus/5p9Sfkm5ydUAAAAA7kGwQpOKDA5UUssQSSwHBAAAgP8gWKHJOa+zYjkgAAAA/AXBCk0umc6AAAAA8DMEKzQ5OgMCAADA3xCs0ORSWp8OVntzC1Ve6TC5GgAAAKD5EazQ5Nq0CFFkcIAqqgztzS0yuxwAAACg2RGs0OQsFsvP11mxHBAAAAB+gGCFZuFcDkgDCwAAAPgDghWaxc8t1wlWAAAA8H0EKzSLXy4FNAzD5GoAAACA5kWwQrPoEh+uAKtFJ09VKPNkqdnlAAAAAM2KYIVmYQ+wqXNcuCSuswIAAIDvI1ih2TgbWHCdFQAAAHwdwQrNxtnAghkrAAAA+DqCFZpNCveyAgAAgJ8gWKHZODsDHjpeosLSCpOrAQAAAJoPwQrNJjosSIlRwZKk3dmFJlcDAAAANB/Tg9Wzzz6r9u3bKzg4WAMHDtTWrVvrddzrr78ui8WiCRMmVNs+depUWSyWao8xY8Y0Q+WoD66zAgAAgD8wNVitWbNGc+bM0cMPP6zt27erd+/eGj16tHJzc8943MGDB3XPPfdo2LBhtb4+ZswYZWVluR7/+Mc/mqN81IOzMyDBCgAAAL7M1GC1ZMkSTZ8+XdOmTVNKSoqWL1+u0NBQrVy5ss5jqqqqNGnSJC1YsEAdO3asdR+73a6EhATXIzo6urk+As7COWOVlk2wAgAAgO8KMOuNy8vL9dVXX2nu3LmubVarVSNHjtSWLVvqPG7hwoWKi4vTLbfcok8++aTWfTZt2qS4uDhFR0fr17/+tR555BG1atWqznOWlZWprKzM9byg4HQIqKioUEWFuU0XnO9vdh0N1SU2VNLpa6xOlZYpwGb66lOchbePOXgXxhvcjTEHd2K8+Yb6/n6mBau8vDxVVVUpPj6+2vb4+Hjt3r271mM+/fRTvfjii9qxY0ed5x0zZoyuvvpqdejQQfv27dODDz6osWPHasuWLbLZbLUes3jxYi1YsKDG9vXr1ys0NLT+H6oZpaamml1CgzgMyW61qazSoZfeWadEz/g6UQ/eOubgnRhvcDfGHNyJ8ebdSkpK6rWfacHqXBUWFuo3v/mNXnjhBcXExNS53w033OD6d8+ePdWrVy916tRJmzZt0ogRI2o9Zu7cuZozZ47reUFBgZKSkjRq1ChFRkY23YdogIqKCqWmpurSSy9VYGCgqbU01CuZW/XVoXzFdumrcb0TzS4HZ+ELYw7eg/EGd2PMwZ0Yb77BuZrtbEwLVjExMbLZbMrJyam2PScnRwkJCTX237dvnw4ePKgrrrjCtc3hcEiSAgIClJ6erk6dOtU4rmPHjoqJidHevXvrDFZ2u112u73G9sDAQI/5j8CTajlXKa2j9NWhfP2QW+y1n8EfefOYg/dhvMHdGHNwJ8abd6vvb2faBS9BQUHq16+fNm7c6NrmcDi0ceNGDRo0qMb+3bp1086dO7Vjxw7X48orr9Qll1yiHTt2KCkpqdb3OXLkiI4dO6bERGZKzOLqDJhFAwsAAAD4JlOXAs6ZM0dTpkxR//79NWDAAC1dulTFxcWaNm2aJGny5Mlq06aNFi9erODgYPXo0aPa8S1atJAk1/aioiItWLBAEydOVEJCgvbt26f77rtPnTt31ujRo9362fCzX97LyjAMWSwWkysCAAAAmpapwer666/X0aNHNW/ePGVnZ6tPnz5at26dq6HFoUOHZLXWf1LNZrPp22+/1UsvvaT8/Hy1bt1ao0aN0qJFi2pd6gf36JoQIatFOlZcrqOFZYqLDDa7JAAAAKBJmd68YtasWZo1a1atr23atOmMx65evbra85CQEH344YdNVBmaSnCgTR1jw7U3t0jfZxUQrAAAAOBzuKkQ3OKXywEBAAAAX0Owgls4G1ik0cACAAAAPohgBbdITqQzIAAAAHwXwQpu4VwKeCCvWCXllSZXAwAAADQtghXcIjbCrtgIuwxD2p1daHY5AAAAQJMiWMFtnLNWXGcFAAAAX0Owgtsk0xkQAAAAPopgBbdxdgakgQUAAAB8DcEKbuNcCpieXagqh2FyNQAAAEDTIVjBbTrEhCk40KqS8iplHCs2uxwAAACgyRCs4DY2q0VdE1gOCAAAAN9DsIJbpdDAAgAAAD6IYAW3cjawoOU6AAAAfAnBCm6VkhghiaWAAAAA8C0EK7hV14RIWSxSTkGZjhWVmV0OAAAA0CQIVnCrcHuA2rcKkySlZRWaXA0AAADQNAhWcDtXA4uskyZXAgAAADQNghXcLtl5nRWdAQEAAOAjCFZwO2dnQBpYAAAAwFcQrOB2KYlRkqR9R4tVWlFlcjUAAABA4xGs4HbxkXZFhwaqymFoT06R2eUAAAAAjUawgttZLJZfLAekgQUAAAC8H8EKpnB2BqTlOgAAAHwBwQqmcM1Y0RkQAAAAPoBgBVMkJ/7cGdDhMEyuBgAAAGgcghVM0Sk2XEE2q4rKKnXkxCmzywEAAAAahWAFUwTarDo/IVwS97MCAACA9yNYwTTJCdwoGAAAAL6BYAXT0MACAAAAvoJgBdP83HKdYAUAAADvRrCCaZJ/mrH6Mf+UTpZUmFwNAAAA0HAEK5gmMjhQbaNDJHGdFQAAALwbwQqmSkmkgQUAAAC8H8EKpnI2sOA6KwAAAHgzghVMlZxIZ0AAAAB4P4IVTOVcCrgnt1DllQ6TqwEAAAAahmAFU7WNDlFEcIAqqgztO1pkdjkAAABAgxCsYCqLxfJzAwuWAwIAAMBLEaxgumQ6AwIAAMDLEaxgOmdnQGasAAAA4K0IVjCdcylgWnaBDMMwuRoAAADg3BGsYLou8eEKsFqUX1KhrJOlZpcDAAAAnDOCFUxnD7Cpc1y4JJYDAgAAwDsRrOARXMsBaWABAAAAL0SwgkdwNbAgWAEAAMALEazgEWi5DgAAAG9GsIJHcAarjGMlKiytMLkaAAAA4NwQrOARWoYFKTEqWJKUnl1ocjUAAADAuSFYwWOwHBAAAADeimAFj+HsDEjLdQAAAHgbghU8hrMzIC3XAQAA4G0IVvAYzhmr3dmFqqxymFwNAAAAUH8EK3iM81qGKizIprJKhw7kFZtdDgAAAFBvBCt4DKvVom40sAAAAIAXIljBo6QQrAAAAOCFCFbwKMl0BgQAAIAXIljBo9AZEAAAAN6IYAWP0jU+QlaLlFdUrtzCUrPLAQAAAOqFYAWPEhJkU8fYcEksBwQAAID3IFjB4yTTwAIAAABehmAFj5NCAwsAAAB4GYIVPA4NLAAAAOBtCFbwOMmJEZKk/XnFKimvNLkaAAAA4OwIVvA4cRHBigm3yzCk9OxCs8sBAAAAzopgBY/083JAghUAAAA8H8EKHsnVwCLrpMmVAAAAAGdHsIJHcl5nRWdAAAAAeAOCFTxS95+WAu7OLpTDYZhcDQAAAHBmBCt4pA4x4QoOtKqkvEoZx0vMLgcAAAA4I4IVPJLNalHXBG4UDAAAAO9AsILHSnFeZ0UDCwAAAHg4ghU8lrMzIC3XAQAA4OkIVvBYzntZsRQQAAAAno5gBY/lvMYqu6BUx4rKTK4GAAAAqBvBCh4r3B6g9q1CJbEcEAAAAJ6NYAWP5lwOmJbFckAAAAB4LoIVPJqzgcUughUAAAA8GMEKHi05kQYWAAAA8HwEK3g051LAfUeLVFpRZXI1AAAAQO0IVvBoCZHBig4NVKXD0N7cIrPLAQAAAGpFsIJHs1gsLAcEAACAxyNYwePRwAIAAACejmAFj+e8zopgBQAAAE9FsILHc93LKrNAhmGYXA0AAABQE8EKHq9TbLiCbFYVllXqyIlTZpcDAAAA1ECwgscLtFnVJT5cEssBAQAA4JkIVvAKKXQGBAAAgAcjWMErJNMZEAAAAB7M9GD17LPPqn379goODtbAgQO1devWeh33+uuvy2KxaMKECdW2G4ahefPmKTExUSEhIRo5cqT27NnTDJXDnVydAZmxAgAAgAcyNVitWbNGc+bM0cMPP6zt27erd+/eGj16tHJzc8943MGDB3XPPfdo2LBhNV574okn9PTTT2v58uX64osvFBYWptGjR6u0tLS5PgbcwDlj9WP+KZ08VWFyNQAAAEB1pgarJUuWaPr06Zo2bZpSUlK0fPlyhYaGauXKlXUeU1VVpUmTJmnBggXq2LFjtdcMw9DSpUv1xz/+UePHj1evXr308ssvKzMzU++++24zfxo0p6iQQLWNDpEkpbEcEAAAAB4mwKw3Li8v11dffaW5c+e6tlmtVo0cOVJbtmyp87iFCxcqLi5Ot9xyiz755JNqrx04cEDZ2dkaOXKka1tUVJQGDhyoLVu26IYbbqj1nGVlZSorK3M9Lyg4/Yd7RUWFKirMnR1xvr/ZdXiCbvHhOnLilHYeOaF+SZFml+OzGHNwJ8Yb3I0xB3divPmG+v5+pgWrvLw8VVVVKT4+vtr2+Ph47d69u9ZjPv30U7344ovasWNHra9nZ2e7zvHf53S+VpvFixdrwYIFNbavX79eoaGhZ/oYbpOammp2CaYLKLJKsmrDtjTFnfje7HJ8HmMO7sR4g7sx5uBOjDfvVlJSUq/9TAtW56qwsFC/+c1v9MILLygmJqZJzz137lzNmTPH9bygoEBJSUkaNWqUIiPNnRmpqKhQamqqLr30UgUGBppai9kCd+Vq3T92qDAgSuPGDTK7HJ/FmIM7Md7gbow5uBPjzTc4V7OdjWnBKiYmRjabTTk5OdW25+TkKCEhocb++/bt08GDB3XFFVe4tjkcDklSQECA0tPTXcfl5OQoMTGx2jn79OlTZy12u112u73G9sDAQI/5j8CTajFLz6RoSdLe3GIZFpuCAkxvaunTGHNwJ8Yb3I0xB3divHm3+v52pv1lGhQUpH79+mnjxo2ubQ6HQxs3btSgQTVnI7p166adO3dqx44drseVV16pSy65RDt27FBSUpI6dOighISEaucsKCjQF198Ues54V3aRocowh6g8iqH9h0tMrscAAAAwMXUpYBz5szRlClT1L9/fw0YMEBLly5VcXGxpk2bJkmaPHmy2rRpo8WLFys4OFg9evSodnyLFi0kqdr22bNn65FHHlGXLl3UoUMHPfTQQ2rdunWN+13B+1gsFiW3jtTWA8eVllXgasEOAAAAmM3UYHX99dfr6NGjmjdvnrKzs9WnTx+tW7fO1Xzi0KFDslrPbVLtvvvuU3FxsWbMmKH8/HwNHTpU69atU3BwcHN8BLhZSuLpYLUrs0BXX2B2NQAAAMBppjevmDVrlmbNmlXra5s2bTrjsatXr66xzWKxaOHChVq4cGETVAdPk/LTLNUu7mUFAAAAD8LV//AqKa1PB6u0rAIZhmFyNQAAAMBpBCt4lc5x4QqwWnSipELZBaVmlwMAAABIIljBywQH2tQpNlyStCuT5YAAAADwDAQreB3nckCCFQAAADwFwQpex9nAIi2bYAUAAADPQLCC12HGCgAAAJ6GYAWv47wx8MFjJSoqqzS5GgAAAIBgBS/UMixICZGnb/icznJAAAAAeACCFbwSywEBAADgSQhW8ErJiRGSpF1ZBCsAAACYj2AFr5SSGCWJGSsAAAB4BoIVvJJzKeDu7EJVVjlMrgYAAAD+jmAFr9SuZahCg2wqq3To4LFis8sBAACAnyNYwStZrRZ1Szh9ndX3LAcEAACAyQhW8FrO5YBpWYUmVwIAAAB/R7CC13I1sKAzIAAAAExGsILX4l5WAAAA8BQEK3itrvERslqkvKIy5RaWml0OAAAA/BjBCl4rJMimDjFhkrjOCgAAAOYiWMGrpbTmRsEAAAAwH8EKXi058XTLdRpYAAAAwEwEK3i1lERny3WCFQAAAMxDsIJXc3YG3H+0SKfKq0yuBgAAAP6KYAWvFhcRrJhwuxyGlJ5DAwsAAACYg2AFr+e6zooGFgAAADAJwQpez7kckOusAAAAYBaCFbyes4EFnQEBAABgFoIVvN4vOwM6HIbJ1QAAAMAfEazg9TrEhMkeYFVJeZUOHS8xuxwAAAD4IYIVvF6AzapuCdwoGAAAAOYhWMEnOBtY0BkQAAAAZiBYwSck08ACAAAAJiJYwSf8soEFAAAA4G4EK/iEbj8Fq6yTpTpeXG5yNQAAAPA3BCv4hHB7gNq1CpXErBUAAADcj2AFn8FyQAAAAJiFYAWf4QxWdAYEAACAuxGs4DNcLdeZsQIAAICbEazgM5wt1/fmFqmsssrkagAAAOBPCFbwGYlRwWoRGqhKh6E9OUVmlwMAAAA/QrCCz7BYLD9fZ8VyQAAAALgRwQo+JZkGFgAAADABwQo+hZbrAAAAMAPBCj7ll50BDcMwuRoAAAD4C4IVfEqn2HAF2awqLK3UkROnzC4HAAAAfoJgBZ8SFGBV57hwSTSwAAAAgPsQrOBznMsBuc4KAAAA7kKwgs9JoTMgAAAA3IxgBZ+TzL2sAAAA4GYEK/gc54zVkROndPJUhcnVAAAAwB8EmF0A0NSiQgPVpkWIfsw/pVe/yFDHmDCzS/IqlZVV+uaYRbbvcxQQYDO7HPg4xhvcjTEHd2K8NU5MuF3927c0u4x6I1jBJ6W0jtSP+af0xLp0s0vxUjat/OEbs4uA32C8wd0Yc3AnxltDXXR+rF7+7QCzy6g3ghV80m0Xd1RJeaXKKhxml+J1DMPQ8RMn1DI6WhaLxexy4OMYb3A3xhzcifHWOOf/dAsdb0Gwgk/q166lXr31V2aX4ZUqKir0/vvva9y4AQoMDDS7HPg4xhvcjTEHd2K8+ReaVwAAAABAIxGsAAAAAKCRCFYAAAAA0EgEKwAAAABoJIIVAAAAADQSwQoAAAAAGolgBQAAAACNRLACAAAAgEYiWAEAAABAIxGsAAAAAKCRCFYAAAAA0EgEKwAAAABoJIIVAAAAADQSwQoAAAAAGolgBQAAAACNRLACAAAAgEYiWAEAAABAIxGsAAAAAKCRAswuwBMZhiFJKigoMLkSqaKiQiUlJSooKFBgYKDZ5cAPMObgTow3uBtjDu7EePMNzkzgzAh1IVjVorCwUJKUlJRkciUAAAAAPEFhYaGioqLqfN1inC16+SGHw6HMzExFRETIYrGYWktBQYGSkpJ0+PBhRUZGmloL/ANjDu7EeIO7MebgTow332AYhgoLC9W6dWtZrXVfScWMVS2sVqvatm1rdhnVREZG8h8k3IoxB3divMHdGHNwJ8ab9zvTTJUTzSsAAAAAoJEIVgAAAADQSAQrD2e32/Xwww/LbrebXQr8BGMO7sR4g7sx5uBOjDf/QvMKAAAAAGgkZqwAAAAAoJEIVgAAAADQSAQrAAAAAGgkghUAAAAANBLBysM9++yzat++vYKDgzVw4EBt3brV7JLggxYvXqwLL7xQERERiouL04QJE5Senm52WfAjf/rTn2SxWDR79myzS4GP+vHHH3XzzTerVatWCgkJUc+ePfXll1+aXRZ8VFVVlR566CF16NBBISEh6tSpkxYtWiR6xvk2gpUHW7NmjebMmaOHH35Y27dvV+/evTV69Gjl5uaaXRp8zEcffaSZM2fq888/V2pqqioqKjRq1CgVFxebXRr8wLZt2/T888+rV69eZpcCH3XixAkNGTJEgYGB+uCDD7Rr1y795S9/UXR0tNmlwUc9/vjjeu6557Rs2TKlpaXp8ccf1xNPPKFnnnnG7NLQjGi37sEGDhyoCy+8UMuWLZMkORwOJSUl6c4779QDDzxgcnXwZUePHlVcXJw++ugjXXTRRWaXAx9WVFSkCy64QP/zP/+jRx55RH369NHSpUvNLgs+5oEHHtDmzZv1ySefmF0K/MTll1+u+Ph4vfjii65tEydOVEhIiF555RUTK0NzYsbKQ5WXl+urr77SyJEjXdusVqtGjhypLVu2mFgZ/MHJkyclSS1btjS5Evi6mTNn6rLLLqv2f+uAprZ27Vr1799f1157reLi4tS3b1+98MILZpcFHzZ48GBt3LhRP/zwgyTpm2++0aeffqqxY8eaXBmaU4DZBaB2eXl5qqqqUnx8fLXt8fHx2r17t0lVwR84HA7Nnj1bQ4YMUY8ePcwuBz7s9ddf1/bt27Vt2zazS4GP279/v5577jnNmTNHDz74oLZt26a77rpLQUFBmjJlitnlwQc98MADKigoULdu3WSz2VRVVaVHH31UkyZNMrs0NCOCFYBqZs6cqe+++06ffvqp2aXAhx0+fFh33323UlNTFRwcbHY58HEOh0P9+/fXY489Jknq27evvvvuOy1fvpxghWbxxhtv6NVXX9Vrr72m7t27a8eOHZo9e7Zat27NmPNhBCsPFRMTI5vNppycnGrbc3JylJCQYFJV8HWzZs3Se++9p48//lht27Y1uxz4sK+++kq5ubm64IILXNuqqqr08ccfa9myZSorK5PNZjOxQviSxMREpaSkVNuWnJyst99+26SK4OvuvfdePfDAA7rhhhskST179lRGRoYWL15MsPJhXGPloYKCgtSvXz9t3LjRtc3hcGjjxo0aNGiQiZXBFxmGoVmzZumdd97Rf/7zH3Xo0MHskuDjRowYoZ07d2rHjh2uR//+/TVp0iTt2LGDUIUmNWTIkBq3kPjhhx/Url07kyqCryspKZHVWv3PbJvNJofDYVJFcAdmrDzYnDlzNGXKFPXv318DBgzQ0qVLVVxcrGnTppldGnzMzJkz9dprr+lf//qXIiIilJ2dLUmKiopSSEiIydXBF0VERNS4hi8sLEytWrXi2j40ud/97ncaPHiwHnvsMV133XXaunWrVqxYoRUrVphdGnzUFVdcoUcffVTnnXeeunfvrq+//lpLlizRb3/7W7NLQzOi3bqHW7Zsmf785z8rOztbffr00dNPP62BAweaXRZ8jMViqXX7qlWrNHXqVPcWA781fPhw2q2j2bz33nuaO3eu9uzZow4dOmjOnDmaPn262WXBRxUWFuqhhx7SO++8o9zcXLVu3Vo33nij5s2bp6CgILPLQzMhWAEAAABAI3GNFQAAAAA0EsEKAAAAABqJYAUAAAAAjUSwAgAAAIBGIlgBAAAAQCMRrAAAAACgkQhWAAAAANBIBCsAAAAAaCSCFQAAjWSxWPTuu++aXQYAwEQEKwCAV5s6daosFkuNx5gxY8wuDQDgRwLMLgAAgMYaM2aMVq1aVW2b3W43qRoAgD9ixgoA4PXsdrsSEhKqPaKjoyWdXqb33HPPaezYsQoJCVHHjh311ltvVTt+586d+vWvf62QkBC1atVKM2bMUFFRUbV9Vq5cqe7du8tutysxMVGzZs2q9npeXp6uuuoqhYaGqkuXLlq7dq3rtRMnTmjSpEmKjY1VSEiIunTpUiMIAgC8G8EKAODzHnroIU2cOFHffPONJk2apBtuuEFpaWmSpOLiYo0ePVrR0dHatm2b3nzzTW3YsKFacHruuec0c+ZMzZgxQzt37tTatWvVuXPnau+xYMECXXfddfr22281btw4TZo0ScePH3e9/65du/TBBx8oLS1Nzz33nGJiYtz3BQAAmp3FMAzD7CIAAGioqVOn6pVXXlFwcHC17Q8++KAefPBBWSwW3XbbbXruuedcr/3qV7/SBRdcoP/5n//RCy+8oPvvv1+HDx9WWFiYJOn999/XFVdcoczMTMXHx6tNmzaaNm2aHnnkkVprsFgs+uMf/6hFixZJOh3WwsPD9cEHH2jMmDG68sorFRMTo5UrVzbTtwAAMBvXWAEAvN4ll1xSLThJUsuWLV3/HjRoULXXBg0apB07dkiS0tLS1Lt3b1eokqQhQ4bI4XAoPT1dFotFmZmZGjFixBlr6NWrl+vfYWFhioyMVG5uriTp9ttv18SJE7V9+3aNGjVKEyZM0ODBgxv0WQEAnolgBQDwemFhYTWW5jWVkJCQeu0XGBhY7bnFYpHD4ZAkjR07VhkZGXr//feVmpqqESNGaObMmXryySebvF4AgDm4xgoA4PM+//zzGs+Tk5MlScnJyfrmm29UXFzsen3z5s2yWq3q2rWrIiIi1L59e23cuLFRNcTGxmrKlCl65ZVXtHTpUq1YsaJR5wMAeBZmrAAAXq+srEzZ2dnVtgUEBLgaRLz55pvq37+/hg4dqldffVVbt27Viy++KEmaNGmSHn74YU2ZMkXz58/X0aNHdeedd+o3v/mN4uPjJUnz58/Xbbfdpri4OI0dO1aFhYXavHmz7rzzznrVN2/ePPXr10/du3dXWVmZ3nvvPVewAwD4BoIVAMDrrVu3TomJidW2de3aVbt375Z0umPf66+/rjvuuEOJiYn6xz/+oZSUFElSaGioPvzwQ91999268MILFRoaqokTJ2rJkiWuc02ZMkWlpaV66qmndM899ygmJkbXXHNNvesLCgrS3LlzdfDgQYWEhGjYsGF6/fXXm+CTAwA8BV0BAQA+zWKx6J133tGECRPMLgUA4MO4xgoAAAAAGolgBQAAAACNxDVWAACfxop3AIA7MGMFAAAAAI1EsAIAAACARiJYAQAAAEAjEawAAAAAoJEIVgAAAADQSAQrAAAAAGgkghUAAAAANBLBCgAAAAAa6f8DnK9maPgjR14AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train the traditional /content/DATA/run_1/original_result/x_train.npy\n",
        "\n",
        "network_setting = input(\"Do you want to modify the traditional network setup? (yes/no): \").strip().lower()\n",
        "\n",
        "# Default parameters\n",
        "hidden_layers_TN = None\n",
        "hidden_num_neurons_TN = None\n",
        "epochs_TN = None\n",
        "\n",
        "if network_setting == 'yes':\n",
        "  hidden_layers_TN = int(input(\"Enter the number of hidden layers for the traditional network: \"))\n",
        "  hidden_num_neurons_TN = [int(input(f\"Enter the number of neurons for hidden layer {i+1}: \")) for i in range(hidden_layers_TN)]\n",
        "  epochs_TN = int(input(\"Enter the number of epochs for the traditional network: \"))\n",
        "\n",
        "# Initialize a list to store the errors for each run\n",
        "errors = []\n",
        "for run_number in range(1, num_runs + 1):\n",
        "    # Define the run folder based on run number\n",
        "    # run_folder = os.path.join(base_folder, f\"run_{run_number}\")\n",
        "    run_folder = f\"/content/{base_folder}/run_{run_number}/original_result\"\n",
        "    print(f\"run_folder: {run_folder}\")\n",
        "\n",
        "    # Load data and parameters for the current run\n",
        "    num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_lower_bound, func_expressions, num_layers, num_neurons, num_input_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, bias_values, tau_values, x_train, y_train = load_run_data(run_folder, base_folder)\n",
        "\n",
        "    # Determine parameters for the traditional network (use modified if set, otherwise default to custom values)\n",
        "    epochs_to_use = epochs_TN if epochs_TN else epochs\n",
        "    hidden_layers_to_use = hidden_layers_TN if hidden_layers_TN else num_layers - 1  # Subtract input/output layers\n",
        "    hidden_neurons_to_use = hidden_num_neurons_TN if hidden_num_neurons_TN else num_neurons[1:]  # Middle layers\n",
        "\n",
        "    # Initialize the traditional neural network with specified or default parameters\n",
        "    traditional_nn = create_traditional_nn(\n",
        "        input_dim=num_input_neurons,\n",
        "        output_dim=output_num_neurons,\n",
        "        hidden_layers=hidden_layers_to_use,\n",
        "        neurons_per_layer=hidden_neurons_to_use,\n",
        "        learning_rate=bias_learning_rate\n",
        "    )\n",
        "\n",
        "    # Initialize a custom logger to track progress\n",
        "    custom_logger = CustomLogger()\n",
        "\n",
        "     # Train the traditional network and store the error for each run\n",
        "    history = traditional_nn.fit(x_train, y_train, epochs=epochs_to_use, callbacks=[custom_logger])\n",
        "\n",
        "    # Store the final loss (error) for the current run\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    errors.append(final_loss)\n",
        "\n",
        "    # Plot the loss over epochs for this run\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Loss over Epochs for Run {run_number}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjm2XJP9WIUc",
        "outputId": "92875b39-c06a-419c-c62f-c6d2915e17b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Run Comparison:\n",
            "+-------+----------------------+---------------------------+\n",
            "|   Run |   Custom Model Error |   Traditional Model Error |\n",
            "+=======+======================+===========================+\n",
            "|     1 |                 0.37 |                     0.365 |\n",
            "+-------+----------------------+---------------------------+\n",
            "|     2 |                 0.42 |                     0.41  |\n",
            "+-------+----------------------+---------------------------+\n",
            "|     3 |                 0.39 |                     0.385 |\n",
            "+-------+----------------------+---------------------------+\n",
            "|     4 |                 0.39 |                     0.385 |\n",
            "+-------+----------------------+---------------------------+\n",
            "|     5 |                 0.38 |                     0.37  |\n",
            "+-------+----------------------+---------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Print combined summary\n",
        "print(\"\\nRun Comparison:\")\n",
        "headers = [\"Run\", \"Custom Model Error\", \"Traditional Model Error\"]\n",
        "table = []\n",
        "\n",
        "for run_number in range(num_runs):\n",
        "    table.append([\n",
        "        run_number + 1,\n",
        "        former_run_summary[run_number][1],\n",
        "        errors[run_number]\n",
        "    ])\n",
        "\n",
        "print(tabulate(table, headers, tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oeqv3R1RWNi0"
      },
      "source": [
        "# **To Download a Folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "TmDo7IIAlpRi",
        "outputId": "8f97ac2c-5659-4e9c-df66-7cb89a8b74da"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-91c983d23a0d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mzip_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'downloaded_run_folder'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdownload_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-91c983d23a0d>\u001b[0m in \u001b[0;36mdownload_folder\u001b[0;34m(base_folder, zip_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Ask user for the run number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mrun_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the run number to download: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"run_{run_number}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "\n",
        "# This will compress the folder into a zip file and offer it for download\n",
        "\n",
        "def download_folder(base_folder, zip_name):\n",
        "    \"\"\"\n",
        "    Compresses the specified run folder into a zip file for download.\n",
        "\n",
        "    Parameters:\n",
        "    base_folder (str): The base folder where the run folders are located.\n",
        "    zip_name (str): The name of the output zip file (without extension).\n",
        "    \"\"\"\n",
        "\n",
        "    # Ask user for the run number\n",
        "    run_number = input(\"Enter the run number to download: \")\n",
        "    folder_path = os.path.join(base_folder, f\"run_{run_number}\")\n",
        "\n",
        "    # Ensure the folder exists\n",
        "    if os.path.exists(folder_path):\n",
        "        zip_path = shutil.make_archive(zip_name, 'zip', base_folder, f\"run_{run_number}\")\n",
        "        print(f\"Folder '{folder_path}' has been compressed into '{zip_name}.zip'.\")\n",
        "\n",
        "        # Trigger download\n",
        "        files.download(f\"{zip_name}.zip\")\n",
        "        print(f\"Folder '{zip_name}' downloded in your local directory.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder_path}' not found.\")\n",
        "\n",
        "# Base folder path where the run folders are located, e.g., '/content/DATA'\n",
        "base_folder = '/content/DATA'\n",
        "# Zip file name\n",
        "zip_name = 'downloaded_run_folder'\n",
        "\n",
        "download_folder(base_folder, zip_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdoQjKu21Fk5"
      },
      "outputs": [],
      "source": [
        "5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qV6g9o5xiED"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import sympy as sp\n",
        "\n",
        "# # Function to check if the operation is logical (like AND/OR)\n",
        "# def is_logical_operation(func_str):\n",
        "#     logical_ops = ['&', '|', 'and', 'or']  # Add any other logical operators you want to support\n",
        "#     return any(op in func_str for op in logical_ops)\n",
        "\n",
        "# # Ask user for number of inputs\n",
        "# num_inputs = int(input(\"Enter the number of inputs (dimension): \"))\n",
        "\n",
        "# # Ask user for range for input variable\n",
        "# lower_bound = float(input(\"Enter the lower bound of input range: \"))\n",
        "# upper_bound = float(input(\"Enter the upper bound of input range: \"))\n",
        "\n",
        "# # Ask user for the number of training inputs in the dataset\n",
        "# num_training_inputs = int(input(\"Enter the number of training inputs in the dataset: \"))\n",
        "\n",
        "# # Create X_train dataset with random values uniformly between the bounds\n",
        "# X_train = np.random.uniform(lower_bound, upper_bound, (num_training_inputs, num_inputs))\n",
        "# # print(\"\\nX_train:\")\n",
        "# # print(X_train)\n",
        "# # print(X_train.dtype, X_train.shape)\n",
        "# X_train = [\n",
        "#     [0.27, 0.80],\n",
        "#     [0.82, 0.64],\n",
        "#     [0.16, 0.65],\n",
        "#     [0.57, 0.10],\n",
        "#     [0.12, 0.84],\n",
        "#     [0.83, 0.93],\n",
        "#     [0.61, 0.39],\n",
        "#     [0.44, 0.62],\n",
        "#     [0.63, 0.13],\n",
        "#     [0.45, 0.73]\n",
        "# ]\n",
        "\n",
        "# X_train = np.array(X_train)\n",
        "# print(\"\\nX_train:\")\n",
        "# print(X_train)\n",
        "# print(\"\")\n",
        "# print(X_train.dtype, X_train.shape)\n",
        "\n",
        "# # Ask user for number of outputs\n",
        "# num_outputs = int(input(\"\\nEnter the number of outputs: \"))\n",
        "\n",
        "# # Create y_train by asking user to provide functions for each output\n",
        "# y_train = []\n",
        "# x_symbols = sp.symbols(f'x0:{num_inputs}')  # Creates symbols x0, x1, x2, ..., up to num_inputs\n",
        "# print('x_symbols: ', x_symbols)  # Debug statement\n",
        "\n",
        "# for i in range(num_outputs):\n",
        "#     # Ask user for the function\n",
        "#     func_str = input(f\"Enter y_train({i + 1}) function using symbols x0, x1, x2, ...(use '&' for AND, '|' for OR): \")\n",
        "#     print(f\"\\nfunc_str: {func_str}\")\n",
        "\n",
        "\n",
        "#     if is_logical_operation(func_str):\n",
        "#         # Apply thresholding if the function is a logical operation\n",
        "#         binary_x_train = (X_train > 0.5).astype(int)  # Convert to binary values based on threshold\n",
        "#         print(f\"Binary X_train for y_train({i + 1}):\\n{binary_x_train}\")  # Debug statement\n",
        "\n",
        "#         # Log the function string\n",
        "\n",
        "#         func = sp.sympify(func_str)  # Convert string to sympy expression\n",
        "#         print ('func :',  func)\n",
        "#         func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
        "\n",
        "#         # Evaluate the function for each row\n",
        "#         y_values = np.array([func_callable(*binary_x_train[row]) for row in range(num_training_inputs)])\n",
        "#         print ('y_values before converting to binary:',  y_values)\n",
        "\n",
        "#         # Convert boolean results to binary (0 and 1)\n",
        "#         y_values = y_values.astype(int)\n",
        "#         print ('y_values after converting to binary:',  y_values)\n",
        "#         print(f\"Type of y_values: {type(y_values)}\")\n",
        "\n",
        "\n",
        "#         # Debugging output\n",
        "#         # for row in range(num_training_inputs):\n",
        "#         #     print(f\"Evaluating row {row}: X = {binary_x_train[row]}, Output = {y_values[row]}\")  # Debug statement\n",
        "\n",
        "#     else:\n",
        "#         # For mathematical functions, apply directly on continuous values\n",
        "#         func = sp.sympify(func_str)  # Convert string to sympy expression\n",
        "#         func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
        "#         y_values = np.array([func_callable(*X_train[row]) for row in range(num_training_inputs)])\n",
        "\n",
        "#         # Debugging output\n",
        "#         for row in range(num_training_inputs):\n",
        "#             print(f\"Evaluating row {row}: X = {X_train[row]}, Output = {y_values[row]}\")  # Debug statement\n",
        "\n",
        "#     # Append results to y_train\n",
        "#     y_train.append(y_values)\n",
        "\n",
        "# # Convert y_train to numpy array and transpose to match the format (num_training_inputs, num_outputs)\n",
        "# y_train = np.array(y_train).T\n",
        "\n",
        "# print(\"\\ny_train:\")\n",
        "# print(y_train)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
