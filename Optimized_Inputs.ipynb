{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lovHNIh5BuDv"
   },
   "source": [
    "# Modified Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qcggmytf_eLl"
   },
   "source": [
    "## Objective\n",
    "\n",
    "The objective of this notebook is to modify the Neural Network that runs on custom tau and bias values and dataset. This customization aims to aid in understanding the model training process and the data flow within the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY7ekxTvBz2D"
   },
   "source": [
    "## Step 1: Import libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./mlp/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in ./mlp/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in ./mlp/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: sympy in ./mlp/lib/python3.12/site-packages (1.13.3)\n",
      "Requirement already satisfied: tabulate in ./mlp/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: reportlab in ./mlp/lib/python3.12/site-packages (4.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./mlp/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./mlp/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./mlp/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./mlp/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./mlp/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./mlp/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./mlp/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./mlp/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./mlp/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./mlp/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./mlp/lib/python3.12/site-packages (from sympy) (1.3.0)\n",
      "Requirement already satisfied: chardet in ./mlp/lib/python3.12/site-packages (from reportlab) (5.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./mlp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib sympy tabulate reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1vcnRkRk6XS",
    "outputId": "128a8107-8b35-47f8-d3ee-a848e102bb14"
   },
   "outputs": [],
   "source": [
    "#Import the dataset and other python libraries\n",
    "\n",
    "# !pip install reportlab\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import sys\n",
    "import sympy as sp\n",
    "\n",
    "from tabulate import tabulate\n",
    "from array import array\n",
    "from os.path import join\n",
    "# from google.colab import files\n",
    "# from google.colab import drive\n",
    "from zipfile import ZipFile\n",
    "import zipfile\n",
    "# from google.colab import files\n",
    "from io import StringIO\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.utils import ImageReader\n",
    "\n",
    "def clear_seed():\n",
    "    np.random.seed(None)\n",
    "    random.seed(None)\n",
    "    os.environ.pop('PYTHONHASHSEED', None)\n",
    "\n",
    "# Call clear_seed at the beginning of the script\n",
    "clear_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "DFA2zc3YkkQe"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def reset_seed():\n",
    "    np.random.seed(None)\n",
    "    random.seed(None)\n",
    "    os.environ.pop('PYTHONHASHSEED', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Jrg10v8KSvyT"
   },
   "outputs": [],
   "source": [
    "#This code handles the folders creation and saving results to review after the run\n",
    "\n",
    "def create_run_folder(base_folder, run_number): # Create a folder for each run\n",
    "    run_folder = os.path.join(base_folder, f\"run_{run_number}\")\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    return run_folder\n",
    "\n",
    "\n",
    "def save_results(folder_path, run_number, x_train, y_train, bias_values, tau_values, errors, final_bias_values, final_tau_values, output_text, subfolder_name=None):\n",
    "    \"\"\"\n",
    "    Save the results under the specified folder path.\n",
    "    If subfolder_name is None, files will be saved directly under the folder path.\n",
    "    \"\"\"\n",
    "    # If subfolder_name is provided, create it under the folder path; otherwise, use the main folder path\n",
    "    if subfolder_name:\n",
    "        subfolder = os.path.join(folder_path, subfolder_name)\n",
    "    else:\n",
    "        subfolder = folder_path\n",
    "\n",
    "    os.makedirs(subfolder, exist_ok=True)\n",
    "\n",
    "    # Save x_train and y_train as .npy\n",
    "    np.save(os.path.join(subfolder, \"x_train.npy\"), x_train)\n",
    "    np.save(os.path.join(subfolder, \"y_train.npy\"), y_train)\n",
    "\n",
    "     # Save x_train and y_train as CSV files\n",
    "    pd.DataFrame(x_train).to_csv(os.path.join(subfolder, \"x_train.csv\"), index=False)\n",
    "    pd.DataFrame(y_train).to_csv(os.path.join(subfolder, \"y_train.csv\"), index=False)\n",
    "\n",
    "    # Save x_train and y_train as .json\n",
    "    pd.DataFrame(x_train).to_json(os.path.join(subfolder, \"x_train.json\"))\n",
    "    pd.DataFrame(y_train).to_json(os.path.join(subfolder, \"y_train.json\"))\n",
    "\n",
    "    # Save bias_values and tau_values as .json\n",
    "    with open(os.path.join(subfolder, \"bias_values.json\"), \"w\") as f:\n",
    "        json.dump([b.tolist() for b in bias_values], f, indent=4)\n",
    "    with open(os.path.join(subfolder, \"tau_values.json\"), \"w\") as f:\n",
    "        json.dump([t.tolist() for t in tau_values], f, indent=4)\n",
    "\n",
    "    # Save final bias_values and tau_values as .json\n",
    "    with open(os.path.join(subfolder, \"final_bias_values.json\"), \"w\") as f:\n",
    "        json.dump([b.tolist() for b in final_bias_values], f, indent=4)\n",
    "    with open(os.path.join(subfolder, \"final_tau_values.json\"), \"w\") as f:\n",
    "        json.dump([t.tolist() for t in final_tau_values], f, indent=4)\n",
    "\n",
    "    # Save the errors array\n",
    "    errors_file_path = os.path.join(subfolder, \"errors.npy\")\n",
    "    np.save(errors_file_path, errors)\n",
    "    with open(os.path.join(subfolder, \"errors.json\"), \"w\") as f:\n",
    "        json.dump([b.tolist() for b in errors], f, indent=4)\n",
    "    print(f\"Errors saved to: {errors_file_path}\")\n",
    "\n",
    "    # Save the error plot\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(errors) + 1), errors, marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Error')\n",
    "    plt.title('Error Rate Across Epochs')\n",
    "    plt.savefig(os.path.join(subfolder, \"error_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Save console output as .json\n",
    "    output_file_json = os.path.join(subfolder, \"output.json\")\n",
    "    with open(output_file_json, \"w\") as f:\n",
    "        json.dump({\"output\": output_text}, f, indent=4)\n",
    "\n",
    "    # Save console output as .pdf\n",
    "    output_file_pdf = os.path.join(subfolder, \"output.pdf\")\n",
    "    save_output_as_pdf(output_text, os.path.join(subfolder, \"error_plot.png\"), output_file_pdf, run_number)\n",
    "\n",
    "    # Copy parameters.json file to the run's subfolder\n",
    "    original_parameters_path = \"/content/DATA/parameters.json\"\n",
    "    parameters_destination_path = os.path.join(folder_path, \"parameters.json\")\n",
    "    shutil.copyfile(original_parameters_path, parameters_destination_path)\n",
    "\n",
    "def save_output_as_pdf(output_text, plot_file, file_path, run_number):\n",
    "    c = canvas.Canvas(file_path, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    # Add a big title with the run number\n",
    "    c.setFont(\"Helvetica-Bold\", 20)\n",
    "    c.drawString(50, height - 50, f\"Run Number: {run_number}\")\n",
    "\n",
    "    # Add the console output text\n",
    "    c.setFont(\"Helvetica\", 10)\n",
    "    x_margin = 50\n",
    "    y_margin = height - 100\n",
    "    line_height = 12\n",
    "\n",
    "    lines = output_text.splitlines()\n",
    "    y_position = y_margin\n",
    "    for line in lines:\n",
    "        if y_position < 50:  # start a new page if there's no more space\n",
    "            c.showPage()\n",
    "            y_position = height - 50\n",
    "        c.drawString(x_margin, y_position, line)\n",
    "        y_position -= line_height\n",
    "\n",
    "    # Add the error plot to the PDF\n",
    "    if os.path.exists(plot_file):\n",
    "        c.showPage()  # Start a new page for the plot\n",
    "        c.drawImage(ImageReader(plot_file), 100, 400, width=400, height=300)\n",
    "\n",
    "    c.save()\n",
    "\n",
    "def save_console_output(run_number, output_text, final_error, run_folder, is_bias_update=False):\n",
    "    # Determine the subfolder based on whether it's a bias update or not\n",
    "    subfolder_name = \"bias_update_result\" if is_bias_update else \"original_result\"\n",
    "    run_folder_path = os.path.join(run_folder, subfolder_name)\n",
    "\n",
    "    # Ensure the run_folder exists\n",
    "    os.makedirs(run_folder_path, exist_ok=True)\n",
    "\n",
    "    # Save console output as JSON\n",
    "    output_file_json = os.path.join(run_folder_path, \"output.json\")\n",
    "    with open(output_file_json, \"w\") as f:\n",
    "        json.dump({\"output\": output_text}, f, indent=4)\n",
    "\n",
    "    # Define the path to the existing error plot (if any)\n",
    "    plot_file = os.path.join(run_folder_path, \"error_plot.png\")\n",
    "\n",
    "    # Save console output as PDF\n",
    "    output_file_pdf = os.path.join(run_folder_path, \"output.pdf\")\n",
    "    save_output_as_pdf(output_text, plot_file, output_file_pdf, run_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nvo9ienknvhQ"
   },
   "outputs": [],
   "source": [
    "#Loads the particular data for specific re-run\n",
    "\n",
    "def load_run_data(run_folder, base_folder):\n",
    "    parameters_file_path = os.path.join(base_folder, \"parameters.json\")\n",
    "    with open(parameters_file_path, \"r\") as file:\n",
    "        parameters = json.load(file)\n",
    "\n",
    "    num_training_inputs = parameters['num_training_inputs']\n",
    "    lower_bound = parameters['lower_bound']\n",
    "    max_lower_bound = parameters['max_lower_bound']\n",
    "    upper_bound = parameters['upper_bound']\n",
    "    min_upper_bound = parameters['min_upper_bound']\n",
    "    func_expressions = parameters['func_expressions']\n",
    "    num_layers = parameters['num_layers']\n",
    "    num_neurons = parameters['num_neurons']\n",
    "    epochs = parameters['epochs']\n",
    "    bias_learning_rate = parameters['bias_learning_rate']\n",
    "    tau_learning_rate = parameters['tau_learning_rate']\n",
    "    tau_bouncing_mode = parameters['tau_bouncing_mode']\n",
    "    output_num_neurons =  parameters['output_num_neurons']\n",
    "    num_input_neurons =  parameters['num_input_neurons']\n",
    "\n",
    "    # Load x_train and y_train\n",
    "    x_train = np.load(os.path.join(run_folder, \"x_train.npy\"))\n",
    "    y_train = np.load(os.path.join(run_folder, \"y_train.npy\"))\n",
    "\n",
    "    # Load bias_values and tau_values\n",
    "    with open(os.path.join(run_folder, \"bias_values.json\"), \"r\") as f:\n",
    "        bias_values = [np.array(b) for b in json.load(f)]\n",
    "    with open(os.path.join(run_folder, \"tau_values.json\"), \"r\") as f:\n",
    "        tau_values = [np.array(t) for t in json.load(f)]\n",
    "\n",
    "    return num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, num_layers, num_neurons, num_input_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, x_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "cLdYDRTURvZ0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def rerun_specific_run(run_number, base_folder):\n",
    "    set_seed(42)  # Set the seed only for reruns\n",
    "\n",
    "    # Define the folder where the run data is stored\n",
    "    run_folder = os.path.join(base_folder, f\"run_{run_number}\", 'original_result')\n",
    "\n",
    "    if not os.path.exists(run_folder):\n",
    "        print(f\"Run folder for run number {run_number} does not exist.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Rerunning run number: {run_number} from folder: {run_folder}\")\n",
    "\n",
    "    # Load data from the run folder\n",
    "    num_training_inputs, lower_bound, max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, num_input_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, x_train, y_train = load_run_data(run_folder, base_folder)\n",
    "\n",
    "    net = Network()\n",
    "    output_num_neurons = num_neurons[-1] #added by KW - present in the original code, not sure the imapct of this change\n",
    "    for i in range(num_layers):\n",
    "        if i == num_layers - 1:  # last layer\n",
    "            net.add(FCLayer(num_neurons[i], output_num_neurons, f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
    "        else:\n",
    "            net.add(FCLayer(num_neurons[i], num_neurons[i + 1], f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
    "        net.add(ActivationLayer(ReLU))\n",
    "\n",
    "    net.use(mse)\n",
    "\n",
    "    # No capturing of console output, just printing everything to the console - not saving the results\n",
    "    errors, final_bias_values, final_tau_values = net.fit(x_train, y_train, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode)\n",
    "\n",
    "    final_error = errors[-1]\n",
    "    print(f\"Re-Run of run number {run_number} completed with final error: {final_error}\")\n",
    "\n",
    "    reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "A_C6hC97xEay"
   },
   "outputs": [],
   "source": [
    "# function to check if the last two epochs' error difference is constant\n",
    "def is_error_stable(errors):\n",
    "    # Set a tolerance level for floating-point comparisons\n",
    "    tolerance = 1e-10\n",
    "\n",
    "    # Ensure we have at least three error values to compare\n",
    "    if len(errors) < 3:\n",
    "        return False  # Not enough data to determine consistency\n",
    "\n",
    "    # Calculate differences between consecutive error values\n",
    "    print('error 1 :',errors[-1])\n",
    "    print('error 2 :',errors[-2])\n",
    "    print('error 3 :',errors[-3])\n",
    "    diffs = np.diff(errors[-3:])\n",
    "\n",
    "    # Case of stagnation: All differences are zero\n",
    "    if np.all(np.abs(diffs) < tolerance):\n",
    "      print('Error Status: Stagnant')\n",
    "      return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "_nlFVBVTknA_"
   },
   "outputs": [],
   "source": [
    "# not used anywhere\n",
    "\n",
    "def get_new_parameter_value(param_name):\n",
    "    # Prompt the user for the new value of the specified parameter\n",
    "    return float(input(f\"Enter new value for {param_name}: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "UFtxOzoq3yUy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Update parameters for a specific run and rerun the process with the new values.\n",
    "\n",
    "    This function fetches the original data for the specified run number, updates parameters on the provided\n",
    "    `new_value` flags, and then reruns the neural network training process with these updated parameters.\n",
    "\n",
    "    Args:\n",
    "        run_number (int): The number of the run to update.\n",
    "        base_folder (str): The base directory where run data is stored.\n",
    "        new_value (list of tuples): A list containing tuples that represent whether a parameter should be updated\n",
    "                                     (1) or not (0), along with the new value for the parameter if it is to be updated.\n",
    "                                     Format: [(bias_update_flag, new_bias_learning_rate),\n",
    "                                              (tau_update_flag, new_tau_learning_rate),\n",
    "                                              (taubounce_update_flag, new_tau_bouncing_mode),\n",
    "                                              (epochs_update_flag, new_epochs),\n",
    "                                              (dataset_update_flag, new_dataset)].\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the run number and the rounded final error after rerunning the training.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the original run data folder does not exist.\n",
    "\n",
    "    Example:\n",
    "        run_number = 2\n",
    "        base_folder = \"/path/to/data\"\n",
    "        new_value = [(1, 0.01), (0, None), (1, 'yes'), (1, 100), (0, None)]\n",
    "        result = update_rerun_parameter(run_number, base_folder, new_value)\n",
    "    \"\"\"\n",
    "# def update_rerun_parameter(run_number, base_folder, param_to_update, new_value):\n",
    "def update_rerun_parameter(run_number, base_folder, new_value):\n",
    "\n",
    "    run_folder = os.path.join(base_folder, f\"run_{run_number}\", 'original_result')\n",
    "    if os.path.exists(run_folder):\n",
    "       # Load the original data and parameters\n",
    "      num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, num_layers, num_neurons, num_input_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, x_train, y_train = load_run_data(run_folder, base_folder)\n",
    "      # Initialize debug_mode as 'no' by default\n",
    "      debug_mode = 'no'\n",
    "\n",
    "       # Create the folder structure for parameter updates\n",
    "      # parameter_update_folder = os.path.join(base_folder, f\"run_{run_number}\", \"parameter_update_result\", f\"{param_to_update}_result\")\n",
    "      parameter_update_folder = os.path.join(base_folder, f\"run_{run_number}\", \"parameter_update_result\")\n",
    "      os.makedirs(parameter_update_folder, exist_ok=True)\n",
    "\n",
    "      # Capture the console output\n",
    "      original_stdout = sys.stdout\n",
    "      sys.stdout = StringIO()\n",
    "\n",
    "       # Update the specific parameter\n",
    "      # if param_to_update == 'tau':\n",
    "      #   tau_learning_rate = new_value\n",
    "      # elif param_to_update == 'bias':\n",
    "      #   bias_learning_rate = new_value\n",
    "      # elif param_to_update == 'epochs':\n",
    "      #   epochs = int(new_value)\n",
    "      # elif param_to_update == 'taubounce':\n",
    "      #   tau_bouncing_mode = new_value\n",
    "\n",
    "      # Update the specific parameters based on the flags in new_value\n",
    "      if new_value[0][0] == 1:  # Update bias learning rate\n",
    "            bias_learning_rate = new_value[0][1]\n",
    "      if new_value[1][0] == 1:  # Update tau learning rate\n",
    "            tau_learning_rate = new_value[1][1]\n",
    "      if new_value[2][0] == 1:  # Update tau bouncing mode\n",
    "            tau_bouncing_mode = new_value[2][1]\n",
    "      if new_value[3][0] == 1:  # Update epochs\n",
    "            epochs = int(new_value[3][1])\n",
    "      if new_value[4][0] == 1:  # Update dataset\n",
    "            # x_train, y_train = generate_data(num_neurons[0], \"/content/DATA\", True)\n",
    "            x_train, y_train = generate_data(num_neurons[0], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, \"/content/DATA\", True)\n",
    "      if new_value[5][0] == 1:  # Enter the debug mode\n",
    "            debug_mode = new_value[5][1]\n",
    "\n",
    "      try:\n",
    "        # Rerun the process for this specific run with the new parameters' value\n",
    "        net = Network()\n",
    "        for i in range(num_layers):\n",
    "          if i == num_layers - 1:  # last layer\n",
    "            net.add(FCLayer(num_neurons[i], output_num_neurons, f'Layer {i}', bias=bias_values[i], tau=tau_values[i]))\n",
    "          else:\n",
    "            net.add(FCLayer(num_neurons[i], num_neurons[i + 1], f'Layer {i}', bias=bias_values[i], tau=tau_values[i]))\n",
    "          net.add(ActivationLayer(ReLU))\n",
    "\n",
    "        net.use(mse)\n",
    "        errors, final_bias_values, final_tau_values = net.fit(x_train, y_train, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, debug_mode)\n",
    "\n",
    "        final_error = errors[-1]\n",
    "        rounded_final_error = round(final_error, 2)\n",
    "\n",
    "        # Save results directly in the update folder without the original_result subfolder\n",
    "        save_results(parameter_update_folder, run_number, x_train, y_train, bias_values, tau_values, errors, final_bias_values, final_tau_values, sys.stdout.getvalue())\n",
    "\n",
    "      finally:\n",
    "                # Restore the original stdout\n",
    "                sys.stdout = original_stdout\n",
    "\n",
    "    else:\n",
    "            print(f\"Run folder for run number {run_number} does not exist.\")\n",
    "\n",
    "    # return new_run_summary\n",
    "    return (run_number, rounded_final_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "WzmNUP0MGUxp"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Manage the rerunning or updating of neural network training parameters based on user input.\n",
    "\n",
    "    This function prompts the user to decide whether to rerun an existing training run or update the parameters\n",
    "    for all runs. It collects new parameter values if necessary, calls the `update_rerun_parameter` function to\n",
    "    perform the rerun or update, and displays a comparison of errors before and after the update.\n",
    "\n",
    "    Args:\n",
    "        rerun_or_update (str): A string indicating the operation to perform; either 'rerun' or 'update'.\n",
    "        base_folder (str): The base directory where the run data is stored.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return a value but prints the results and comparison tables to the console.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the user inputs an invalid run number or if binary input for updates is not valid.\n",
    "\n",
    "    Example:\n",
    "        manage_rerun_update(\"update\", \"/path/to/data\")\n",
    "    \"\"\"\n",
    "\n",
    "def manage_rerun_update(rerun_or_update, base_folder):\n",
    "\n",
    "    new_run_summary = []\n",
    "    # Initialize new_value to have 6 elements as (0, None)\n",
    "    new_value = [(0, None) for _ in range(6)]\n",
    "\n",
    "    if rerun_or_update == \"rerun\":\n",
    "      rerun_number = int(get_valid_input( f\"Enter the run number to update the parameters for (between 1 to {num_runs}): \", lambda x: validate_run_number(x, num_runs),\n",
    "        f\"Please enter a run number.\" ))\n",
    "    update_type = get_valid_input(\n",
    "    \"Do you want to update the bias learning rate, tau learning rate, tau bouncing mode, epochs, change the dataset, or enter debug mode? Enter binary digits (e.g., 011001) 0 = no, 1 = yes: \",\n",
    "    validate_binary_string, \"Invalid input. Please enter a 6-digit binary string like '011001'.\" )\n",
    "\n",
    "    # Parse the binary flags (e.g., '0110' -> bias: 0, tau: 1, taubounce: 1, epochs: 0)\n",
    "    for i, flag in enumerate(update_type):\n",
    "      if flag == '1':\n",
    "        if i == 0:\n",
    "            new_bias_learning_rate = get_valid_input(\"Enter new value for bias learning rate: \", is_valid_number, \"Invalid input.\")\n",
    "            new_value[i] = (1, float(new_bias_learning_rate))  # Assign to the correct index\n",
    "        elif i == 1:\n",
    "            new_tau_learning_rate = get_valid_input(\"Enter new value for tau learning rate: \", is_valid_number, \"Invalid input.\")\n",
    "            new_value[i] = (1, float(new_tau_learning_rate))  # Assign to the correct index\n",
    "        elif i == 2:\n",
    "            new_tau_bouncing_mode = get_valid_input(\"Tau Bouncing Mode (yes/no): \", validate_yes_no, \"Enter either 'yes' or 'no'\")\n",
    "            new_value[i] = (1, new_tau_bouncing_mode)  # Assign to the correct index\n",
    "        elif i == 3:\n",
    "            new_epochs = get_valid_input(\"Enter new value for epochs: \", is_valid_number, \"Invalid input.\")\n",
    "            new_value[i] = (1, int(new_epochs))  # Assign to the correct index\n",
    "        elif i == 4:\n",
    "            new_dataset = get_valid_input(\"Do you want to generate a new dataset? (yes/no): \", validate_yes_no, \"Enter either 'yes' or 'no'\")\n",
    "            new_value[i] = (1, new_dataset)  # Assign to the correct index\n",
    "        elif i == 5:\n",
    "            new_debug = get_valid_input(\"Do you want to enter debug mode (yes/no): \", validate_yes_no, \"Enter either 'yes' or 'no'\")\n",
    "            new_value[i] = (1, new_debug)  # Assign to the correct index\n",
    "\n",
    "    if rerun_or_update == \"update\":\n",
    "      for run_number in range(1, num_runs + 1):\n",
    "        # run_summary_tuple = update_rerun_parameter(run_number, base_folder, param_to_update, new_value)\n",
    "        run_summary_tuple = update_rerun_parameter(run_number, base_folder, new_value)\n",
    "        new_run_summary.append(run_summary_tuple)\n",
    "\n",
    "      print(\"Update completed with updated parameters.\")\n",
    "\n",
    "      # Generate comparison table\n",
    "      headers = [\"Former Run Number\", \"Former Error\", \"New Error\"]\n",
    "      comparison_table = [[i+1, former_run_summary[i][1], new_run_summary[i][1]] for i in range(len(new_run_summary))]\n",
    "      print(tabulate(comparison_table, headers, tablefmt=\"grid\"))\n",
    "\n",
    "    elif rerun_or_update == \"rerun\":\n",
    "      # run_summary_tuple = update_rerun_parameter(rerun_number, base_folder, param_to_update, new_value)\n",
    "      run_summary_tuple = update_rerun_parameter(rerun_number, base_folder, new_value)\n",
    "      new_run_summary.append(run_summary_tuple)\n",
    "\n",
    "      print(f\"Rerun for run number {rerun_number} completed with updated {update_type}.\")\n",
    "\n",
    "      # Generate comparison table using former_run_summary\n",
    "      headers = [\"Run Number\", \"Former Error\", \"New Error\"]\n",
    "      comparison_table = [[rerun_number, former_run_summary[rerun_number - 1][1], new_run_summary[0][1]]]\n",
    "      print(tabulate(comparison_table, headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-7fPdnpLYeRo"
   },
   "outputs": [],
   "source": [
    "#The clear_folders function is used to clear out a folder completely, removing all of its contents, and then recreating an empty folder with the same name\n",
    "\n",
    "def clear_folders(base_folder):\n",
    "    if os.path.exists(base_folder):\n",
    "        shutil.rmtree(base_folder)\n",
    "    os.makedirs(base_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Wylo1P2OgLWu"
   },
   "outputs": [],
   "source": [
    "#The zip_folder function compresses the contents of a folder into a ZIP file\n",
    "\n",
    "def zip_folder(folder_path, zip_name):\n",
    "    with ZipFile(zip_name, 'w') as zipf:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                zipf.write(file_path, os.path.relpath(file_path, folder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "yToH5TnxSQpg"
   },
   "outputs": [],
   "source": [
    "#Function to get user inputs - This function has two modes:\n",
    "#it either loads existing parameters from a file or prompts the user to input new parameters\n",
    "def get_input_with_default(prompt, default, data_type=str):\n",
    "    user_input = input(f\"{prompt} [{default}]: \")\n",
    "    return data_type(user_input) if user_input else default\n",
    "\n",
    "\n",
    "def get_user_inputs(create_new, run_number=1):\n",
    "    folder_path = \"/content/DATA\"\n",
    "    run_folder_path = os.path.join(folder_path, f\"run_{run_number}\")\n",
    "    parameters_file_path = os.path.join(folder_path, \"parameters.json\")\n",
    "\n",
    "    # Create the path to the parameters file within the specific run folder\n",
    "    # parameters_file_path = os.path.join(run_folder_path, \"parameters.json\")\n",
    "\n",
    "    if create_new and run_number == 1:\n",
    "        print(\"Clearing existing files and creating new parameters...\")\n",
    "        clear_folders(folder_path)\n",
    "\n",
    "    # if not create_new and os.path.exists(parameters_file_path):\n",
    "    if not create_new:\n",
    "      if os.path.exists(parameters_file_path):\n",
    "            print(f\"Loading parameters from {parameters_file_path} in common folder.\")\n",
    "      else:\n",
    "        # If it doesn't exist in the common folder, check the run folder\n",
    "        parameters_file_path = os.path.join(run_folder_path, \"parameters.json\")\n",
    "        if os.path.exists(parameters_file_path):\n",
    "                print(f\"Loading parameters from {parameters_file_path} in run folder.\")\n",
    "        else:\n",
    "                raise FileNotFoundError(\"Parameters file not found in either run or common folder.\")\n",
    "\n",
    "      print(\"Loading parameters from file...\")\n",
    "      with open(parameters_file_path, \"r\") as file:\n",
    "          parameters = json.load(file)\n",
    "      num_training_inputs = parameters['num_training_inputs']\n",
    "      lower_bound = parameters['lower_bound']\n",
    "      max_lower_bound = parameters['max_lower_bound']\n",
    "      upper_bound = parameters['upper_bound']\n",
    "      min_upper_bound = parameters['min_upper_bound']\n",
    "      func_expressions = parameters['func_expressions']\n",
    "      num_layers = parameters['num_layers']\n",
    "      num_neurons = parameters['num_neurons']\n",
    "      output_num_neurons =  parameters['output_num_neurons']\n",
    "      epochs = parameters['epochs']\n",
    "      bias_learning_rate = parameters['bias_learning_rate']\n",
    "      tau_learning_rate = parameters['tau_learning_rate']\n",
    "      tau_bouncing_mode = parameters['tau_bouncing_mode']\n",
    "      bias_values = [np.random.randint(0, 2, (num_neurons[i + 1], num_neurons[i])) for i in range(num_layers - 1)]\n",
    "      bias_values.append(np.random.randint(0, 2, (output_num_neurons, num_neurons[-1])))\n",
    "      tau_values = [np.round(np.random.rand(num_neurons[i + 1], num_neurons[i]) * 0.8+0.1, decimals=1) for i in range(num_layers - 1)]\n",
    "      tau_values.append(np.round(np.random.rand(output_num_neurons, num_neurons[-1]) * 0.8+0.1, decimals=1))\n",
    "\n",
    "      folder_path = \"/content/DATA\"\n",
    "      # return num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path  # Ensure initial values are returned\n",
    "      return num_training_inputs, lower_bound, max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path  # Ensure initial values are returned\n",
    "\n",
    "    else:\n",
    "        print(\"No parameters file found or user chose to create new parameters. Asking user for inputs...\")\n",
    "\n",
    "        # Ask for training data details\n",
    "        # num_training_inputs = int(input(\"Enter the number of training inputs (total number of samples) in the dataset: \"))\n",
    "        num_training_inputs = get_input_with_default(\"Enter the number of training inputs (total number of samples) in the dataset\", 50, int)\n",
    "        # num_input_neurons = int(input(\"Enter the number of neurons for the input layer (>=1): \"))\n",
    "        num_input_neurons = get_input_with_default(\"Enter the number of neurons for the input layer (>=1)\", 2, int)\n",
    "        # lower_bound = float(input(\"Enter the lower bound of input range: \"))\n",
    "        lower_bound = get_input_with_default(\"Enter the lower bound of input range\", 0.0, float)\n",
    "        # max_lower_bound = float(input(\"Enter the max lower bound for input: \"))\n",
    "        max_lower_bound = get_input_with_default(\"Enter the max lower bound for input\", 0.45, float)\n",
    "        # upper_bound = float(input(\"Enter the upper bound of input range: \"))\n",
    "        upper_bound = get_input_with_default(\"Enter the upper bound of input range\", 1.0, float)\n",
    "        # min_upper_bound = float(input(\"Enter the min upper bound for input: \"))\n",
    "        min_upper_bound = get_input_with_default(\"Enter the min upper bound for input\", 0.55, float)\n",
    "        # output_num_neurons = int(input(\"Enter the number of neurons for the output layer (>=1): \"))\n",
    "        output_num_neurons = get_input_with_default(\"Enter the number of neurons for the output layer (>=1)\", 2, int)\n",
    "\n",
    "        # Collect function expressions before running\n",
    "        func_expressions = []\n",
    "        for i in range(output_num_neurons):\n",
    "            func_str = input(f\"Enter y_train({i+1}) function using symbols x0, x1, x2,...(use '&' for AND, '|' for OR): \")\n",
    "            func_expressions.append(func_str)\n",
    "\n",
    "        # num_layers = int(input(\"Enter the total number of layers (>=2): \"))\n",
    "        num_layers = get_input_with_default(\"Enter the total number of layers (>=2): \", 3, int)\n",
    "        # num_layers_save = num_layers\n",
    "        # num_input_neurons = int(input(\"Enter the number of neurons for the input layer (>=1): \"))\n",
    "        print ('The number of neurons in the input layer are: ', num_input_neurons)\n",
    "        num_layers -= 2   # Subtracting input and output layers\n",
    "        hidden_num_neurons = [int(input(f\"Enter the number of neurons for layer {i+1}: \")) for i in range(num_layers)]\n",
    "        num_neurons = [num_input_neurons] + hidden_num_neurons\n",
    "        print ('The number of neurons in the output layer are: ', output_num_neurons)\n",
    "        # output_num_neurons = int(input(\"Enter the number of neurons for the output layer (>=1): \"))\n",
    "        epochs = int(input(\"Enter the number of epochs: \"))\n",
    "        # bias_learning_rate = float(input(\"Enter the bias learning rate : \"))\n",
    "        bias_learning_rate = get_input_with_default(\"Enter the bias learning rate : \", 5.0, float)\n",
    "        # tau_learning_rate = float(input(\"Enter the tau learning rate : \"))\n",
    "        tau_learning_rate = get_input_with_default(\"Enter the tau learning rate : \", 3.0, float)\n",
    "        tau_bouncing_mode = 'yes'\n",
    "        bias_values = []\n",
    "        tau_values = []\n",
    "        num_layers= num_layers+1\n",
    "        values_choice = input(\"Enter '0' to manually input values for bias and tau, or '1' to generate random values: \")\n",
    "\n",
    "        if values_choice == '0':\n",
    "          #Input for layers except the output layer\n",
    "            for i in range(1, num_layers):\n",
    "                current_layer_neurons = num_neurons[i]\n",
    "                previous_layer_neurons = num_neurons[i - 1]\n",
    "\n",
    "                # Input for each neuron in the current layer\n",
    "                layer_biases = []\n",
    "                layer_taus = []\n",
    "\n",
    "                for j in range(current_layer_neurons):\n",
    "                    for k in range(previous_layer_neurons):\n",
    "                        layer_biases.append(int(input(f\"Enter bias value for hidden layer {i}, neuron {k+1}: \")))\n",
    "                        layer_taus.append(float(input(f\"Enter tau value for hidden layer {i}, neuron {k+1}: \")))\n",
    "\n",
    "                #Reshape and append for layers except the last layer\n",
    "                layer_biases = np.array(layer_biases).reshape(num_neurons[i], num_neurons[i-1])\n",
    "                layer_taus = np.array(layer_taus).reshape(num_neurons[i], num_neurons[i-1])\n",
    "                bias_values.append(layer_biases)\n",
    "                tau_values.append(layer_taus)\n",
    "\n",
    "            #Input for the output layer separately\n",
    "            layer_biases = []\n",
    "            layer_taus = []\n",
    "            previous_layer_neurons = num_neurons[-2]\n",
    "\n",
    "            for j in range(num_neurons[-1]):\n",
    "                for k in range(previous_layer_neurons):\n",
    "                    layer_biases.append(int(input(f\"Enter bias value for output layer, neuron {k+1}: \")))\n",
    "                    layer_taus.append(float(input(f\"Enter tau value for output layer, neuron {k+1}: \")))\n",
    "\n",
    "            #Reshape and append for the output layer\n",
    "            layer_biases = np.array(layer_biases).reshape(output_num_neurons, num_neurons[-1])\n",
    "            layer_taus = np.array(layer_taus).reshape(output_num_neurons, num_neurons[-1])\n",
    "            bias_values.append(layer_biases)\n",
    "            tau_values.append(layer_taus)\n",
    "\n",
    "        else:\n",
    "           #randomly initialized bias values\n",
    "            bias_values = [np.random.randint(0, 2, (num_neurons[i + 1], num_neurons[i])) for i in range(num_layers - 1)]\n",
    "            #Reshape and append bias for the output layer\n",
    "            bias_values.append(np.random.randint(0, 2, (output_num_neurons, num_neurons[-1])))\n",
    "\n",
    "            #randomly initialized tau values\n",
    "            tau_values = [np.round(np.random.rand(num_neurons[i + 1], num_neurons[i]) * 0.8+0.1, decimals=1) for i in range(num_layers - 1)]\n",
    "            #Reshape and append tau for the output layer\n",
    "            tau_values.append(np.round(np.random.rand(output_num_neurons, num_neurons[-1]) * 0.8+0.1, decimals=1))\n",
    "\n",
    "\n",
    "        #prepare a dictionary of parameters, save these parameters to a JSON file, and return them\n",
    "        parameters = {\n",
    "            'num_training_inputs': num_training_inputs,\n",
    "            'lower_bound': lower_bound,\n",
    "            'max_lower_bound': max_lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'min_upper_bound': min_upper_bound,\n",
    "            'func_expressions': func_expressions,\n",
    "            'num_layers': num_layers,\n",
    "            'num_neurons' : num_neurons,\n",
    "            'num_input_neurons' : num_input_neurons,\n",
    "            'output_num_neurons': output_num_neurons,\n",
    "            'epochs': epochs,\n",
    "            'bias_learning_rate': bias_learning_rate,\n",
    "            'tau_learning_rate': tau_learning_rate,\n",
    "            'tau_bouncing_mode': tau_bouncing_mode,\n",
    "            'bias_values': [b.tolist() for b in bias_values],\n",
    "            'tau_values': [t.tolist() for t in tau_values]\n",
    "        }\n",
    "        with open(parameters_file_path, \"w\") as file:\n",
    "            json.dump(parameters, file, indent=4)\n",
    "\n",
    "        # return num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path\n",
    "        return num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nra_P8Poanye"
   },
   "source": [
    "Afsaneh\n",
    "Explanation for Different Cases for XOR function: Case 1: All inputs are the same\n",
    "\n",
    "Example: [1, 1, 1] Sum: 3, which is odd XOR output: 3 % 2 = 1 This is because an odd number of 1s results in 1. Case 2: One input is different\n",
    "\n",
    "Example: [1, 0, 1] Sum: 2, which is even XOR output: 2 % 2 = 0 This is because an even number of 1s results in 0. Case 3: All inputs are different\n",
    "\n",
    "Example: [0, 1, 0] Sum: 1, which is odd XOR output: 1 % 2 = 1 This is because an odd number of 1s results in 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "RZn-Vtp_daxv"
   },
   "outputs": [],
   "source": [
    "# Function to generate random inputs and compute outputs for AND, OR, XOR gates\n",
    "# def generate_data(num_input_neurons, output_num_neurons, folder_path, create_new):\n",
    "def generate_data(num_input_neurons, output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, create_new):\n",
    "  x_train_file_path = os.path.join(folder_path, \"x_train.npy\")\n",
    "  y_train_file_path = os.path.join(folder_path, \"y_train.npy\")\n",
    "  print ('value for create_new: ', create_new)\n",
    "\n",
    "  # Function to check if the operation is logical (like AND/OR)\n",
    "  def is_logical_operation(func_str):\n",
    "    logical_ops = ['&', '|', 'and', 'or']  # Add any other logical operators you want to support\n",
    "    return any(op in func_str for op in logical_ops)\n",
    "\n",
    "  if not create_new and os.path.exists(x_train_file_path) and os.path.exists(y_train_file_path):\n",
    "    #if os.path.exists(x_train_file_path) and os.path.exists(y_train_file_path):\n",
    "        print(\"Loading data from files...\")\n",
    "        x_train = np.load(x_train_file_path)\n",
    "        y_train = np.load(y_train_file_path)\n",
    "  else:\n",
    "\n",
    "        # Create X_train dataset with random values uniformly between the bounds\n",
    "        x_train = np.random.uniform(lower_bound, upper_bound, (num_training_inputs, num_input_neurons))\n",
    "        # Apply limiting conditions\n",
    "        for i in range(num_input_neurons):\n",
    "            mask = np.random.choice([0, 1], size=num_training_inputs)\n",
    "\n",
    "            # Set values in the range [lower_bound, max_lower_bound]\n",
    "            x_train[:, i][mask == 0] = np.random.uniform(lower_bound, max_lower_bound, size=(mask == 0).sum())\n",
    "\n",
    "            # Set values in the range [min_upper_bound, upper_bound]\n",
    "            x_train[:, i][mask == 1] = np.random.uniform(min_upper_bound, upper_bound, size=(mask == 1).sum())\n",
    "\n",
    "        x_train = np.round(x_train, 2)\n",
    "      #   x_train = np.array([[0.74, 0.22],\n",
    "      #               [0.67, 0.08],\n",
    "      #               [0.32, 0.12],\n",
    "      #               [0.67, 0.66],\n",
    "      #               [0.98, 0.01],\n",
    "      #               [0.77, 0.19],\n",
    "      #               [0.27, 0.76],\n",
    "      #               [0.10, 0.23],\n",
    "      #               [0.12, 0.05],\n",
    "      #               [0.02, 0.31]\n",
    "      #  ])\n",
    "\n",
    "      #   print(\"\\nX_train:\")\n",
    "      #   print(x_train)\n",
    "\n",
    "        # Create y_train by asking user to provide functions for each output\n",
    "        y_train = []\n",
    "        x_symbols = sp.symbols(f'x0:{num_input_neurons}')  # Creates symbols x0, x1, x2, ..., up to num_inputs\n",
    "\n",
    "        # for i in  range(output_num_neurons):\n",
    "        for func_str in  func_expressions:\n",
    "          func = sp.sympify(func_str)  # Convert string to sympy expression\n",
    "          func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
    "\n",
    "          if is_logical_operation(func_str):\n",
    "            # Apply thresholding if the function is a logical operation\n",
    "            binary_x_train = (x_train > 0.5).astype(int)  # Convert to binary values based on threshold\n",
    "\n",
    "            # func = sp.sympify(func_str)  # Convert string to sympy expression\n",
    "            # func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
    "\n",
    "            # Evaluate the function for each row\n",
    "            y_values = np.array([func_callable(*binary_x_train[row]) for row in range(num_training_inputs)])\n",
    "            # y_values = np.array([func_callable(*x_train[row]) for row in range(num_training_inputs)])\n",
    "\n",
    "            # Convert boolean results to binary (0 and 1)\n",
    "            y_values = y_values.astype(int)\n",
    "\n",
    "          else:\n",
    "            # For mathematical functions, apply directly on continuous values\n",
    "            # func = sp.sympify(func_str)  # Convert string to sympy expression\n",
    "            # func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
    "            y_values = np.array([func_callable(*x_train[row]) for row in range(num_training_inputs)])\n",
    "\n",
    "          # Append results to y_train\n",
    "          y_train.append(y_values)\n",
    "\n",
    "      # Convert y_train to numpy array and transpose to match the format (num_training_inputs, num_outputs)\n",
    "        y_train = np.array(y_train).T\n",
    "\n",
    "        print(\"\\ny_train:\")\n",
    "        print(y_train)\n",
    "\n",
    "        # Save x_train and y_train to the folder path\n",
    "        # np.save(x_train_file_path, x_train)\n",
    "        # np.save(y_train_file_path, y_train)\n",
    "\n",
    "        # dataset_file_path = os.path.join(folder_path, \"dataset.csv\")\n",
    "        #dataset = pd.DataFrame(np.hstack((x_train, y_train)), columns=[f'Input_{i+1}' for i in range(num_input_neurons)] + ['AND', 'OR', 'XOR'])\n",
    "        # dataset = pd.DataFrame(np.hstack((x_train, y_train)), columns=[f'Input_{i+1}' for i in range(num_input_neurons)] + [f'Output_{i+1}' for i in range(output_num_neurons)])\n",
    "\n",
    "        # dataset.to_csv(dataset_file_path, index=False)\n",
    "\n",
    "  return x_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Ry3N5Dl4BGBl"
   },
   "outputs": [],
   "source": [
    "# def zip_files(zip_filename, folder_path): - not used anywhere\n",
    "#     with ZipFile(zip_filename, 'w') as zipf:\n",
    "#         for root, dirs, files in os.walk(folder_path):\n",
    "#             for file in files:\n",
    "#                 zipf.write(os.path.join(root, file), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "V7Tylug0qvcy"
   },
   "outputs": [],
   "source": [
    "def get_valid_input(prompt, validation_fn, error_message):\n",
    "    \"\"\"\n",
    "    General function to get validated user input.\n",
    "\n",
    "    Parameters:\n",
    "    - prompt: The message to display when asking for input.\n",
    "    - validation_fn: A function that takes the input and returns True if it's valid, False otherwise.\n",
    "    - error_message: The message to display when the input is invalid.\n",
    "\n",
    "    Returns:\n",
    "    - The validated input.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        user_input = input(prompt).strip().lower()\n",
    "        if validation_fn(user_input):\n",
    "            return user_input\n",
    "        else:\n",
    "            print(error_message)\n",
    "\n",
    "#  validation functions for different input types\n",
    "def validate_run_number(input_value, max_runs):\n",
    "    try:\n",
    "        run_number = int(input_value)\n",
    "        return 1 <= run_number <= max_runs\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# def validate_update_type(input_value):\n",
    "#     valid_types = {\"bias\", \"tau\", \"taubounce\", \"epochs\"}\n",
    "#     return input_value.lower() in valid_types\n",
    "\n",
    "def validate_yes_no(input_value):\n",
    "    return input_value.lower() in {\"yes\", \"no\"}\n",
    "\n",
    "def is_valid_number(value):\n",
    "    \"\"\"Check if the input value can be converted to a valid number (int or float).\"\"\"\n",
    "    try:\n",
    "        num_value = float(value.strip())  # Convert to float\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def validate_binary_string(input_str):    #This is a replacement for the validate_update_type function defined above.\n",
    "    \"\"\"Check if the input string is a 6-digit binary string (i.e., contains only '0' or '1').\"\"\"\n",
    "    return len(input_str) == 6 and all(char in '01' for char in input_str)\n",
    "\n",
    "def validate_rerun_or_update(input_value):\n",
    "    \"\"\"Check if the input value is one of the valid options.\"\"\"\n",
    "    return input_value in {\"rerun\", \"update\", \"exit\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpcakg7VB_QQ"
   },
   "source": [
    "## Step 2: Code the Base Class\n",
    "\n",
    "Abstract Base Class Layer: This class Layer, which all other layers will inherit from, handles simple properties which are an input, an output, and both a forward and backward methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "WtVG9EmrdenT"
   },
   "outputs": [],
   "source": [
    "# Base class\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.bias = None\n",
    "        self.tau = None\n",
    "        self.z = None\n",
    "        self.Z = None  # New attribute to store the value of Z\n",
    "        self.constant = None  # New attribute to store the value of constant\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    # def backward_propagation(self, output_error, learning_rate):\n",
    "    def backward_propagation(self, output_error, bias_learning_rate, tau_learning_rate, bouncing_tau):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RyvcxqwCLOo"
   },
   "source": [
    "## Step 3: Fully Connected Layer\n",
    "\n",
    "The below piece of code defines a custom Fully Connected Layer (FCLayer) for a neural network. It inherits from a base class Layer (which is defined above) and includes methods for forward propagation and backward propagation.\n",
    "\n",
    "**Forward propagation for each layer**\n",
    "\n",
    "    Z(i) = self.Z * [{j=1to J}  ((200 b (j, i) - 100) * (y(j) -   (j, i))) - J + self.constant]\n",
    "        \n",
    "    y(i) = ReLU (Zi)\n",
    "        \n",
    "**Backward Propagation for each layer:**\n",
    "\n",
    "*Calculate beta and theta*\n",
    "\n",
    "    (i,k) = self.Z *  ' [(200 b(i, k) - 100) (yi -  (i , k))] * (200 b(i,k) - 100)\n",
    "\n",
    "     (i, k) = self.Z *  ' [(200 b(i, k) - 100) (yi -  (i , k))] * 200 * (yi -  (i, k))\n",
    "\n",
    "*Calculate the gradients*\n",
    "\n",
    "     b(i, k) =  { (yk  tk) *  (i, k)}, if z(k) > 0\n",
    "\n",
    "     b(i, k) =  { 0 }, if z(k) < 0\n",
    "\n",
    "*Update the bias and tau*\n",
    "\n",
    "    self.bias = self.bias + bias_gradient\n",
    "\n",
    "    self.tau = self.tau + tau_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(x, alpha=1):\n",
    "#     x = x.astype(float)\n",
    "#     return 1 / (1 + np.exp(-alpha * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.z = self.Z * (np.sum(sigmoid((200 * 0 - 100) * (2 - self.tau)), axis = 1, keepdims=True) - 4 + 7)\n",
    "# self.output =  self.z.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7OCsSBjae26U"
   },
   "outputs": [],
   "source": [
    "# inherit from base class Layer: Fully connected layer code\n",
    "class FCLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size, name, bias=None, tau=None):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.name = name  # Layer name\n",
    "        self.bias = bias\n",
    "        self.tau = tau\n",
    "\n",
    "    # Set Z and constant based on the input_size\n",
    "        if self.input_size >= 10:\n",
    "            self.Z = 0.1\n",
    "            self.constant = 10\n",
    "        else :\n",
    "            self.Z = round(1/self.input_size, 2)\n",
    "            self.constant = self.input_size\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        print(f\"Forward propagation for : {self.name} with input neurons: {self.input_size} and output neurons: {self.output_size}\")\n",
    "        self.input = input_data\n",
    "        print(f\"input for the layer : {self.input} with tau: {self.tau} and bias: {self.bias}\")\n",
    "\n",
    "        # Ensure input and tau are reshaped to 1D arrays for element-wise comparison\n",
    "        input_flat = self.input.flatten()\n",
    "        tau_flat = self.tau.flatten()\n",
    "\n",
    "        # Ensure that tau and input have the same size before comparison\n",
    "        min_size = min(input_flat.size, tau_flat.size)\n",
    "        input_flat = input_flat[:min_size]\n",
    "        tau_flat = tau_flat[:min_size]\n",
    "\n",
    "        # Identify indices where input and tau are identical\n",
    "        # identical_indices = np.where(input_flat == tau_flat)\n",
    "\n",
    "        # Use np.isclose to find indices where input and tau are approximately the same within a tolerance\n",
    "        identical_indices = np.where(np.isclose(input_flat, tau_flat, atol=0.01))\n",
    "\n",
    "        if identical_indices[0].size > 0:\n",
    "            print(f\"Identical input and tau detected at indices {identical_indices}. Updating input values.\")\n",
    "\n",
    "            # Loop through the indices where input and tau are close\n",
    "            for idx in identical_indices[0]:\n",
    "              if (0.6 <= tau_flat[idx] <= 1) or (0.4 <= tau_flat[idx] <= 0.5):  #between 0.6 to 1 OR between 0.4 and 0.5\n",
    "                # Subtract 0.1 if tau is greater than 0.8\n",
    "                input_flat[idx] -= 0.1\n",
    "                print(f\"Tau value at index {idx} is greater than 0.8. Subtracting 0.1 from input.\")\n",
    "              else:\n",
    "                # Add 0.1 if tau is 0.8 or less\n",
    "                input_flat[idx] += 0.1\n",
    "                print(f\"Tau value at index {idx} is less than or equal to 0.8. Adding 0.1 to input.\")\n",
    "\n",
    "            # Reshape input_flat back to original shape\n",
    "            self.input = input_flat.reshape(self.input.shape)\n",
    "            print(f\"Updated input for the layer: {self.input} with tau: {self.tau} and bias: {self.bias}\")\n",
    "\n",
    "\n",
    "            # # Update the input at those indices with random values\n",
    "            # input_flat[identical_indices] += np.random.uniform(low=0.1, high=1.0, size=identical_indices[0].shape)\n",
    "\n",
    "            # # Reshape input_flat back to original shape\n",
    "            # self.input = input_flat.reshape(self.input.shape)\n",
    "            # print(f\"new inptut for the layer : {self.input} with same tau: {self.tau} and same bias: {self.bias}\")\n",
    "\n",
    "        #calculate z(i)\n",
    "        self.z = self.Z * (np.sum(sigmoid((200 * self.bias - 100) * (self.input - self.tau)), axis = 1, keepdims=True) - self.input_size + self.constant)\n",
    "        self.output =  self.z.T\n",
    "        print(f\"output after Forward Propagation : {self.output}\")\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    # computes thresholds - tau, tau_gradient and bias_gradient\n",
    "    def backward_propagation(self, is_last_layer, layer_target_output, prev_tau_gradient, bias_learning_rate, tau_learning_rate, tau_bouncing_mode):\n",
    "        print(f\"Backward propagation begins for : {self.name}\")\n",
    "\n",
    "        #calculate the beta and theta for the previous layer\n",
    "        beta = self.Z * sigmoid_prime((2 * self.bias - 1) * (self.input - self.tau)) * (2 * self.bias - 1)\n",
    "        theta = self.Z * sigmoid_prime((2 * self.bias - 1) * (self.input - self.tau)) * 2 * (self.input - self.tau)\n",
    "        print('beta from BP: ',  beta)\n",
    "        print('theta from BP: ',  theta)\n",
    "\n",
    "        #calculate gradients based on layer\n",
    "        print(' Calculate tau and bias gradients:')\n",
    "        if is_last_layer:\n",
    "\n",
    "            tau_gradient = np.round((layer_target_output - self.output).T * beta, decimals=2)\n",
    "            tau_gradient = np.where(tau_gradient < 0.1, tau_gradient * tau_learning_rate, tau_gradient)\n",
    "\n",
    "            bias_gradient = (self.output - layer_target_output).T * theta\n",
    "            bias_gradient = np.where(np.absolute(bias_gradient) < 0.05, bias_gradient , bias_gradient * bias_learning_rate)\n",
    "\n",
    "        else:\n",
    "            tau_gradient = np.round(np.dot(np.sum(prev_tau_gradient, axis=0), beta), decimals = 2)\n",
    "            tau_gradient = np.where(tau_gradient < 0.1, tau_gradient * tau_learning_rate, tau_gradient)\n",
    "\n",
    "            bias_gradient = np.dot(np.sum(prev_tau_gradient, axis=0), -theta)\n",
    "            bias_gradient = np.where(np.absolute(bias_gradient) < 0.05, bias_gradient , bias_gradient * bias_learning_rate)\n",
    "            bias_gradient = np.where(bias_gradient > 1, 1, bias_gradient)\n",
    "\n",
    "        #tau_gradient = np.clip(tau_gradient, -1, 1)\n",
    "        print('tau_gradient from BP: ',  tau_gradient)\n",
    "        print('bias_gradient from BP: ',  bias_gradient)\n",
    "\n",
    "        #store the value of tau_gradient to be passed on to the next layer in BP\n",
    "        prev_tau_gradient = tau_gradient\n",
    "\n",
    "        # update bias and tau for the current layer\n",
    "        print(' Update tau and bias values based on gradients:')\n",
    "\n",
    "        # Update tau values with clipping logic\n",
    "        #tau_pre = self.tau\n",
    "        self.tau = np.round(self.tau - tau_gradient, decimals=2)\n",
    "        # if difference between self.tau (before)  and self.tau(after) is still between 0 and 1\n",
    "\n",
    "        #if tau_bouncing_mode in [\"yes\", \"y\"]:\n",
    "        #  self.tau = np.where(self.tau > 0.9, 0.9 - (self.tau - 0.9), self.tau)\n",
    "        #  self.tau = np.where(self.tau < 0.1, 0.1 + (0.1 - self.tau), self.tau)\n",
    "\n",
    "        #self.tau = np.clip(self.tau, 0.1, 0.9)\n",
    "        self.tau = np.where(\n",
    "          np.logical_or(self.tau > 0.9, self.tau < 0.1),\n",
    "          # Your specific formula here (e.g., bouncing back)\n",
    "          np.where(self.tau > 0.9, 0.9 - (self.tau - 0.9), 0.1 + (0.1 - self.tau)),\n",
    "          # Limit the change to 0.2 for values within the range\n",
    "          np.clip(self.tau, self.tau - 0.2, self.tau + 0.2)\n",
    "         )\n",
    "\n",
    "\n",
    "        self.tau = np.clip(self.tau, 0.1, 0.9)\n",
    "\n",
    "\n",
    "        # Update bias values\n",
    "        self.bias = np.round(self.bias - bias_gradient, decimals=0)\n",
    "        self.bias = np.clip(self.bias, 0, 1)\n",
    "\n",
    "        print('self.tau after update from BP: ', self.tau)\n",
    "        print('self.bias after update from BP: ',  self.bias)\n",
    "        print(f\"Backward propagation ends for : {self.name}\")\n",
    "\n",
    "        return prev_tau_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.z = self.Z * (np.sum(a, axis = 1, keepdims=True) - self.input_size + self.constant)\n",
    "# a = sigmoid((200 * self.bias - 100) * (self.input - self.tau))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlGmlQSiE9WU"
   },
   "source": [
    "## Step 4: Activation Layer\n",
    "\n",
    "In a neural network, an activation layer introduces non-linearity to the model. The Relu is implemented in this layer which returns the activated input: y(i) = ReLU (Zi) which is the final output for the layer. For the backward propagation, the activation layer is not used in the modified NN.\n",
    "\n",
    "*This can be further edited by removing it and directly integrating the Relu function into the forward propagation within the FC layer class. The activation layer has been defined separately to enhance clarity in the network structure.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "9Iw0WeYue7d-"
   },
   "outputs": [],
   "source": [
    "# inherit from base class Layer: Activation Layer code\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "\n",
    "    # returns the activated input: y(i) = ReLU (Zi)\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        print('output for the layer after ReLu (y): ', self.output)\n",
    "        return self.output\n",
    "\n",
    "    # Not returning anything for the modified NN\n",
    "    def backward_propagation(self, is_last_layer, layer_target_output, prev_tau_gradient, bias_learning_rate, tau_learning_rate, tau_bouncing_mode):\n",
    "        _ = self.activation(self.input)\n",
    "        return prev_tau_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1k6n7JcOe-R8"
   },
   "outputs": [],
   "source": [
    "# updated activation function and its derivative\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Modified Sigmoid activation function used in FP\n",
    "def sigmoid(x, alpha=1):\n",
    "    x = x.astype(float)\n",
    "    return 1 / (1 + np.exp(-alpha * x))\n",
    "\n",
    "# Modified Derivative of Sigmoid activation function used in BP\n",
    "def sigmoid_prime(x, alpha=1):\n",
    "    sig_prime_1 = (1 - sigmoid(x, alpha))\n",
    "    sig_prime_2 = sigmoid(x, alpha)\n",
    "    return alpha * sigmoid(x, alpha) * (1 - sigmoid(x, alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7SjilzdFKbT"
   },
   "source": [
    "## Step 5: Loss Function\n",
    "\n",
    "Mean Squared Error (MSE) Loss Function: The mse function calculates the mean of the squared differences between the true values (y_true) and the predicted values (y_pred). It quantifies the average magnitude of the errors.\n",
    "\n",
    "*This is solely for visualizing the model training, and the values are not directly utilized in the backward propagation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "1pFhZ_P9fBCv"
   },
   "outputs": [],
   "source": [
    "# loss function: Used to visualize the model learning - error after each epoch\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(np.squeeze(y_true) - np.squeeze(y_pred), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6NVs6ZeFJvh"
   },
   "source": [
    "## Step 6: Network Layer\n",
    "The below code defines a basic neural network class named Network.\n",
    "\n",
    "init Method: Initializes the neural network with an empty list of layers and placeholders for the loss function and its derivative.\n",
    "\n",
    "add Method: Adds a layer to the neural network.\n",
    "\n",
    "use Method: Sets the loss function\n",
    "\n",
    "predict Method: Predicts the output for a given input by performing forward propagation through all the layers.\n",
    "\n",
    "fit Method: Trains the neural network on the provided training data (x_train, y_train) using a specified number of epochs and learning rate. It includes a training loop that iterates through each epoch and sample, performing forward and backward propagation for each sample. The loss is computed during training for display purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ay80dAN7fJaZ"
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss):\n",
    "        self.loss = loss\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samplesnp\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "              output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, debug_mode='no'):\n",
    "\n",
    "        # sample dimension first\n",
    "        prev_tau_gradient = None\n",
    "        samples = len(x_train)\n",
    "        errors = []  # List to store average errors for plotting\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0  # Reset error for the current epoch\n",
    "            for j in range(samples):\n",
    "\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                sample_err = self.loss(y_train[j], output)\n",
    "                err += sample_err\n",
    "                print(f'Epoch {i+1}, Sample {j+1}, Error: {sample_err}')\n",
    "\n",
    "                #backward propagation\n",
    "                print(f\" *** Backward propagation Begins *** \")\n",
    "                for layer in reversed(self.layers):\n",
    "                    is_last_layer = (layer == self.layers[-2])  # Check if it's the last layerindex\n",
    "                    prev_tau_gradient = layer.backward_propagation(is_last_layer, y_train[j], prev_tau_gradient, bias_learning_rate, tau_learning_rate, tau_bouncing_mode)\n",
    "                print(f\" *** End of Backward propagation *** \\n\")\n",
    "\n",
    "                # forward propagation - once again FP only for debug mode\n",
    "                if debug_mode in [\"yes\", \"y\"]:\n",
    "                  print(f\" *** Forward propagation 2 Begins for the same input *** \")\n",
    "                  for layer in self.layers:\n",
    "                    output = layer.forward_propagation(x_train[j])\n",
    "\n",
    "                  sample_err = self.loss(y_train[j], output)\n",
    "                  err += sample_err\n",
    "                  print(f'Epoch {i+1}, Sample {j+1}, Error: {sample_err}')\n",
    "                  print(f\" *** End of Forward propagation 2 *** \\n\")\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            avg_err = err / samples\n",
    "            errors.append(avg_err)\n",
    "            print('Epoch %d/%d,   Average Error: %f \\n' % (i+1, epochs, avg_err))\n",
    "\n",
    "        # Save final bias and tau values\n",
    "        final_bias_values = [layer.bias for layer in self.layers if isinstance(layer, FCLayer)]\n",
    "        final_tau_values = [layer.tau for layer in self.layers if isinstance(layer, FCLayer)]\n",
    "\n",
    "        # Plotting error rates across epochs\n",
    "        plt.plot(range(1, epochs + 1), errors, marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Error')\n",
    "        plt.title('Error Rate Across Epochs')\n",
    "        plt.show()\n",
    "\n",
    "        return errors, final_bias_values, final_tau_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0nu5n7drCJMv",
    "outputId": "d1b87cba-8934-4987-cc3f-c44fee910552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_number:  1\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m   x_train, y_train \u001b[38;5;241m=\u001b[39m generate_data(num_neurons[\u001b[38;5;241m0\u001b[39m], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m   num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_user_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_new\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_number\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m   x_train, y_train \u001b[38;5;241m=\u001b[39m generate_data(num_neurons[\u001b[38;5;241m0\u001b[39m], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, create_new)\n\u001b[1;32m     68\u001b[0m net \u001b[38;5;241m=\u001b[39m Network()\n",
      "Cell \u001b[0;32mIn[39], line 18\u001b[0m, in \u001b[0;36mget_user_inputs\u001b[0;34m(create_new, run_number)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_new \u001b[38;5;129;01mand\u001b[39;00m run_number \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClearing existing files and creating new parameters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mclear_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# if not create_new and os.path.exists(parameters_file_path):\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m create_new:\n",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m, in \u001b[0;36mclear_folders\u001b[0;34m(base_folder)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(base_folder):\n\u001b[1;32m      5\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(base_folder)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/content'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main function\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_folder = \"DATA\"\n",
    "    clear_folders(base_folder)  # Clear previous data\n",
    "    create_new = True #by default always create new dataset when running the program for the 1st run\n",
    "\n",
    "    debug_mode = input(\"Do you want to enter debug mode? (yes/no): \").strip().lower()\n",
    "    upload_mode = input(\"Do you want to upload existing run folder? (yes/no): \").strip().lower()\n",
    "    if upload_mode == 'yes':\n",
    "      # Step 2: Upload the compressed folder (.zip file)\n",
    "      print(\"Please upload a .zip file containing the folder.\")\n",
    "      uploaded = files.upload()  # Prompts user to upload files\n",
    "\n",
    "      # Assuming only one file is uploaded, get its name\n",
    "      zip_filename = list(uploaded.keys())[0]\n",
    "\n",
    "      # Step 3: Extract the .zip to /content/DATA/\n",
    "      extract_path = '/content/DATA/'\n",
    "      os.makedirs(extract_path, exist_ok=True)  # Ensure the destination directory exists\n",
    "\n",
    "      with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        # Extract the entire folder (e.g., 'run_3') to '/content/DATA/run_3/'\n",
    "          zip_ref.extractall(extract_path)\n",
    "          print(f\"Folder '{zip_filename}' has been extracted to '{extract_path}'.\")\n",
    "\n",
    "          # Get the list of folders inside the extracted directory\n",
    "          extracted_folders = [f for f in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, f))]\n",
    "\n",
    "          # Assuming only one folder exists (e.g., 'run_3'), get the folder name\n",
    "          if len(extracted_folders) == 1:\n",
    "              extracted_folder = extracted_folders[0]\n",
    "              print(f\"Detected extracted folder: {extracted_folder}\")\n",
    "\n",
    "              # Locate parameters.json file inside the extracted folder (e.g., 'run_3')\n",
    "              parameters_file_path_in_run = os.path.join(extract_path, extracted_folder, 'parameters.json')\n",
    "\n",
    "              # If parameters.json exists, copy it to /content/DATA/ directory\n",
    "              if os.path.exists(parameters_file_path_in_run):\n",
    "                  shutil.copy(parameters_file_path_in_run, os.path.join(extract_path, 'parameters.json'))\n",
    "                  print(f\"'parameters.json' has been copied to '{extract_path}'.\")\n",
    "              else:\n",
    "                  print(\"No 'parameters.json' file found in the extracted folder.\")\n",
    "          else:\n",
    "              print(\"Error: More than one folder was extracted. Unable to locate the run folder.\")\n",
    "\n",
    "    num_runs = int(input(\"Enter the number of runs: \").strip())\n",
    "    run_summary = []\n",
    "\n",
    "    for run_number in range(1, num_runs + 1):\n",
    "        print ('run_number: ', run_number)\n",
    "\n",
    "        # Create a folder for this run\n",
    "        run_folder = create_run_folder(base_folder, run_number)\n",
    "\n",
    "        # Capture the console output\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = StringIO()\n",
    "\n",
    "        try:\n",
    "            if upload_mode == \"yes\":\n",
    "              num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path = get_user_inputs(False, run_number)\n",
    "              x_train, y_train = generate_data(num_neurons[0], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, False)\n",
    "            else:\n",
    "              num_training_inputs, lower_bound,max_lower_bound, upper_bound,min_upper_bound, func_expressions, num_layers, num_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, folder_path = get_user_inputs(create_new if run_number == 1 else False, run_number)\n",
    "              x_train, y_train = generate_data(num_neurons[0], output_num_neurons, num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_upper_bound, func_expressions, folder_path, create_new)\n",
    "\n",
    "            net = Network()\n",
    "            for i in range(num_layers):\n",
    "                if i == num_layers - 1:  # last layer\n",
    "                    net.add(FCLayer(num_neurons[i], output_num_neurons, f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
    "                else:\n",
    "                    net.add(FCLayer(num_neurons[i], num_neurons[i + 1], f'Layer {i+1}', bias=bias_values[i], tau=tau_values[i]))\n",
    "                net.add(ActivationLayer(ReLU))\n",
    "\n",
    "            net.use(mse)\n",
    "\n",
    "            print(f\"Run number: {run_number}\")\n",
    "            # Train the network\n",
    "            errors, final_bias_values, final_tau_values = net.fit(x_train, y_train, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, debug_mode)\n",
    "\n",
    "            # Check final error rate and save results in the appropriate folder\n",
    "            final_error = errors[-1]\n",
    "            print(f'Final Error: {final_error}')  # Debug statement to check the final error\n",
    "\n",
    "            rounded_final_error = round(final_error, 2)\n",
    "            if rounded_final_error == 0.00:\n",
    "                rounded_final_error = 0\n",
    "            run_summary.append((run_number, rounded_final_error))\n",
    "\n",
    "            # Capture the console output to save\n",
    "            output_text = sys.stdout.getvalue()\n",
    "\n",
    "            # Save results in the run folder\n",
    "            save_results(run_folder, run_number, x_train, y_train, bias_values, tau_values, errors, final_bias_values, final_tau_values, output_text, subfolder_name=\"original_result\")\n",
    "\n",
    "        finally:\n",
    "            # Restore the original stdout\n",
    "            sys.stdout = original_stdout\n",
    "\n",
    "        # Save the output to JSON and PDF\n",
    "        save_console_output(run_number, output_text, rounded_final_error, run_folder, None)\n",
    "\n",
    "\n",
    "\n",
    "    # # Create zip file of the DATA folder\n",
    "    # zip_name = \"DATA.zip\"\n",
    "    # zip_folder(base_folder, zip_name)\n",
    "    # print(\"Downloading the zip file. Please move the downloaded file to your preferred folder after downloading.\")\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nRun Summary:\")\n",
    "    headers = [\"Run\", \"Final Error\"]\n",
    "    table = [[run_number, error] for run_number, error in run_summary]\n",
    "    print(tabulate(table, headers, tablefmt=\"grid\"))\n",
    "\n",
    "    # Store former run summary before rerunning with updated bias\n",
    "    former_run_summary = run_summary.copy()\n",
    "\n",
    "    while True:\n",
    "        rerun_or_update = get_valid_input( \"Do you want to rerun a run number or update the parameters? (rerun/update/exit): \",\n",
    "    validate_rerun_or_update, \"Invalid input. Please enter 'rerun', 'update', or 'exit'.\" )\n",
    "\n",
    "        if rerun_or_update == \"exit\":\n",
    "            print(\"Exiting the program.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "          manage_rerun_update(rerun_or_update, base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5GO4X-GV-2B"
   },
   "source": [
    "# **Traditional Neural Network vs Custom**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9O_Amty3WBp3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting absl-py (from keras)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in ./mlp/lib/python3.12/site-packages (from keras) (2.1.3)\n",
      "Collecting rich (from keras)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting h5py (from keras)\n",
      "  Downloading h5py-3.12.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.13.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes (from keras)\n",
      "  Downloading ml_dtypes-0.5.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (21 kB)\n",
      "Requirement already satisfied: packaging in ./mlp/lib/python3.12/site-packages (from keras) (24.2)\n",
      "Collecting typing-extensions>=4.5.0 (from optree->keras)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./mlp/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp312-cp312-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.0-cp312-cp312-macosx_10_9_universal2.whl (750 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m750.2/750.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-macosx_11_0_arm64.whl (322 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m322.3/322.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, typing-extensions, ml-dtypes, mdurl, h5py, absl-py, optree, markdown-it-py, rich, keras\n",
      "Successfully installed absl-py-2.1.0 h5py-3.12.1 keras-3.6.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.0 namex-0.0.8 optree-0.13.1 rich-13.9.4 typing-extensions-4.12.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./mlp/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./mlp/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting setuptools (from tensorflow)\n",
      "  Using cached setuptools-75.5.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in ./mlp/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./mlp/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.68.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./mlp/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Using cached numpy-2.0.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./mlp/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in ./mlp/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in ./mlp/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./mlp/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./mlp/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./mlp/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./mlp/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-macosx_12_0_arm64.whl (239.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m239.6/239.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.68.0-cp312-cp312-macosx_10_9_universal2.whl (11.1 MB)\n",
      "Downloading libclang-18.1.1-1-py2.py3-none-macosx_11_0_arm64.whl (25.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m25.8/25.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-macosx_10_9_universal2.whl (405 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.0.2-cp312-cp312-macosx_14_0_arm64.whl (5.0 MB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-75.5.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.45.0-py3-none-any.whl (72 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorboard-data-server, setuptools, protobuf, opt-einsum, numpy, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, werkzeug, requests, ml-dtypes, astunparse, tensorboard, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.0\n",
      "    Uninstalling ml_dtypes-0.5.0:\n",
      "      Successfully uninstalled ml_dtypes-0.5.0\n",
      "Successfully installed MarkupSafe-3.0.2 astunparse-1.6.3 certifi-2024.8.30 charset-normalizer-3.4.0 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.0 idna-3.10 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 numpy-2.0.2 opt-einsum-3.4.0 protobuf-5.28.3 requests-2.32.3 setuptools-75.5.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 urllib3-2.2.3 werkzeug-3.1.3 wheel-0.45.0 wrapt-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " !pip install keras\n",
    " !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2g2sQQqWDC7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "File \u001b[0;32m~/Desktop/mlp/mlp/lib/python3.12/site-packages/tensorflow/__init__.py:30\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mTop-level module of TensorFlow. By convention, we refer to this module as\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m`tf` instead of `tensorflow`, following the common practice of importing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mthis file with a file generated from [`api_template.__init__.py`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/api_template.__init__.py)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order,protected-access,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_distutils\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_inspect\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "base_folder = \"DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXmQPPYSWEtW"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomLogger\u001b[39;00m(\u001b[43mCallback\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Access the weights and biases after each epoch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_weights()  \u001b[38;5;66;03m# List of numpy arrays\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Access the weights and biases after each epoch\n",
    "        weights = self.model.get_weights()  # List of numpy arrays\n",
    "\n",
    "        # Access the current learning rate from the optimizer\n",
    "        learning_rate = float(self.model.optimizer.learning_rate.numpy())\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1} ended\")\n",
    "        print(f\"Loss: {logs['loss']}, Accuracy: {logs['accuracy']}\")\n",
    "        print(f\"Learning Rate: {learning_rate}\")\n",
    "\n",
    "        # Log each layer's weights and biases\n",
    "        for i, weight in enumerate(weights):\n",
    "            print(f\"Layer {i // 2 + 1} {'Weights' if i % 2 == 0 else 'Biases'}: {weight}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BdgO8f-s_EQ"
   },
   "outputs": [],
   "source": [
    "# Define the Traditional Neural Network\n",
    "\n",
    "# input_dim = number of input neurons\n",
    "# output_dim = number of output neurons\n",
    "\n",
    "def create_traditional_nn(input_dim, output_dim, hidden_layers, neurons_per_layer, learning_rate):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer with the first hidden layer\n",
    "    model.add(Dense(neurons_per_layer[0], input_dim=input_dim, activation='relu'))\n",
    "\n",
    "    # Add additional hidden layers as per the specified configuration\n",
    "    for i in range(1, hidden_layers):\n",
    "        model.add(Dense(neurons_per_layer[i], activation='relu'))\n",
    "\n",
    "     # Output layer\n",
    "    model.add(Dense(output_dim, activation='sigmoid')) # output layer\n",
    "\n",
    "    # Compile the model with specified learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=float(learning_rate)), loss='mse', metrics=['accuracy'])\n",
    "\n",
    "    # Display the model summary\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # model.add(Dense(input_dim[1], input_dim=input_dim[0], activation='relu')) # Hidden layer with 1 neuron\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, before_sleep_log\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "from crewai import Agent, Task, Crew, LLM\n",
    "from crewai_tools import BaseTool\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(_name)\n",
    "\n",
    "# Instantiate the LLM with OpenAI API\n",
    "llm = LLM(model=\"gpt-4o\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# YouTube Transcript Tool\n",
    "class YouTubeTranscriptTool(BaseTool):\n",
    "    name: str = \"YouTube Transcript Tool\"\n",
    "    description: str = \"Extracts the transcript from a YouTube video.\"\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5),\n",
    "           wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "           before_sleep=before_sleep_log(logger, logging.WARNING))\n",
    "    async def _arun(self, video_url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Asynchronous implementation for fetching YouTube transcript.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            video_id = video_url.split('v=')[-1].split('&')[0]\n",
    "            loop = asyncio.get_event_loop()\n",
    "            transcript = await loop.run_in_executor(\n",
    "                None, YouTubeTranscriptApi.get_transcript, video_id)\n",
    "            return \"\\n\".join(entry['text'] for entry in transcript)\n",
    "        except TranscriptsDisabled:\n",
    "            return \"Transcripts are disabled for this video.\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching transcript: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _run(self, video_url: str) -> str:\n",
    "        \"\"\"\n",
    "        Synchronous wrapper for the asynchronous _arun method.\n",
    "        \"\"\"\n",
    "        return asyncio.run(self._arun(video_url))\n",
    "\n",
    "\n",
    "# Instantiate the tool\n",
    "transcript_tool = YouTubeTranscriptTool()\n",
    "\n",
    "# Create an agent\n",
    "video_analyst = Agent(\n",
    "    role=\"Video Analyst\",\n",
    "    goal=\"Extract and analyze YouTube video transcripts.\",\n",
    "    backstory=\n",
    "    \"A seasoned expert in analyzing video content and generating insights.\",\n",
    "    tools=[transcript_tool],  # Pass the initialized tool\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=10,\n",
    "    max_rpm=10,)\n",
    "\n",
    "# Define the task\n",
    "analyze_video_task = Task(\n",
    "    description=\n",
    "    \"Analyze the YouTube video at https://youtu.be/vOvLFT4v4LQ and extract its transcript.\",\n",
    "    expected_output=\"The transcript of the video content.\",\n",
    "    agent=video_analyst,\n",
    "    llm=llm,\n",
    "    max_iterations=10,\n",
    "max_rpm=10,)\n",
    "\n",
    "# Assemble the crew\n",
    "crew = Crew(agents=[video_analyst], tasks=[analyze_video_task], verbose=True)\n",
    "\n",
    "\n",
    "# Sync execution for crew kickoff\n",
    "def kickoff_crew(crew):\n",
    "    try:\n",
    "        crew.kickoff()  # Use the synchronous kickoff method\n",
    "        logger.info(\"Crew tasks completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during crew execution: {e}\")\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    kickoff_crew(crew)\n",
    "\n",
    "\n",
    "# Entry point\n",
    "if name == \"main_\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zsEBqfMsWGgv",
    "outputId": "06e68d41-67c4-461f-8bf5-3c0d496c874a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_runs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize a list to store the errors for each run\u001b[39;00m\n\u001b[1;32m     16\u001b[0m errors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run_number \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mnum_runs\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Define the run folder based on run number\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# run_folder = os.path.join(base_folder, f\"run_{run_number}\")\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     run_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/original_result\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_runs' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the traditional /content/DATA/run_1/original_result/x_train.npy\n",
    "\n",
    "network_setting = input(\"Do you want to modify the traditional network setup? (yes/no): \").strip().lower()\n",
    "\n",
    "# Default parameters\n",
    "hidden_layers_TN = None\n",
    "hidden_num_neurons_TN = None\n",
    "epochs_TN = None\n",
    "\n",
    "if network_setting == 'yes':\n",
    "  hidden_layers_TN = int(input(\"Enter the number of hidden layers for the traditional network: \"))\n",
    "  hidden_num_neurons_TN = [int(input(f\"Enter the number of neurons for hidden layer {i+1}: \")) for i in range(hidden_layers_TN)]\n",
    "  epochs_TN = int(input(\"Enter the number of epochs for the traditional network: \"))\n",
    "\n",
    "# Initialize a list to store the errors for each run\n",
    "errors = []\n",
    "for run_number in range(1, num_runs + 1):\n",
    "    # Define the run folder based on run number\n",
    "    # run_folder = os.path.join(base_folder, f\"run_{run_number}\")\n",
    "    run_folder = f\"/content/{base_folder}/run_{run_number}/original_result\"\n",
    "    print(f\"run_folder: {run_folder}\")\n",
    "\n",
    "    # Load data and parameters for the current run\n",
    "    num_training_inputs, lower_bound, max_lower_bound, upper_bound, min_lower_bound, func_expressions, num_layers, num_neurons, num_input_neurons, output_num_neurons, epochs, bias_learning_rate, tau_learning_rate, tau_bouncing_mode, bias_values, tau_values, x_train, y_train = load_run_data(run_folder, base_folder)\n",
    "\n",
    "    # Determine parameters for the traditional network (use modified if set, otherwise default to custom values)\n",
    "    epochs_to_use = epochs_TN if epochs_TN else epochs\n",
    "    hidden_layers_to_use = hidden_layers_TN if hidden_layers_TN else num_layers - 1  # Subtract input/output layers\n",
    "    hidden_neurons_to_use = hidden_num_neurons_TN if hidden_num_neurons_TN else num_neurons[1:]  # Middle layers\n",
    "\n",
    "    # Initialize the traditional neural network with specified or default parameters\n",
    "    traditional_nn = create_traditional_nn(\n",
    "        input_dim=num_input_neurons,\n",
    "        output_dim=output_num_neurons,\n",
    "        hidden_layers=hidden_layers_to_use,\n",
    "        neurons_per_layer=hidden_neurons_to_use,\n",
    "        learning_rate=bias_learning_rate\n",
    "    )\n",
    "\n",
    "    # Initialize a custom logger to track progress\n",
    "    custom_logger = CustomLogger()\n",
    "\n",
    "     # Train the traditional network and store the error for each run\n",
    "    history = traditional_nn.fit(x_train, y_train, epochs=epochs_to_use, callbacks=[custom_logger])\n",
    "\n",
    "    # Store the final loss (error) for the current run\n",
    "    final_loss = history.history['loss'][-1]\n",
    "    errors.append(final_loss)\n",
    "\n",
    "    # Plot the loss over epochs for this run\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Loss over Epochs for Run {run_number}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mjm2XJP9WIUc",
    "outputId": "157b9de3-009e-4595-b5bb-16da7141e93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run Comparison:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_runs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCustom Model Error\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraditional Model Error\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m table \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run_number \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_runs\u001b[49m):\n\u001b[1;32m      7\u001b[0m     table\u001b[38;5;241m.\u001b[39mappend([\n\u001b[1;32m      8\u001b[0m         run_number \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m         former_run_summary[run_number][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     10\u001b[0m         errors[run_number]\n\u001b[1;32m     11\u001b[0m     ])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(tabulate(table, headers, tablefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_runs' is not defined"
     ]
    }
   ],
   "source": [
    "# Print combined summary\n",
    "print(\"\\nRun Comparison:\")\n",
    "headers = [\"Run\", \"Custom Model Error\", \"Traditional Model Error\"]\n",
    "table = []\n",
    "\n",
    "for run_number in range(num_runs):\n",
    "    table.append([\n",
    "        run_number + 1,\n",
    "        former_run_summary[run_number][1],\n",
    "        errors[run_number]\n",
    "    ])\n",
    "\n",
    "print(tabulate(table, headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oeqv3R1RWNi0"
   },
   "source": [
    "# **To Download a Folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "TmDo7IIAlpRi",
    "outputId": "449e32cc-bf11-4642-8515-718e68ea3b49"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This will compress the folder into a zip file and offer it for download\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_folder\u001b[39m(base_folder, zip_name):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from IPython.display import display\n",
    "from google.colab import files\n",
    "\n",
    "# This will compress the folder into a zip file and offer it for download\n",
    "\n",
    "def download_folder(base_folder, zip_name):\n",
    "    \"\"\"\n",
    "    Compresses the specified run folder into a zip file for download.\n",
    "\n",
    "    Parameters:\n",
    "    base_folder (str): The base folder where the run folders are located.\n",
    "    zip_name (str): The name of the output zip file (without extension).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ask user for the run number\n",
    "    run_number = input(\"Enter the run number to download: \")\n",
    "    folder_path = os.path.join(base_folder, f\"run_{run_number}\")\n",
    "\n",
    "    # Ensure the folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        zip_path = shutil.make_archive(zip_name, 'zip', base_folder, f\"run_{run_number}\")\n",
    "        print(f\"Folder '{folder_path}' has been compressed into '{zip_name}.zip'.\")\n",
    "\n",
    "        # Trigger download\n",
    "        files.download(f\"{zip_name}.zip\")\n",
    "        print(f\"Folder '{zip_name}' downloded in your local directory.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' not found.\")\n",
    "\n",
    "# Base folder path where the run folders are located, e.g., '/content/DATA'\n",
    "base_folder = '/content/DATA'\n",
    "# Zip file name\n",
    "zip_name = 'downloaded_run_folder'\n",
    "\n",
    "download_folder(base_folder, zip_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qV6g9o5xiED"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import sympy as sp\n",
    "\n",
    "# # Function to check if the operation is logical (like AND/OR)\n",
    "# def is_logical_operation(func_str):\n",
    "#     logical_ops = ['&', '|', 'and', 'or']  # Add any other logical operators you want to support\n",
    "#     return any(op in func_str for op in logical_ops)\n",
    "\n",
    "# # Ask user for number of inputs\n",
    "# num_inputs = int(input(\"Enter the number of inputs (dimension): \"))\n",
    "\n",
    "# # Ask user for range for input variable\n",
    "# lower_bound = float(input(\"Enter the lower bound of input range: \"))\n",
    "# upper_bound = float(input(\"Enter the upper bound of input range: \"))\n",
    "\n",
    "# # Ask user for the number of training inputs in the dataset\n",
    "# num_training_inputs = int(input(\"Enter the number of training inputs in the dataset: \"))\n",
    "\n",
    "# # Create X_train dataset with random values uniformly between the bounds\n",
    "# X_train = np.random.uniform(lower_bound, upper_bound, (num_training_inputs, num_inputs))\n",
    "# # print(\"\\nX_train:\")\n",
    "# # print(X_train)\n",
    "# # print(X_train.dtype, X_train.shape)\n",
    "# X_train = [\n",
    "#     [0.27, 0.80],\n",
    "#     [0.82, 0.64],\n",
    "#     [0.16, 0.65],\n",
    "#     [0.57, 0.10],\n",
    "#     [0.12, 0.84],\n",
    "#     [0.83, 0.93],\n",
    "#     [0.61, 0.39],\n",
    "#     [0.44, 0.62],\n",
    "#     [0.63, 0.13],\n",
    "#     [0.45, 0.73]\n",
    "# ]\n",
    "\n",
    "# X_train = np.array(X_train)\n",
    "# print(\"\\nX_train:\")\n",
    "# print(X_train)\n",
    "# print(\"\")\n",
    "# print(X_train.dtype, X_train.shape)\n",
    "\n",
    "# # Ask user for number of outputs\n",
    "# num_outputs = int(input(\"\\nEnter the number of outputs: \"))\n",
    "\n",
    "# # Create y_train by asking user to provide functions for each output\n",
    "# y_train = []\n",
    "# x_symbols = sp.symbols(f'x0:{num_inputs}')  # Creates symbols x0, x1, x2, ..., up to num_inputs\n",
    "# print('x_symbols: ', x_symbols)  # Debug statement\n",
    "\n",
    "# for i in range(num_outputs):\n",
    "#     # Ask user for the function\n",
    "#     func_str = input(f\"Enter y_train({i + 1}) function using symbols x0, x1, x2, ...(use '&' for AND, '|' for OR): \")\n",
    "#     print(f\"\\nfunc_str: {func_str}\")\n",
    "\n",
    "\n",
    "#     if is_logical_operation(func_str):\n",
    "#         # Apply thresholding if the function is a logical operation\n",
    "#         binary_x_train = (X_train > 0.5).astype(int)  # Convert to binary values based on threshold\n",
    "#         print(f\"Binary X_train for y_train({i + 1}):\\n{binary_x_train}\")  # Debug statement\n",
    "\n",
    "#         # Log the function string\n",
    "\n",
    "#         func = sp.sympify(func_str)  # Convert string to sympy expression\n",
    "#         print ('func :',  func)\n",
    "#         func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
    "\n",
    "#         # Evaluate the function for each row\n",
    "#         y_values = np.array([func_callable(*binary_x_train[row]) for row in range(num_training_inputs)])\n",
    "#         print ('y_values before converting to binary:',  y_values)\n",
    "\n",
    "#         # Convert boolean results to binary (0 and 1)\n",
    "#         y_values = y_values.astype(int)\n",
    "#         print ('y_values after converting to binary:',  y_values)\n",
    "#         print(f\"Type of y_values: {type(y_values)}\")\n",
    "\n",
    "\n",
    "#         # Debugging output\n",
    "#         # for row in range(num_training_inputs):\n",
    "#         #     print(f\"Evaluating row {row}: X = {binary_x_train[row]}, Output = {y_values[row]}\")  # Debug statement\n",
    "\n",
    "#     else:\n",
    "#         # For mathematical functions, apply directly on continuous values\n",
    "#         func = sp.sympify(func_str)  # Convert string to sympy expression\n",
    "#         func_callable = sp.lambdify(x_symbols, func, 'numpy')  # Convert to callable\n",
    "#         y_values = np.array([func_callable(*X_train[row]) for row in range(num_training_inputs)])\n",
    "\n",
    "#         # Debugging output\n",
    "#         for row in range(num_training_inputs):\n",
    "#             print(f\"Evaluating row {row}: X = {X_train[row]}, Output = {y_values[row]}\")  # Debug statement\n",
    "\n",
    "#     # Append results to y_train\n",
    "#     y_train.append(y_values)\n",
    "\n",
    "# # Convert y_train to numpy array and transpose to match the format (num_training_inputs, num_outputs)\n",
    "# y_train = np.array(y_train).T\n",
    "\n",
    "# print(\"\\ny_train:\")\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPGnmbjgri_L"
   },
   "outputs": [],
   "source": [
    "# num_samples=10\n",
    "# x0 = np.random.rand(num_samples)\n",
    "# mask_x0 = np.random.choice([0, 1], size=num_samples)\n",
    "# x0[mask_x0 == 0] = np.random.uniform(0, 0.45, size=(mask_x0 == 0).sum())\n",
    "# x0[mask_x0 == 1] = np.random.uniform(0.55, 1, size=(mask_x0 == 1).sum())\n",
    "# x0 = np.round(x0, 2)\n",
    "\n",
    "# # Generate values for x1 either in the range [0, 0.1] or [0.9, 1]\n",
    "# x1 = np.random.rand(num_samples)\n",
    "# mask_x1 = np.random.choice([0, 1], size=num_samples)\n",
    "# x1[mask_x1 == 0] = np.random.uniform(0, 0.45, size=(mask_x1 == 0).sum())\n",
    "# x1[mask_x1 == 1] = np.random.uniform(0.55, 1, size=(mask_x1 == 1).sum())\n",
    "# x1 = np.round(x1, 2)\n",
    "\n",
    "# # Combine all columns\n",
    "# x_train = np.hstack((x0.reshape(-1, 1), x1.reshape(-1, 1)))\n",
    "# print(\"\\nx_train:\")\n",
    "# print(x_train)\n",
    "# print(x_train.dtype, x_train.shape)\n",
    "\n",
    "# # Thresholding for binary logic (considering 0.5 as the threshold)\n",
    "# binary_inputs = (x_train > 0.5).astype(int)\n",
    "\n",
    "# # Compute outputs for AND, OR, XOR gates\n",
    "# y_train_and = np.all(binary_inputs == 1, axis=1).astype(int)  # AND gate\n",
    "# y_train_or = np.any(binary_inputs == 1, axis=1).astype(int)   # OR gate\n",
    "# # y_train_xor = np.sum(binary_inputs, axis=1) % 2               # XOR gate\n",
    "\n",
    "# # Combine the outputs into a single array\n",
    "# #y_train = np.vstack((y_train_and, y_train_or, y_train_xor)).T\n",
    "# y_train = np.vstack((y_train_and, y_train_or)).T\n",
    "# print(\"\\ny_train:\")\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
